[["index.html", "Spatial Data Science in R Chapter 1 Introduction to the sf package 1.1 Loading sptial data into R 1.2 Creating spatial data yourself. 1.3 Basic operations 1.4 Useful functions 1.5 Exercises", " Spatial Data Science in R Jonathan Jupke 2022-10-14 Chapter 1 Introduction to the sf package The most common way to handle spatial data in R is with the sf package (Pebesma 2018). In this first chapter, you will learn the basics of sf - everything you need to load, visualize, alter, and analyze spatial data in R. If you are looking more comprehensive introduction to sf and some additional packages, a great and free resource is the book Geocomputation in R (Lovelace, Nowosad, and Muenchow 2019) available here. 1.1 Loading sptial data into R First we need to load the sf package. When we do so, we get the following message: library(sf) ## Linking to GEOS 3.9.1, GDAL 3.4.3, PROJ 7.2.1; sf_use_s2() is TRUE So sf establishes a connection between the running R instance and three other programs: GEOS, GDAL and PROJ. GEOS and GDAL are collections of functions to read, modify, and write geodata; PROJ transforms geodata from one coordinate reference system to another. Before we can load a file you need to set your working directory. setwd(&quot;~/Uni/teaching/GIS/&quot;) You have to adjust the working directory to your folder structure. If you are not sure how to write the path to your desired folder you can use the file.choose() function. After executing the function a window promt will open where you can select the desired folder in a point-and-click fashion. In the function you have to choose a file by double clicking it. R will now print the path to the selected file in the console. The argument to setwd() needs to be the path to a folder. Therefore, to use the path file.choose() provided you need to remove the file name from the path. With the command st_read() we read the geopackage file gadm36_DEU_3_pk.gpkg (download here). The function starts, like all functions in sf, with st_, short for spatiotemporal. germany &lt;- st_read(&quot;data/gadm36_DEU_3_pk.gpkg&quot;) ## Reading layer `gadm36_DEU_3_pk&#39; from data source `C:\\Users\\jonat\\Documents\\Uni\\teaching\\GIS\\gisbook2\\data\\gadm36_DEU_3_pk.gpkg&#39; using driver `GPKG&#39; ## Simple feature collection with 4680 features and 16 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: 5.866251 ymin: 47.27012 xmax: 15.04181 ymax: 55.05653 ## Geodetic CRS: WGS 84 After the file is loaded we automatically get a set of information. R Output after loading gadm36_DEU_3_pk.gpkg These are from left to right and top to bottom following the yellow boxes: The layer name of the loaded file. This is not the object name in R, but the name stored in the geopackage (gpkg) file; where the file is located on disk; the file format (here GPKG); the number of rows (features, 4680) and columns (fields, 16); the feature type (GeometryType); the bounding box, i.e. minimum and maximum x and y co-ordinate data or longitude and latitude, and finally the coordinate reference system. If we now open the object in R we see the following: germany ## Simple feature collection with 4680 features and 16 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: 5.866251 ymin: 47.27012 xmax: 15.04181 ymax: 55.05653 ## Geodetic CRS: WGS 84 ## First 10 features: ## GID_0 NAME_0 GID_1 NAME_1 NL_NAME_1 GID_2 NAME_2 NL_NAME_2 GID_3 NAME_3 VARNAME_3 ## 1 DEU Germany DEU.1_1 Baden-Württemberg &lt;NA&gt; DEU.1.1_1 Alb-Donau-Kreis &lt;NA&gt; DEU.1.1.1_1 Allmendingen &lt;NA&gt; ## 2 DEU Germany DEU.1_1 Baden-Württemberg &lt;NA&gt; DEU.1.1_1 Alb-Donau-Kreis &lt;NA&gt; DEU.1.1.2_1 Blaubeuren &lt;NA&gt; ## 3 DEU Germany DEU.1_1 Baden-Württemberg &lt;NA&gt; DEU.1.1_1 Alb-Donau-Kreis &lt;NA&gt; DEU.1.1.3_1 Blaustein &lt;NA&gt; ## 4 DEU Germany DEU.1_1 Baden-Württemberg &lt;NA&gt; DEU.1.1_1 Alb-Donau-Kreis &lt;NA&gt; DEU.1.1.4_1 Dietenheim &lt;NA&gt; ## 5 DEU Germany DEU.1_1 Baden-Württemberg &lt;NA&gt; DEU.1.1_1 Alb-Donau-Kreis &lt;NA&gt; DEU.1.1.5_1 Dornstadt &lt;NA&gt; ## 6 DEU Germany DEU.1_1 Baden-Württemberg &lt;NA&gt; DEU.1.1_1 Alb-Donau-Kreis &lt;NA&gt; DEU.1.1.6_1 Ehingen (Donau) &lt;NA&gt; ## 7 DEU Germany DEU.1_1 Baden-Württemberg &lt;NA&gt; DEU.1.1_1 Alb-Donau-Kreis &lt;NA&gt; DEU.1.1.7_1 Erbach &lt;NA&gt; ## 8 DEU Germany DEU.1_1 Baden-Württemberg &lt;NA&gt; DEU.1.1_1 Alb-Donau-Kreis &lt;NA&gt; DEU.1.1.8_1 Kirchberg-Weihungstal &lt;NA&gt; ## 9 DEU Germany DEU.1_1 Baden-Württemberg &lt;NA&gt; DEU.1.1_1 Alb-Donau-Kreis &lt;NA&gt; DEU.1.1.9_1 Laichinger Alb &lt;NA&gt; ## 10 DEU Germany DEU.1_1 Baden-Württemberg &lt;NA&gt; DEU.1.1_1 Alb-Donau-Kreis &lt;NA&gt; DEU.1.1.10_1 Langenau &lt;NA&gt; ## NL_NAME_3 TYPE_3 ENGTYPE_3 CC_3 HASC_3 geom ## 1 &lt;NA&gt; Verwaltungsgemeinschaft Municipality 084255001 &lt;NA&gt; MULTIPOLYGON (((9.777709 48... ## 2 &lt;NA&gt; Verwaltungsgemeinschaft Municipality 084255002 &lt;NA&gt; MULTIPOLYGON (((9.775818 48... ## 3 &lt;NA&gt; Einheitsgemeinde Municipality 084250141 &lt;NA&gt; MULTIPOLYGON (((9.827528 48... ## 4 &lt;NA&gt; Verwaltungsverband Municipality 084255003 &lt;NA&gt; MULTIPOLYGON (((10.00971 48... ## 5 &lt;NA&gt; Verwaltungsgemeinschaft Municipality 084255004 &lt;NA&gt; MULTIPOLYGON (((9.972806 48... ## 6 &lt;NA&gt; Verwaltungsgemeinschaft Municipality 084255005 &lt;NA&gt; MULTIPOLYGON (((9.803958 48... ## 7 &lt;NA&gt; Einheitsgemeinde Municipality 084250039 &lt;NA&gt; MULTIPOLYGON (((9.837822 48... ## 8 &lt;NA&gt; Verwaltungsverband Municipality 084255006 &lt;NA&gt; MULTIPOLYGON (((10.02729 48... ## 9 &lt;NA&gt; Verwaltungsverband Municipality 084255007 &lt;NA&gt; MULTIPOLYGON (((9.708008 48... ## 10 &lt;NA&gt; Verwaltungsverband Municipality 084255008 &lt;NA&gt; MULTIPOLYGON (((9.946911 48... The header is similar to what we have already seen when reading the file. Below are the first ten rows of all columns. The object has the classes: class(germany) ## [1] &quot;sf&quot; &quot;data.frame&quot; For the most part, we can handle sf objects like normal data frames. We can use the [ subsetting operations. germany[,1] ## Simple feature collection with 4680 features and 1 field ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: 5.866251 ymin: 47.27012 xmax: 15.04181 ymax: 55.05653 ## Geodetic CRS: WGS 84 ## First 10 features: ## GID_0 geom ## 1 DEU MULTIPOLYGON (((9.777709 48... ## 2 DEU MULTIPOLYGON (((9.775818 48... ## 3 DEU MULTIPOLYGON (((9.827528 48... ## 4 DEU MULTIPOLYGON (((10.00971 48... ## 5 DEU MULTIPOLYGON (((9.972806 48... ## 6 DEU MULTIPOLYGON (((9.803958 48... ## 7 DEU MULTIPOLYGON (((9.837822 48... ## 8 DEU MULTIPOLYGON (((10.02729 48... ## 9 DEU MULTIPOLYGON (((9.708008 48... ## 10 DEU MULTIPOLYGON (((9.946911 48... germany[1,] ## Simple feature collection with 1 feature and 16 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: 9.61846 ymin: 48.29861 xmax: 9.822202 ymax: 48.36533 ## Geodetic CRS: WGS 84 ## GID_0 NAME_0 GID_1 NAME_1 NL_NAME_1 GID_2 NAME_2 NL_NAME_2 GID_3 NAME_3 VARNAME_3 NL_NAME_3 ## 1 DEU Germany DEU.1_1 Baden-Württemberg &lt;NA&gt; DEU.1.1_1 Alb-Donau-Kreis &lt;NA&gt; DEU.1.1.1_1 Allmendingen &lt;NA&gt; &lt;NA&gt; ## TYPE_3 ENGTYPE_3 CC_3 HASC_3 geom ## 1 Verwaltungsgemeinschaft Municipality 084255001 &lt;NA&gt; MULTIPOLYGON (((9.777709 48... However, when we subset the columns of an sf object, we always keep the geom or geometry column. Usually, this is convenient because this column contains the spatial coordinates of the objects. The geom column is a so-called sticky column. However, if you want to remove this column explicitly, you can use st_drop_geometry(). The geom column is different in another way - it is a list column. Unlike other columns that are vectors, it is a list. The geom column has the class sfc. class(germany$geom) ## [1] &quot;sfc_MULTIPOLYGON&quot; &quot;sfc&quot; sfc is short for simple feature column, i.e. a column for simple features. The individual elements in the column have the class sfg, simple feature geometry. sfg are the individual geometric shapes (points, lines, polygons, …). Below, we will create sfg objects ourselves and compose an sf object from them. An object of class sf can be visualized with plot(). Fr this plot, I subset the data set to the first ten rows and three columns. plot(germany[1:10,7:9]) As you can see, by default each variable is plotted individually. Plots with single variables are created when we subset the dataset to one variable. plot(germany[,&quot;GID_3&quot;]) 1.2 Creating spatial data yourself. Relationship of sf geometries (aus Lovelace et al 2021) In sf, we can create spatial objects ourselves. This is rarely necessary since we usually work with data that was created in other projects, but later procedures are easier to understand once you have gone through the process from the beginning. The functions to create geometric shapes follow a simple rule: st_ + name of the geometryType. So to create a point we use: point1 &lt;- st_point(x = c(1,1)) point1 is a point with coordinates 1 1 and has the classes , XY, POINT, and sfg. class(point1) ## [1] &quot;XY&quot; &quot;POINT&quot; &quot;sfg&quot; plot(point1) Lines consist of several coordinates which are connected with each other. The single coordinates are vectors (c()) just like st_point(). We could use lists, matrices, or data.frames to st_linestring() with several point coordinates. The easiest way is to use matrices with two columns (for X and Y coordiantes) and as many rows as coordinate pairs. In the example we create a line with the coordinates 1 1, 1 2, 2 2, 2 3. line_coordinates &lt;- matrix(data = c(1,1,1,2,2,2,2,3), ncol = 2, byrow = T) line1 &lt;- st_linestring(line_coordinates) plot(line1) To create the object line_coordinates, we have transformed a vector with all coordinates into a matrix with two columns (ncol) and specified that the matrix is filled row by row (byrow = T), i.e., first row 1 column 1, then row 1 column 2 and so on. By default, matrices in R are filled column by column, not row by row. Since this notation is not very intuitive, I prefer the following notation: line_coordinates &lt;- rbind( c(1,1), c(1,2), c(2,2), c(2,3) ) line1 &lt;- st_linestring(line_coordinates) plot(line1) As you can see, the result is the same, but the single coordinates are not in one long vector. The function rbind() (short for row bind) takes single vectors and combines them as rows of a matrix. The equivalent function for columns is called cbind(). With line_coordinates we can also create a multipoint. multipoint1 &lt;- st_multipoint(line_coordinates) plot(multipoint1) Polygons are created with lists. When we have a single one polygon it looks like a LineString with the difference that the first coordinate and the last are the same. polygon_coordinates &lt;- rbind( c(1,1), c(1,2), c(2,2), c(2,1), c(1,1) ) polygon1 &lt;- st_polygon(list(polygon_coordinates)) plot(polygon1) MultiLineStrings and Multipolygons are aslo created with lists. multilinestring_coordinates &lt;- list(rbind(c(1,1), c(1,2), c(1,3), c(1,4)), rbind(c(2,0), c(3,0), c(4,0), c(4,1))) multilinestring1 &lt;- st_multilinestring(multilinestring_coordinates) plot(multilinestring1) multipolygon_coordinates &lt;- list( list(rbind(c(0,0), c(0,1), c(1,1), c(1,0), c(0,0))), list(rbind(c(2,1), c(2,2), c(1,2), c(1,1), c(2,1))) ) multipolygon1 &lt;- st_multipolygon(multipolygon_coordinates) plot(multipolygon1) Geometry collections are single geometric objects that combine different GeometryTypes. geometrycollection1 &lt;- st_geometrycollection(x = list( st_multipolygon(multipolygon_coordinates), st_multilinestring(multilinestring_coordinates) )) plot(geometrycollection1) 1.3 Basic operations So far, our objects are geometric shapes, but not truly spatial. They are not yet assigned to concrete locations on earth, because they do not have a coordinate reference system (CRS). With the command st_crs() we assign a CRS to an object. Alternatively, we can do this with st_sfc() when we turn an sfg into an sfc. We can use different formats to describe the CRS, but in practice the EPSG code is the easiest. We assign geometrycollection1 the CRS WGS 84. geometrycollection_sfc &lt;- st_sfc(geometrycollection1, crs = &quot;EPSG:4326&quot;) geometrycollection_sf &lt;- st_as_sf(geometrycollection_sfc) There are many superior alternatives to the plot() function to create maps in R. Here, we will use the tmap package (Tennekes 2018). Each tmap has at least two elements: 1. tm_shape() The spatial object you want to map. 2. the geometric shape you want to use: tm_dots() for points, tm_lines() for lines and tm_polygons() for polygons. These elements are combined with a +. There are no limits to how many objects or geometric shapes you can include in a single map. If you want to use different objects in one map you can call tm_shape() again with the next object after the +. ## tmap mode set to plotting library(tmap) tm_shape(geometrycollection_sf) + tm_dots(size = 1) + tm_lines() + tm_polygons() With tmap you can create interactive and static maps. Interactive maps are great to explore your data, to check if you have specified the correct CRS, or for interactive documents in html format like this book. To create interactive maps you have to change the tm_mode from “plot” to “view”. tmap_mode(&quot;view&quot;) ## tmap mode set to interactive viewing After that the same function as before will create interactive maps. tm_shape(geometrycollection_sf) + tm_dots(size = 1) + tm_lines() + tm_polygons() 1.4 Useful functions The names of the functions in sf are mostly self-explanatory. you can often just type sf::, scroll through the list of functions that appears, and with some background knowledge and imagination you’ll often find what you’re looking for. This works especially well if you combine this tactic with the help function (?function_name or highlight it and press F1). Nevertheless, to conclude, let’s look at some functions here as an example. 1.4.1 st_area With st_area() we can determine the area of polygons germany_area &lt;- st_area(germany) class(germany_area) ## [1] &quot;units&quot; germany_area has the class units which introduced the eponymous package (Pebesma, Mailund, and Hiebert 2016). With units::drop_units() we can convert the object into simple numbers … germany_area &lt;- units::drop_units(germany_area) … and color the administrative districts according to their area on a map. germany &lt;- dplyr::mutate(germany, area = germany_area) library(mapview) mapview(germany, zcol = &quot;area&quot;) To create this map we used the mapview package (Appelhans et al. 2021). mapview is often the faster and easier solution to create interactive maps. tmap offers more options and is the better solution to create maps for reports. 1.4.2 st_distance With st_distance() we can determine the distance between two objects. As an example we could ask if the 10 largest districts are closer to each other than the ten smallest. big10 &lt;- germany |&gt; dplyr::arrange(area) |&gt; dplyr::slice_tail(n = 10) big10 &lt;- st_distance(big10) The st_distance() can be used with one or two data sets. In both cases the result is a matrix with units. In our case, with only one data set, the distance from each polygon to each polygon is calculated. In cell 3, row 2 is the distance from centroid of the third polygon to the second. Since the distance from the third to the second is equal to the distance from the second to the third, the matrix is symmetric. The diagonal contains the distances of objects to themselves, i.e. 0. big10 &lt;- big10 |&gt; units::drop_units() big10 &lt;- big10[upper.tri(big10)] small10 &lt;- germany |&gt; dplyr::arrange(area) |&gt; dplyr::slice_head(n = 10) |&gt; sf::st_distance() |&gt; units::drop_units() |&gt; {\\(x) x[upper.tri(x)]}() With upper.tri() I select the upper triangle of the matrix (see figure below) so I don’t have every value twice. This also removes the diagonal. For small10 we do the same, but this time with pipe operators. The only differences here are that slice_tail() has been replaced by slice_head() and the annonymous function at the end of the pipe. Anonymous functions are functions without a name. They are not stored as objects but executed directly. In R since version 4.1 we can create anonymous functions as follows: # The two functions do the same thing. plus5 &lt;- function(x) return(x+5) plus5(4) ## [1] 9 4 |&gt; {\\(x) x + 5}() ## [1] 9 Now we still need to package the results in a data set and display them. For the latter, we use the ggplot2 package (Wickham 2016) here. See here for an introduction. library(ggplot2) data &lt;- data.frame(size = rep(c(&quot;big&quot;, &quot;small&quot;), each = 45), distance = c(big10, small10)) ggplot(data, aes(y = distance, x = size)) + geom_boxplot() So we see that the median distance between the ten largest counties is slightly smaller than that between the ten smallest. If we have two data sets and we only want to compare the first element of data set 1 with the first of data set 2, the second with the second, and so on, we set the argument by_element to TRUE in the function st_distance(). 1.4.3 st_nearest_feature With st_nearest_feature() we find the elements in a dataset that are closest to the selected element. # - Select the largest district big1 &lt;- germany |&gt; dplyr::arrange(area) |&gt; dplyr::slice_tail(n = 1) # - Which element from germany is closest to the largest district? nnid &lt;- st_nearest_feature(big1, germany) # - The result is the row number of this closest object. nnid ## [1] 2167 germany[nnid, ] ## Simple feature collection with 1 feature and 17 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: 13.15783 ymin: 52.39318 xmax: 13.24984 ymax: 52.4208 ## Geodetic CRS: WGS 84 ## GID_0 NAME_0 GID_1 NAME_1 NL_NAME_1 GID_2 NAME_2 NL_NAME_2 GID_3 NAME_3 VARNAME_3 NL_NAME_3 ## 2167 DEU Germany DEU.4_1 Brandenburg &lt;NA&gt; DEU.4.13_1 Potsdam-Mittelmark &lt;NA&gt; DEU.4.13.6_1 Kleinmachnow &lt;NA&gt; &lt;NA&gt; ## TYPE_3 ENGTYPE_3 CC_3 HASC_3 geom area ## 2167 Amtsfreie Gemeinde Municipality 120690304 &lt;NA&gt; MULTIPOLYGON (((13.23662 52... 11759803 mapview(rbind(big1,germany[nnid, ]), zcol = &quot;GID_3&quot;) 1.4.4 spatial subsetting There are many ways to subset tables in R, using [, select() and filter(). They use the position of the desired objects in the table (for [), their column names (for select()), or values of the various variables (for filter()). With spatial data, we can spatial relationships to subset data sets. The syntax follows the following scheme: Let X be the data set we want to select from and let Y be the data set we want to select with. For example: we have a set of bird observations all over Germany (X) and a data set with counties (Y) and we want to subset to the birds within one specific county. Now we subset X by Y by X[Y]. As an example, we use a set of bird observations (here for download). In the code below, you can also see how to turn a data frame into an sf object. We use the st_as_sf() and provide it with the data frame, the names of the columns that have the coordinates (coords, first x then y), and the CRS. # - load data birds &lt;- readRDS(&quot;data/birds.rds&quot;) # - drop observations without spatial coordinates birds &lt;- birds[which(!is.na(birds$decimalLongitude)), ] # - turn bird data into sf object. birds &lt;- st_as_sf(birds, coords = c(&quot;decimalLongitude&quot;, &quot;decimalLatitude&quot;), crs = &quot;EPSG:4326&quot;) # - subset data for plot birds_subset &lt;- birds[1:100, ] mapview(birds_subset) Now we want to select only the bird observations that are located in our county. The column NAME_2 holds the county name. We select the county Goslar. Then we subset birds with goslar. goslar &lt;- dplyr::filter(germany, NAME_2 == &quot;Goslar&quot;) goslar_birds &lt;- birds[goslar, ] mapview(goslar_birds) The subsetting checks for a topological relationship between the elements of birds and goslar. If, as in the command above, we do not explicitly choose a topological relationship, the default relationship, intersection (per st_intersects()), is applied. Alternatives are touching the objects (st_touch()), crossing the objects (st_cross()), and covering (st_covers()). See the following figure for more examples. Topological Relationships (Lovelace et al. 2021) We consider here another example to demonstrate a different topological relationship. In this example, we use the germany data set and the largest county from the data set (big1). We want to select all counties that are adjacent to the largest county. So we subset germany the data set containing all counties using big1, the data set containing only the largest county. Instead of the st_intersects() relation, which is set by default, we choose st_touches(). big1_neighbour &lt;- germany[big1, op = st_touches] mapview(big1_neighbour, zcol = &quot;GID_3&quot;, legend = FALSE) 1.4.5 Spatial joins In a spatial join, we add the variables of a second data set to those of a first one. The spatial relationship between the elements of the objects is used to determine which elements are combined. An example would be to add the county to the bird data as a variable. # - Random subset of 500 titmice birds_subset &lt;- birds[sample(1:nrow(birds), 500), ] # - create a dataset based on germany but with only one variable: name_2 germany_name2 &lt;- dplyr::select(germany, NAME_2) # - spatial join birds_name2 &lt;- st_join(birds_subset, germany_name2) mapview(birds_name2, zcol = &quot;NAME_2&quot;, legend = FALSE) 1.4.6 Spatial Aggregation The last thing we want to look at is how we aggregate data. The question here might be: What is the mean abundance of birds in the different counties. So we want to: 1. group the bird data according to which district they fall into. 2. calculate for each group the mean value of the abundance (individualCount). 3. assign these mean values to the counties in a data set. We can do all this with a single function: aggregate(). However, we need to prepare the data a bit for this. aggregate() needs the following arguments: x which data should I aggregate? In our case it is the birds. by in which data are the groups in which I should aggregate? For us it is germany. FUN with which function should I aggregate the data? For us mean(), the mean value. If you want to give arguments to the function you use to aggregate you can do that afterward. # - Create a data set where the tits have only the variable individualCount. birds_count &lt;- dplyr::select(birds, individualCount) # - Reduce to rows that have information for the variable individualCount. birds_count &lt;- dplyr::filter(birds_count, !is.na(individualCount)) # - aggregate the data birds_count using germany with the function mean. birds_agg &lt;- aggregate(x = birds_count, by = germany, FUN = mean) # - If we didn&#39;t remove the NAs we could use the following function: birds_agg &lt;- aggregate(x = birds_count, by = germany, FUN = mean, na.rm = TRUE) # - The argument na.rm = TRUE is an argument of the function mean(). It removes (remove, rm) all NAs before calculating the mean. If you open the help page of aggregate() you will see &quot;...&quot; at the arguments. These so called ellipsis are placeholders for all arguments you can give to the function in FUN. mapview(birds_agg, zcol = &quot;individualCount&quot;) 1.5 Exercises In this folder you will find three files: bike rental stations in London the rail network of London the boroughs of London All data are projected with the projection WGS84 (EPSG: 4326). With this file, answer the following questions: how many bicycle stations are there in the dataset? how many stations have more than 10 bicycles? how big are the different regions of London? Add the area to the dataset as a variable. assign to each bike station the ‘osm_id’ of the nearest train track calculate the distance between each bike station and the nearest train track create a plot showing the number of bikes per station against the distance to the next rail line. create a map on which the bike stations are colored according to the distance to the nearest train line. Calculate the average distance from bike station to the nearest rail line for each borough in London for which bike data is available. References "],["point-pattern-analysis.html", "Chapter 2 Point Pattern Analysis 2.1 the ppp class 2.2 First order processes 2.3 Second order processes", " Chapter 2 Point Pattern Analysis In this script, we will explore point pattern analysis (ppa) in R. The most common package for ppa in R is spatstat (Baddeley and Turner 2005). Other introductions to ppa and spatstat in specific are Chapter 4 in Fletcher and Fortin (2018), Chapter 11 in Pebesma and Bivand (2022), Baddeley and Turner (2005), and Baddeley, Rubak, and Turner (2015). There are some less common package that we will not delve into here but which I do want to mention as they might be helpful to you at some future point: ads (Pélissier and Goreaud 2015), ecespa (Cruz Rot 2008), splancs (Rowlingson and Diggle 2022), and stpp (Gabriel et al. 2022). 2.1 the ppp class spatstat introduced its own object class to R, the ppp class. Most functions we will work with in the following script only work with this object class. To illustrate the package we will use the a data set of bird occurrences in Germany we already used in the first chapter, the Database of Global Administrative Areas (GADM) which we also already used in the first lecture and a DEM of Rhineland Palatinate ( download here). library(pacman) p_load(sf, spatstat, mapview, dplyr, ggplot2, magrittr, terra, raster, maptools, data.table) birds &lt;- readRDS(&quot;data/birds.rds&quot;) gadm &lt;- st_read(&quot;data/gadm36_DEU_3_pk.gpkg&quot;) ## Reading layer `gadm36_DEU_3_pk&#39; from data source `C:\\Users\\jonat\\Documents\\Uni\\teaching\\GIS\\gisbook2\\data\\gadm36_DEU_3_pk.gpkg&#39; using driver `GPKG&#39; ## Simple feature collection with 4680 features and 16 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: 5.866251 ymin: 47.27012 xmax: 15.04181 ymax: 55.05653 ## Geodetic CRS: WGS 84 DEM &lt;- rast(&quot;data/DTM Germany_Rheinland-Pfalz 20m.tif&quot;) We will need to prepare the bird data before we can start with the analyses. First, we remove all entries with missing coordinates. birds2 &lt;- birds[!is.na(birds$decimalLatitude), ] Then we remove duplicate observations, so that there is only one observation per location. This is easiest with the unique() function from the data.table package (Dowle and Srinivasan 2021). While base R also has a unique() function, the base R version only works with vectors. It returns all unique values of a vector. The data.table version works with data frames and returns only rows with unique values in the selected columns. The columns are selected with the by argument. We choose the columns that hold the coordinates decimalLatitude and decimalLongitude. birds2 &lt;- unique(birds2, by = c(&quot;decimalLatitude&quot;, &quot;decimalLongitude&quot;)) Now we can turn the data frame into an sf object. birds2 &lt;- st_as_sf(birds2, coords = c(&quot;decimalLongitude&quot;, &quot;decimalLatitude&quot;), crs = 4326) Lastly, we subset the data to only observations within Rhineland Palatinate. birds2 &lt;- st_filter(birds2, filter(gadm, NAME_1 == &quot;Rheinland-Pfalz&quot;)) Let’s have a look. mapview(birds2) These bird observations are our starting point for the ppa. We can use the as.ppp() function to convert the sf object into a ppp. There is one caveat however: the data need to be in a projected coordinate system. Otherwise as.ppp() will return an error. Currently, our data are in a geographic coordinate system (latitude and longitude) so we need to transform them first. I choose ETRS89 / UTM zone 32N (N-E) (EPSG 3044) because that is the CRS of the DEM we will use later. birds2 &lt;- st_transform(birds2, crs = &quot;EPSG:3044&quot;) We will not cover marked point patterns in this tutorial. Therefore, we need to remove all variables except for the geometry. birds2 &lt;- dplyr::select(birds2, geometry) Now we can create the ppp. birds_ppp &lt;- as.ppp(birds2) birds_ppp ## Planar point pattern: 9547 points ## window: rectangle = [300361, 463668.4] x [5426570, 5641700] units We see the we have a planar (i.e. 2 dimensional) point pattern with 9547 point which is equal to the number of rows in birds2. Besides the points, the ppp object also contains the window, the rectangle in that includes all points. We can plot the ppp using the base plot function. plot(birds_ppp) The window can be queried separately through the Window() function. The resulting object has the class owin (observation window). x &lt;- Window(birds_ppp) class(x) ## [1] &quot;owin&quot; 2.2 First order processes The most basic first order property is the density, the number of points in an area. We can use the quadratcount() function to separate our window in quadrants and count the points in each. The number of quadrant rows and columns is determined through the arguments nx and ny. First, we compute the global density so just a single quadrant, hence both nx and ny are one. Q0 &lt;- quadratcount(birds_ppp, nx= 1, ny=1) plot(birds_ppp, pch=20, cols=&quot;grey70&quot;, main=NULL) plot(Q0, add=TRUE) For the plot I have adjusted the symbol (pch), point color (cols) and plot title (main). We first plot the points (birds_ppp) and then add the quardrants (or just quadrant in this case) with Q0. Notice the add=TRUE this adds the elements from Q0 to the plot of birds_ppp instead of creating a new plot. By varying the number of quadrants we can alter the number of local densities we compute. Q1 &lt;- quadratcount(birds_ppp, nx= 2, ny=2) Q2 &lt;- quadratcount(birds_ppp, nx= 3, ny=6) Q3 &lt;- quadratcount(birds_ppp, nx= 10, ny=20) plot(birds_ppp, pch=20, cols=&quot;grey70&quot;, main = NULL) plot(Q1, add=TRUE) plot(birds_ppp, pch=20, cols=&quot;grey70&quot;, main = NULL) plot(Q2, add=TRUE) plot(birds_ppp, pch=20, cols=&quot;grey70&quot;, main = NULL) plot(Q3, add=TRUE) The intensity is the density per area and can be computed by intensity(). Q0.d &lt;- intensity(Q0, image = T) Q1.d &lt;- intensity(Q1, image = T) Q2.d &lt;- intensity(Q2, image = T) Q3.d &lt;- intensity(Q3, image = T) # density raster plot(Q0.d, main = NULL) plot(Q1.d, main = NULL) plot(Q2.d, main = NULL) plot(Q3.d, main = NULL) The unit - points per square meter - is not very intuitive. We can change it to points per square kilometer with the rescale() function. birds_km &lt;- spatstat.geom::rescale(X = birds_ppp, s = 1000, unitname = &quot;km&quot;) Now we get the same results but in the new and much more intuitive units. Q &lt;- quadratcount(birds_km, nx= 10, ny=20) Q.d &lt;- intensity(Q, image = T) plot(Q.d, main = NULL) We can also use predefined areas to compute density and intensity. We will go through two different approaches here. The first is based on a categorized raster and the second on a polygon layer. The categorized raster is the DEM of Rhineland Palatinate we loaded in the beginning. As the raster is quite large we will aggregate cells to increase cell sizes and decrease the resolution. We loaded the raster with the rast() function from the terra package (Hijmans 2022b). terra is the leading package for working with raster data in R at the moment (see here for an introduction) . It supersedes the raster package (Hijmans 2022a) which was the standard package before. Each of the two packages has its own object class to store the raster internally. The raster package uses the class RasterLayer and the terra package uses the class SpatRaster. Some packages have not adjusted to the switch to terra yet and still require RasterLayer objects. This is also the case for spatstat, which in turn again uses its own raster object class im. Thus, we need to transform the SpatRaster to a RasterLayer which can be done with the function raster() and transform the RasterLayerto an im with as.im(). DEM2 &lt;- terra::aggregate(DEM, fact = 4) DEM3 &lt;- raster(DEM2) DEM4 &lt;- as.im(DEM3) DEM4 is still a continuous raster where each cell stores an elevation value. Now we will create a categorical raster which stores elevation classes instead of elevation values. First, we need to define the breaks, i.e., which elevation values correspond to which classes. As we have no a priori classification in mind here we will use the quartiles of the elevation values. summary(values(DEM2)) ## DTM Germany_Rheinland-Pfalz 20m ## Min. : 27.7 ## 1st Qu.:223.1 ## Median :318.2 ## Mean :318.1 ## 3rd Qu.:415.0 ## Max. :815.9 ## NA&#39;s :2787887 breaks &lt;- c( -Inf, 223, 318, 415 , Inf) With the cut() function we assign the cells of DEM4 to four classes labeld 1 to 4 according to the breaks defined in breaks. Afterwards we need to transform this to another object class one more time with tess(). elev_class &lt;- cut(DEM4, breaks = breaks, labels = 1:4) elev_class &lt;- tess(image = elev_class) plot(elev_class) Now we can use the classified elevation raster as quadrants for the quadratcount() function. Q_elev &lt;- quadratcount(birds_ppp, tess = elev_class) plot(Q_elev) Q_elev2 &lt;- intensity(Q_elev, image = T) plot(Q_elev2) Based on this we can already see, that the number of observations tends to decrease along the elevation classes. As a second example, we will use sf polygons to define quadrants. Here we will do so with the districts within Rhineland Palatinate. In the end, we still need to provide quadratcount()with an object of class tess. This can be created with # subset all of Germany to just RLP rlp &lt;- filter(gadm, NAME_1 == &quot;Rheinland-Pfalz&quot;) # assign new CRS that conforms the to CRS of birds rlp &lt;- st_transform(rlp, 3044) # Aggregate smaller district units to NAME_2 level rlp %&lt;&gt;% group_by(NAME_2) %&gt;% summarise(geom = st_union(geom)) # sf polygons can be transformed to owin objects with as.owin(). # We create a list where each object is one entry is one district as owin. rlp_list &lt;- list() for(i in 1:nrow(rlp)){ rlp_list[[i]] &lt;- as.owin(rlp[i, ]) } # convert owin to tess rlp_owin &lt;- as.tess(rlp_list) Q &lt;- quadratcount(birds_ppp, tess = rlp_owin) cl &lt;- interp.colours(c(&quot;lightyellow&quot;, &quot;orange&quot; ,&quot;red&quot;), length(rlp_list)) plot(Q, col = cl) Q.d &lt;- intensity(Q, image = T) plot(Q.d, col = cl, main = NULL) 2.2.1 Kernel density We can create continuous density surfaces with kernel density estimation. The image below is similar to the intensity plots above but the individuals cells are far smaller. Notice that we have he unintuitive unit of observations per square meter again, thus the small numbers. K1 &lt;- density(birds_ppp) plot(K1, main = NULL) contour(K1, add=TRUE) The bandwidth influences how far the influence of a single observation extends. If we choose a small bandwidth the kernel will only estimate higher densities in direct viccinity of the observations. If we choose a larger bandwidth the density estimations will increase for more removed parts. How exactly that works depends on the kernel function. For the Gaussian Kernel, which is the default setting in density() the function looks like this: \\[G(x, \\sigma) = \\frac{1}{\\sqrt{2\\pi\\sigma}} exp(-\\frac{x^2}{2\\sigma^2})\\] This is the same formula as for the normal probability distribution, where the bandwidth corresponds to the standard deviation. So you can imagine having a normal distribution centered on every observation. The weight of this observation, how much it adds to the estimated density it of a nearby point decreases with the distance to the observation. The speed with which it decreases corresponds to the standard deviation of my normal distribution and hence to the bandwidth. Hopefully Figure @ref{fig:} Figure 2.1: Effect of bandwidth in a gaussian Kernel In R, we can adjust the bandwidth with the sigma argument. To illustrate the effect of the bandwidth on the density maps, we create six different kernels with increasing bandwidths. par(mfrow = c(2,3)) for (i in seq(10000, 60000, by = 8433.33)) { x = density(birds_ppp, sigma = i) plot(x, main = paste0(&quot;bandwidth =&quot;, i)) contour(x, add = T) } We can also compare different kernel functions in the same way. For the density() function from the Raster package, there are four different kernel functions available: 1) Gaussian (the default option), 2) Quartic 3) Disc and 4) Epanechnikov. par(mfrow = c(2,2), mar = c(0,0,0,0)) for (i in c(&quot;gaussian&quot;, &quot;quartic&quot;, &quot;disc&quot;, &quot;epanechnikov&quot;)) { x = density(birds_ppp, sigma = 20000, kernel = i) plot(x, main = paste0(&quot;Kernel =&quot;, i)) contour(x, add = T) } Next, we turn to some models that estimate the relationship between the intensity of a point pattern one some other variable. One such function is the rhohat() function. The name derived from the greek letter \\(\\rho\\) that is commonly used for intensity and the custom to mark mathematical estimates with a hat. So \\(\\hat{\\rho}\\) is an estimate of \\(\\rho\\). The rhohat() function fits a non-parametric model to the intensity that does not assume a specific functional form like linear or quadratic. As arguments it takes the ppp for which we want to model the intensity (object), the variable we want to predict the intensity with (covariate) rho &lt;- rhohat(birds_ppp, DEM4, method =&quot;ratio&quot;) plot(rho) We see four different lines: \\(\\hat{\\rho}\\) is the estimated mean intensity as function of the elevation, \\(\\bar{\\rho}\\) the overall mean instensity, \\(\\rho_{hi}\\) and \\(\\rho_{lo}\\) are the upper and lower bound of the 95% confidence interval respectively. We can use this model to predict the density for each cell of DEM4. pred &lt;- predict(rho) plot(raster(pred)) Again, we see that the intensity is highest in lowland areas and decreases with increasing elevation. Alternatively, we can use a Poisson point process model (PPM) for prediction. The poission point process follow as functional form. It assumes that the number of points within a given region follow the Poisson distribution. The model is written out as: \\[\\mathbf{P}(N(B)=n)=\\frac{\\lambda^{n}(\\nu(B))^{n}}{n !} \\exp (-\\lambda \\nu(B))\\] Where \\(\\mathbf{P}(N(B)=n)\\) is the probability in the area \\(B\\) the number of points (\\(N(B)\\)) equals \\(n\\). \\(\\lambda\\) is the intensity, the rate parameter of the Poission distribution, and therefore the parameter that is estimated when we fit the model. \\(\\nu(B)\\) is the area of \\(B\\). When running this command, there may be a warning message. This is not a problem. PPM1 &lt;- ppm(birds_ppp ~ DEM4) ## Warning: Values of the covariate &#39;DEM4&#39; were NA or undefined at 33% (16530 out of 49551) of the quadrature points. Occurred while ## executing: ppm.ppp(Q = birds_ppp, trend = ~DEM4, data = NULL, interaction = NULL) The effectfun() function now calculates the trend in bird intensity with increasing elevation. PPM1_effect &lt;- effectfun(PPM1, &quot;DEM4&quot;, se.fit=TRUE) plot(PPM1_effect, las=1) Again, we have a mean estimate as a function of the elevation (\\(\\hat{\\lambda}\\)), as well as higher and lower bounds of the 95% confidence interval (\\(\\lambda_{hi}\\) and \\(\\lambda_{lo}\\)). The general form is similar to the non-parametric model, but the decline in intensity is slower. Both methods thus come to similar conclusions: par(mfrow = c(1,2)) plot(rho, main = &quot;non-parametric model&quot;) plot(PPM1_effect, main = &quot;PPM&quot; ) pred_ppm &lt;- predict(PPM1) plot(raster(pred), col = cl, main = &quot;non-parametric model&quot;) plot(pred_ppm, col = cl, main = &quot;PPM&quot;) 2.3 Second order processes Now we can turn to the second order processes, i.e., to metrics and functions that consider the location of point with respect to other points. 2.3.1 Average nearest neighbor For the average nearest neighbor (ANN) method we use the nndist() function. It calculates the distance from each point to the next closest. The argument k indicates in which neighbor we are interested. The nearest neighbor is found with k=1. With k = 2 we would calculate the distance to the second nearest neighbor. # - compute nearest neighbor NN &lt;- nndist(birds_ppp, k = 1) # - average distance to nearest neighbor over all points ANN &lt;- mean(NN) plot(sort(NN), type = &quot;l&quot;) abline(h = ANN, col = &quot;red&quot;, lwd = 2) We can see that most points are close to other ones. Only a few points are isolated with distances of up to approximately 10 kilometers to the closest observation. We have 9547 observations and thus there are 9546 (n-1) potential distance classes (i.e., values for k). This is more than we need here and we will content ourselves with 999 distance classes, to see how distance increases with increasing degrees of neighborhood. To do so, we call nndist() and provide a vector to the k argument. The function computes the distance for each observation and each value of k and returns them in a matrix. Each row is one observation and each column is one value of k. As we are interested in the mean distance per k value, we summarize this with the apply() function. apply() takes the matrix we compute with nndist() and applies the function in FUN to each column. It applies the function to columns because we set the MARGIN to 2. If we would have set the MARGIN to 1 apply() would have computed row-wise means. n &lt;- 999 NN &lt;- nndist(birds_ppp, k = 1:n) ANN &lt;- apply(X = NN, MARGIN = 2, FUN = mean) plot(ANN ~ eval(1:n), type = &quot;l&quot;, main=NULL, las=1, xlab = &quot;k&quot;) The distance to the next neighbor increases with k. 2.3.2 The K-Funktion Next we will use the K,L and G functions. All three are implemented as functions in spatstat. K &lt;- Kest(birds_ppp, correction = &quot;isotropic&quot;) plot(K) K is always larger than that of a poisson distributed pattern. This indicates clustered observations. This is confirmed by the L function. The term . -r ~ r is necessary to set the line straight to zero. L &lt;- Lest(birds_ppp, correction = &quot;isotropic&quot;) plot(L, . -r ~ r) Lastly, the G-function G &lt;- Gest(birds_ppp) plot(G) For the G-Function, different estimators are displayed. The first three lines are different estimators of the G-Function and are generally in agreement in this case. The last line (\\(G_{pois}\\)) is the G-Function of a poisson point process. As with the K and the L function before, that fact that the estimates for our data are higher than that for the poisson processes point to the fact, that our data are clustered. 2.3.3 Morans I Lastly, we will have a look at Moran’s I a function for that looks for spatial autocorrelation. This will be different from the other function we covered in this tutorial so far, because it requires a different package spdep (Bivand, Pebesma, and Gómez-Rubio 2013) and we will need a mark. Thus we need to go back to the bird data and keep the individualCount variable. library(spdep) ## Lade nötiges Paket: spData # - We dont need to specify the geometry column in the call # - to select because it is sticky. birds3 &lt;- birds |&gt; filter(!is.na(decimalLatitude) &amp; !is.na(individualCount)) |&gt; st_as_sf(coords = c(&quot;decimalLongitude&quot;, &quot;decimalLatitude&quot;), crs = 4326) |&gt; st_filter(filter(gadm, NAME_1 == &quot;Rheinland-Pfalz&quot;)) |&gt; dplyr::select(count = individualCount) # - Add x and y coordinates as individual variables. birds3 %&lt;&gt;% mutate(x.coord = st_coordinates(birds3)[,1], y.coord = st_coordinates(birds3)[,2]) # - Turn birds3 into a data table so we can use the unique() function. setDT(birds3) birds3 %&lt;&gt;% unique(by = c(&quot;x.coord&quot;, &quot;y.coord&quot;)) %&gt;% st_as_sf() %&gt;% st_drop_geometry() After preparing the bird data, we need to create a neighborhood list for our data. This works in three steps: 1. identify the nearest (or k\\(^{th}\\)) neighbor with knearneigh(). 2. Turn knn object created by knearneigh() into a neighbors list of class nb with knn2nb(). 3. Add spatial weights nb with nb2listw(). For our case, spatial weights simply reinforce the neighborhood scheme. So only the nearest neighbor is weighted and all other observations have a weight of zero. Then finally, we can use moran.test() to compute Moran’s I. We provide the function with the variable of interest (the mark) and the weighted list of neighbor. knn &lt;- knearneigh(birds3, k = 1) nb &lt;- knn2nb(knn) listw &lt;- nb2listw(nb) moran.test(x = birds3$count, listw = listw) ## ## Moran I test under randomisation ## ## data: birds3$count ## weights: listw ## ## Moran I statistic standard deviate = 24.301, p-value &lt; 2.2e-16 ## alternative hypothesis: greater ## sample estimates: ## Moran I statistic Expectation Variance ## 0.98173162 -0.00104712 0.00163554 From these results we can see that close points are more similar than would be expected by chance (p-value &lt;0.05). Remember that a Moran’s I of 0 would indicate random dispersal, 1 perfect clustering, and -1 perfect dispersal. References "],["interpolation.html", "Chapter 3 Interpolation 3.1 The data 3.2 Proximity Polygons 3.3 Trend Surface Analysis 3.4 Splines 3.5 Weighted average 3.6 Kriging", " Chapter 3 Interpolation In this tutorial, you will learn how to use R to apply different interpolation methods to your data sets. We will use couple of R packages that are familiar to us, like sf and mapview but also some new ones. Most notably, we will use fields (Douglas Nychka et al. 2021) for splines and gstat (Gräler, Pebesma, and Heuvelink 2016) as well as automap (Hiemstra et al. 2008) for distance weighting. library(automap) library(ggplot2) library(sf) library(sp) library(dplyr) library(fields) library(gstat) library(tmap) library(mapview) 3.1 The data We will use the LUCAS soil data. LUCAS is an acronym for Land Use/Land Cover Area Frame Survey. This data base contains approximately 20.000 soil samples from all EU27 countries with the exceptions of Romania and Bulgaria. You can download a subset of the LUCAS data base I created for this course here. Next, we load the LUCAS data. lucas &lt;- readRDS(&quot;data/lucas_saxony.rds&quot;) As always, we first inspect the new object. class(lucas) ## [1] &quot;sf&quot; &quot;data.frame&quot; The glimpse() function from dlyr is similar to the str() from base R. glimpse(lucas) ## Rows: 79 ## Columns: 23 ## $ Point_ID &lt;int&gt; 44743044, 44763132, 44763142, 44883126, 44923048, 44923066, 44923084, 44923152, 44963130, 44983054, 44983158, 45003106… ## $ Coarse &lt;int&gt; 34, NA, NA, NA, 23, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ Clay &lt;int&gt; 22, NA, NA, NA, 22, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ Sand &lt;int&gt; 26, NA, NA, NA, 18, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ Silt &lt;int&gt; 52, NA, NA, NA, 60, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ pH_CaCl2 &lt;dbl&gt; 5.4, 6.5, 6.6, 7.3, 5.3, 6.5, 6.0, 6.8, 4.5, 5.9, 5.8, 5.9, 6.0, 5.5, 4.4, 6.1, 5.2, 4.1, 3.7, 4.7, 6.9, 3.5, 4.1, 6.4… ## $ pH_H20 &lt;dbl&gt; 5.77, 6.55, 6.82, 7.70, 5.53, 6.83, 6.14, 7.00, 4.66, 5.94, 6.10, 5.96, 6.18, 5.80, 4.57, 6.37, 5.70, 4.23, 4.34, 4.85… ## $ EC &lt;dbl&gt; 8.52, 22.00, 17.34, 24.90, 38.90, 9.32, 10.88, 20.50, 46.00, 50.50, 15.48, 122.00, 54.10, 59.00, 8.90, 5.80, 26.80, 23… ## $ OC &lt;dbl&gt; 36.3, 19.9, 15.7, 20.6, 34.1, 17.2, 18.1, 11.1, 97.8, 31.8, 10.2, 19.0, 60.1, 22.0, 40.3, 12.6, 23.3, 29.2, 162.2, 35.… ## $ CaCO3 &lt;int&gt; 0, 2, 0, 16, 0, 1, 0, 2, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 2, 0, 0, 0, 3, 0, 2, 1, 2, 0, 2, 1, 0, 0, 0, … ## $ P &lt;dbl&gt; 60.0, 48.2, 67.7, 4.6, 45.0, 44.1, 49.2, 33.7, 30.6, 18.6, 51.8, 96.1, 64.7, 57.3, 22.2, 25.1, 37.0, 27.6, 72.7, 36.7,… ## $ N &lt;dbl&gt; 4.1, 2.1, 1.8, 0.9, 4.3, 1.9, 1.9, 1.4, 5.7, 3.5, 1.2, 2.6, 6.3, 2.6, 2.1, 1.1, 2.7, 2.2, 10.1, 3.4, 2.8, 10.9, 3.2, 0… ## $ K &lt;dbl&gt; 103.4, 192.1, 265.8, 90.1, 328.2, 123.8, 197.9, 149.4, 77.7, 91.1, 162.4, 724.9, 1752.9, 81.5, 38.0, 115.5, 89.9, 119.… ## $ LC &lt;chr&gt; &quot;E10&quot;, &quot;B11&quot;, &quot;B11&quot;, &quot;C10&quot;, &quot;E20&quot;, &quot;B13&quot;, &quot;B55&quot;, &quot;B11&quot;, &quot;C10&quot;, &quot;E20&quot;, &quot;B11&quot;, &quot;B11&quot;, &quot;E20&quot;, &quot;C10&quot;, &quot;C32&quot;, &quot;B14&quot;, &quot;C32&quot;,… ## $ LU &lt;chr&gt; &quot;U111&quot;, &quot;U111&quot;, &quot;U111&quot;, &quot;U120&quot;, &quot;U111&quot;, &quot;U111&quot;, &quot;U111&quot;, &quot;U111&quot;, &quot;U120&quot;, &quot;U111&quot;, &quot;U111&quot;, &quot;U111&quot;, &quot;U111&quot;, &quot;U420&quot;, &quot;U120&quot;… ## $ NUTS_0 &lt;chr&gt; &quot;DE&quot;, &quot;DE&quot;, &quot;DE&quot;, &quot;DE&quot;, &quot;DE&quot;, &quot;DE&quot;, &quot;DE&quot;, &quot;DE&quot;, &quot;DE&quot;, &quot;DE&quot;, &quot;DE&quot;, &quot;DE&quot;, &quot;DE&quot;, &quot;DE&quot;, &quot;DE&quot;, &quot;DE&quot;, &quot;DE&quot;, &quot;DE&quot;, &quot;DE&quot;, &quot;DE&quot;… ## $ NUTS_1 &lt;chr&gt; &quot;DED&quot;, &quot;DED&quot;, &quot;DED&quot;, &quot;DED&quot;, &quot;DED&quot;, &quot;DED&quot;, &quot;DED&quot;, &quot;DED&quot;, &quot;DED&quot;, &quot;DED&quot;, &quot;DED&quot;, &quot;DED&quot;, &quot;DED&quot;, &quot;DED&quot;, &quot;DED&quot;, &quot;DED&quot;, &quot;DED&quot;,… ## $ NUTS_2 &lt;chr&gt; &quot;DED4&quot;, &quot;DED5&quot;, &quot;DED5&quot;, &quot;DED5&quot;, &quot;DED4&quot;, &quot;DED4&quot;, &quot;DED4&quot;, &quot;DED5&quot;, &quot;DED5&quot;, &quot;DED4&quot;, &quot;DED5&quot;, &quot;DED5&quot;, &quot;DED4&quot;, &quot;DED5&quot;, &quot;DED5&quot;… ## $ NUTS_3 &lt;chr&gt; &quot;DED44&quot;, &quot;DED52&quot;, &quot;DED53&quot;, &quot;DED52&quot;, &quot;DED44&quot;, &quot;DED45&quot;, &quot;DED45&quot;, &quot;DED53&quot;, &quot;DED52&quot;, &quot;DED45&quot;, &quot;DED53&quot;, &quot;DED52&quot;, &quot;DED45&quot;, &quot;… ## $ LC0_Desc &lt;chr&gt; &quot;Grassland&quot;, &quot;Cropland&quot;, &quot;Cropland&quot;, &quot;Woodland&quot;, &quot;Grassland&quot;, &quot;Cropland&quot;, &quot;Cropland&quot;, &quot;Cropland&quot;, &quot;Woodland&quot;, &quot;Grassla… ## $ LC1_Desc &lt;chr&gt; &quot;Grassland with sparse tree/shrub cover&quot;, &quot;Common wheat&quot;, &quot;Common wheat&quot;, &quot;Broadleaved woodland&quot;, &quot;Grassland without t… ## $ LU1_Desc &lt;chr&gt; &quot;Agriculture (excluding fallow land and kitchen gardens)&quot;, &quot;Agriculture (excluding fallow land and kitchen gardens)&quot;, … ## $ geometry &lt;POINT [°]&gt; POINT (12.15614 50.48765), POINT (12.22172 51.27776), POINT (12.22607 51.3676), POINT (12.39081 51.22046), POINT… We use the mapview package to display the data on an interactive map. mapview(lucas) We will need a layer of the the German federal state saxony to prepare maps later on. We get it from the Database of Global Administrative Areas (GADM). We have used the GADM data for Germany before (gadm36_DEU_3_pk.gpkg). You can download them here. You can always query other countries or resolutions by following this tutorial. saxony &lt;- st_read(&quot;data/gadm36_DEU_3_pk.gpkg&quot;) |&gt; st_as_sf() |&gt; filter(NAME_1 == &quot;Sachsen&quot;) ## Reading layer `gadm36_DEU_3_pk&#39; from data source `C:\\Users\\jonat\\Documents\\Uni\\teaching\\GIS\\gisbook2\\data\\gadm36_DEU_3_pk.gpkg&#39; using driver `GPKG&#39; ## Simple feature collection with 4680 features and 16 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: 5.866251 ymin: 47.27012 xmax: 15.04181 ymax: 55.05653 ## Geodetic CRS: WGS 84 mapview(saxony) 3.2 Proximity Polygons We start out with proximity polygons. We can create Voronoi polygons around our points with st_voronoi(). #- st_voroni works better with projected coordinate reference systems voroni &lt;- st_voronoi(lucas) ## Warning in st_voronoi.sfc(st_geometry(x), st_sfc(envelope), dTolerance, : st_voronoi does not correctly triangulate longitude/latitude ## data The warning messages notifies us that it its advisable to use projected coordinates, thus we transform our data from WGS84 to Lamber Azimuthal Eqal Area. lucas %&lt;&gt;% st_transform(3035) saxony %&lt;&gt;% st_transform(3035) With the following code we create Voronoi polygons around the observations in lucas and then keep only those areas of the polygons that intersect with the saxony polygon. As you can see we need more functions than just st_voronoi() to do this. With st_union() we combine the single POINT objects in lucas to a single MULTIPOINT object. With st_voronoi() we can now create the Voronoi polygons which are returned in the GEOMERTRYCOLLECTION. To extract the polygons from this collection, we can use the st_collection_extract() function. Its is well worth your time to execute the steps one by one and to check out the intermediate products. voroni &lt;- lucas |&gt; st_union() |&gt; st_voronoi() |&gt; st_collection_extract() |&gt; st_intersection(y= saxony) |&gt; st_as_sf() Next, we add the electrical conductivity (EC, our focal variable for this example) to the respective polygons. We assign a point to each polygon with the st_nearest_feature() function. id &lt;- st_nearest_feature(voroni, lucas) voroni$EC &lt;- lucas$EC[id] Now we have the final product also shown in the lecture: mapview(voroni, zcol = &quot;EC&quot;) + mapview(lucas, zcol = &quot;EC&quot;, legend = F) 3.3 Trend Surface Analysis Trend surface analysis (TSA) is a multiple regression in which EC is explained by spatial coordinates. We can use the base R regression function lm() for TSA. To prepare the TSA, we need to extract the coordinates from the geometry column of lucas. For this we use the st_coordinates() function. coords &lt;- st_coordinates(lucas) x.coord &lt;- coords[,1] y.coord &lt;- coords[,2] lucas &lt;- mutate(lucas, x = x.coord, y = y.coord ) Then we conduct a simple linear regression and also a polynomial regression with polynomial of second degree. tsa1 &lt;- lm(EC ~ x+y, data = lucas) tsa2 &lt;- lm(EC ~ polym(x, y, degree=2), data = lucas) To predict the values of unobserved locations with the TSA we need the coordinates of these locations. Here, I show to different approaches: i) randomly distributed point in Saxony and ii) regularly spaced points. The randomly distributed points are created with st_sample(). The first argument to this function is the bounding box, i.e., the area in which the point can lie. The second argument is the number of points. Here, we create 300 new points in Saxony. random_points &lt;- st_sample(saxony, 300) mapview(random_points) Again, we have to extract the coordinates from the geometry column with st_coordinates(). Currently the random_points object is still of class sfc. This means it is only a sf column and not a table to which we can add columns. To create a sf table, we first need to use the st_as_sf() function. The geometry columns of sf objects are typically called geom or geometry. However, after calling st_as_sf() the column is called \"x\". We rename it with the rename() function available in the dplyr package. Laslty, we add the coordiantes. random_points &lt;- random_points |&gt; st_as_sf() |&gt; rename(geom = x) |&gt; mutate(x = st_coordinates(random_points)[,1], y = st_coordinates(random_points)[,2]) We predict the EC for these coordinates with predict() random_points$EC_tsa1 &lt;- predict(tsa1, random_points) random_points$EC_tsa2 &lt;- predict(tsa2, random_points) Let’s have a look at the results. On the following map circles are true observaions and diamonds are predicted values. breaks = c(0, 10, 15,20, 25, 30, 35, 40, 170) tmap_mode(&quot;plot&quot;) ## tmap mode set to plotting tm_shape(saxony) + tm_polygons() + tm_shape(random_points) + tm_dots(col = &quot;EC_tsa1&quot;, shape = 23, size = .5, breaks = breaks) + tm_shape(lucas) + tm_dots(col = &quot;EC&quot;, shape = 21, size = .5, breaks = breaks) + tm_layout(legend.outside = TRUE) tm_shape(saxony) + tm_polygons() + tm_shape(random_points) + tm_dots(col = &quot;EC_tsa2&quot;, shape = 23, size = .5, breaks = breaks, midpoint = NA) + tm_layout(legend.outside = TRUE) + tm_shape(lucas) + tm_dots(col = &quot;EC&quot;, shape = 21, size = .5, breaks = breaks) Now to the regularly spaced points. First we need to get the bounding box of saxony. We can get the bounding box with st_bbox(). saxony_bbox &lt;- st_bbox(saxony) saxony_bbox ## xmin ymin xmax ymax ## 4453658 3009238 4672526 3178781 With st_as_sfc() we can create a polygon from the bounding box. saxony_bbox_sfc &lt;- st_as_sfc(saxony_bbox) mapview(saxony_bbox_sfc) + mapview(saxony) With st_make_grid() we can create a grid within the bounding box. In addition to the bounding box, we provide the function with the envisioned cell size. Our data have a projected coordinate reference system (LAEA), therefore the cell size is given in meters. Here we use square cells with a side length of 10 km. saxony_grid &lt;- st_make_grid( x = saxony_bbox_sfc, cellsize = c(1e4,1e4), what = &quot;polygons&quot; ) |&gt; st_as_sf() mapview(saxony_grid) ## Warning in cbind(`Feature ID` = fid, mat): number of rows of result is not a multiple of vector length (arg 1) Now we need to crop the grid to saxony. saxony_grid %&lt;&gt;% st_intersection(st_union(saxony)) mapview(saxony_grid) ## Warning in cbind(`Feature ID` = fid, mat): number of rows of result is not a multiple of vector length (arg 1) Now we have several polygons but not points. Since predictions are made for a singe pair of x and y coordinates and the squares have four pairs we need to extract points from the polygons. There are two possibilities: the corners of the polygons or their centroids. Both can be created with the st_make_grid() function. saxony_grid_corner &lt;- st_make_grid( x = saxony_bbox_sfc, cellsize = c(1e4,1e4), what = &quot;corners&quot;) |&gt; st_as_sf() |&gt; st_intersection(saxony) ## Warning: attribute variables are assumed to be spatially constant throughout all geometries map1 &lt;- mapview(saxony_grid) + saxony_grid_corner ## Warning in cbind(`Feature ID` = fid, mat): number of rows of result is not a multiple of vector length (arg 1) saxony_grid_centroid &lt;- st_make_grid( x = saxony_bbox_sfc, cellsize = c(1e4,1e4), what = &quot;centers&quot;) |&gt; st_as_sf() |&gt; st_intersection(saxony) ## Warning: attribute variables are assumed to be spatially constant throughout all geometries map2 &lt;- mapview(saxony_grid) + saxony_grid_centroid ## Warning in cbind(`Feature ID` = fid, mat): number of rows of result is not a multiple of vector length (arg 1) map1 | map2 ## Lade nötigen Namensraum: leaflet.extras2 We could also have extracted the centroids from the polygons with the following code: saxony_grid_centroid &lt;- saxony_grid |&gt; st_centroid() |&gt; st_as_sf() |&gt; rename(geom = x) Now we can make predictions for the centroids and transfer them back to the grid polygons for visualization. saxony_grid_centroid %&lt;&gt;% mutate( x = st_coordinates(saxony_grid_centroid)[,1], y = st_coordinates(saxony_grid_centroid)[,2] ) saxony_grid_centroid$EC_tsa_1 &lt;- predict(tsa1, saxony_grid_centroid) saxony_grid_centroid$EC_tsa_2 &lt;- predict(tsa2, saxony_grid_centroid) saxony_grid$EC_tsa1 &lt;- saxony_grid_centroid$EC_tsa_1 saxony_grid$EC_tsa2 &lt;- saxony_grid_centroid$EC_tsa_2 mapview(saxony_grid, zcol = &quot;EC_tsa1&quot;) + mapview(lucas, zcol = &quot;EC&quot;, legend = F) mapview(saxony_grid, zcol = &quot;EC_tsa2&quot;) + mapview(lucas, zcol = &quot;EC&quot;, legend = F) 3.4 Splines Next, we have a quick look at interpolation with splines. For this we will need the fields package. We use a sort of spline that is called thin plate spline which can be easily called through the Tps() function. As arguments we provide the coordinates as a matrix and the independent variable (EC). tps_fit &lt;- Tps(x = matrix(c(lucas$x, lucas$y), ncol = 2), Y = lucas$EC) Again, we can use the predict() function to predict the electrical conductivity for unobserved locations. The result is a matrix which we transform to a numeric vector with as.numeric(). tps_prediction &lt;- predict(tps_fit, st_drop_geometry(saxony_grid_centroid[, c(&quot;x&quot;, &quot;y&quot;)])) saxony_grid$EC_tps &lt;- as.numeric(tps_prediction) mapview(saxony_grid, zcol = &quot;EC_tps&quot;) + mapview(lucas, zcol = &quot;EC&quot;, legend = F) 3.5 Weighted average In the rest of the script we cover methods that weight the observed values based on their distance to the predicted location. One method we did not discuss in the lecture is k nearest neighbors. With this method we consider the k nearest points and build their mean value. These k points are all weighted equally. All other points are not considered. For all the weighted average procedures we will use the gstat package. With the epinomous function we create a model. As in lm() regression models, we start with the formula. ~1 indicates that we do not wish to use any explanatory variables but instead estimate the mean value of the dependent variable. With locations we choose the data set that gives the spatial coordinates, nmax gives the maximal number of points to use in any one prediction (k) and idp is the inverse distance parameter. It determines how harshly distanced points are down weighted. Here we choose the five nearest neighbors and no distance weighting, i.e., idp = 0. knn_mod &lt;- gstat(formula=EC~1, locations=lucas, nmax=5, set=list(idp = 0)) knn_pred &lt;- predict(knn_mod, saxony_grid) ## [inverse distance weighted interpolation] mapview(knn_pred, zcol = &quot;var1.pred&quot;) + mapview(lucas, zcol = &quot;EC&quot;, legend = F) For inverse distance weighting (IDW) the model construction and prediction are packaged into a single function. idw1 &lt;- idw(EC ~ 1, locations = lucas, newdata = saxony_grid_centroid, idp = 2) ## [inverse distance weighted interpolation] idw2 &lt;- idw(EC ~ 1, locations = lucas, newdata = saxony_grid_centroid, idp = 3) ## [inverse distance weighted interpolation] saxony_grid$EC_idw_1 &lt;- idw1$var1.pred saxony_grid$EC_idw_2 &lt;- idw2$var1.pred mapview(saxony_grid, zcol = &quot;EC_idw_1&quot;) + mapview(lucas, zcol = &quot;EC&quot;, legend = F) mapview(saxony_grid, zcol = &quot;EC_idw_2&quot;) + mapview(lucas, zcol = &quot;EC&quot;, legend = F) 3.6 Kriging First we compute the empirical variogram. gstat is becoming increasingly compatible with sf but it was originally designed for the predecessor sp. To be on the safe side we will transform all sf objects in to sp objects. Luckily, this can be done with a single function. lucas_sp &lt;- as(lucas, &quot;Spatial&quot;) Now we can plot a variogram cloud and fit the empirical variogram with the variogram() function. v_emp_cloud &lt;- variogram(EC~1, lucas_sp, cloud = T) v_emp &lt;- variogram(EC~1, lucas_sp) As with the idw() function we don’t assume that there is a spatial trend and construct a variogram for a constant mean value. plot(v_emp_cloud) plot(v_emp) To find the best variogram model we can either fit a selection of models manually … # spherical v_mod_sph &lt;- fit.variogram( object = v_emp, model = vgm(&quot;Sph&quot;) ) # exponential v_mod_exp &lt;- fit.variogram( object = v_emp, model = vgm(&quot;Exp&quot;) ) plot(v_emp, v_mod_sph) plot(v_emp, v_mod_exp) … or we use the automap package to automatically fit and compare multiple models. autovar &lt;- autofitVariogram(formula = EC ~ 1, input_data = lucas_sp) With this variogram we can use kriging to interpolate the EC values. kriging &lt;- krige( formula = EC~1, locations = lucas_sp, newdata = saxony_grid_centroid, model = autovar$var_model ) ## [using ordinary kriging] saxony_grid$EC_krige &lt;- kriging$var1.pred saxony_grid$EC_krige_variance &lt;- kriging$var1.var mapview(saxony_grid, zcol = &quot;EC_krige&quot;) + mapview(lucas, zcol = &quot;EC&quot;, legend = F) Lastly, we can look at the kriging variance wish shows our uncertainty for the predicted values. The further we are removed from measured points we larger the uncertainty. mapview(saxony_grid, zcol = &quot;EC_krige_variance&quot;) + mapview(lucas) References "],["machine-learning.html", "Chapter 4 Machine Learning 4.1 Terrain analysis with terra 4.2 Ploting the data 4.3 Logistic regression 4.4 mlr3", " Chapter 4 Machine Learning In this tutorial you will learn how to use R to - fit and predict from logistic generalized linear models - fit and predict from different machine learning algorithms with the mlr3 package (Lang et al. 2019) - conduct spatial and non-spatial cross validation to evaluate the genralizability of your model. Specifically, we will predict the probability of landslides in a small part of the Ecuadorian Andes. As always we start out by loading all the necessary packages. library(data.table) library(dplyr) library(ggplot2) library(magrittr) library(mapview) library(mlr3spatiotempcv) library(mlr3tuning) library(mlr3verse) library(sf) library(spdep) library(statmod) library(terra) library(tmap) The data you need for this exercise consists of a digital elevation model (DEM) for the area (download here) and a data set of landslides locations (download here). dem &lt;- rast(&quot;data/ml_raster.tiff&quot;) lsl &lt;- read.csv(&quot;data/landslides.csv&quot;) Let have a short look at the data. head(lsl) ## x y lslpts slope cplan cprof elev log10_carea ## 1 713887.7 9558537 FALSE 33.75185 0.023180449 0.003193061 2422.810 2.784319 ## 2 712787.7 9558917 FALSE 39.40821 -0.038638908 -0.017187813 2051.771 4.146013 ## 3 713407.7 9560307 FALSE 37.45409 -0.013329108 0.009671087 1957.832 3.643556 ## 4 714887.7 9560237 FALSE 31.49607 0.040931452 0.005888638 1968.621 2.268703 ## 5 715247.7 9557117 FALSE 44.07456 0.009686948 0.005149810 3007.774 3.003426 ## 6 714927.7 9560777 FALSE 29.85981 -0.009047707 -0.005738329 1736.887 3.174073 Here we have 350 observations from the Andes in Ecuador. The data include the initiation points of 175 landslides (lslpts = TRUE). In addition, 175 reference points which are randomly distributed in the sampling area are included. We will try to predict landslides, or determine the probability of a landslide occurring for the whole study area. For these predictions we will use the slope, the plan curvature (cplan), the profile curvature (cprof), the elevation (elev), and the log\\(_{10}\\) of the catchment area (log10_carea). These predictors are already part of the data you loaded. Nonetheless, we will go through the steps with which you can derive them from you elevation raster and add them to your sf data set. The specific variables that are already provided in the data are a little trickier to derive than what we will do. You would need to use a so called bridge from R to other GIS software (in this case SAGA R) to do so, and this goes beyond the scope of this course. If you are interested in this topic and want to give it a try I recommend Chapter 10 of Lovelace, Nowosad, and Muenchow (2019). 4.1 Terrain analysis with terra Conducting a terrain analysis with terra is reasonably easy. There is a single function (terrain()) with which you can compute several variables. The function has four arguments you will want to consider and some more for writing the results to file. x is the DEM in SpatRaster format, the format used by terra. v is the variable or variables you want to compute. You can choose slope, aspect, TPI, TRI, roughness, and flow direction. TRI (Terrain Ruggedness Index) is the mean of the absolute differences between the value of a cell and the value of its 8 surrounding cells. TPI (Topographic Position Index) is the difference between the value of a cell and the mean value of its 8 surrounding cells. Roughness is the difference between the maximum and the minimum value of a cell and its 8 surrounding cells. You can provide multiple values to v by combining them in a vector. neighbors is the number of neighboring cells you want to consider to compute your slope and aspect. You can choose between 4 and 8 also known as rook and queen case after the chess pieces (see Figure 4.1 taken from Lloyd (2010)). Figure 4.1: Queens and Rook case ta1 &lt;- terrain(dem$elev, v = &quot;slope&quot;) ta2 &lt;- terrain(dem$elev, v = c(&quot;slope&quot;, &quot;aspect&quot;, &quot;TPI&quot;)) We can see that the results are again SpatRaster objects. ta2 ## class : SpatRaster ## dimensions : 415, 383, 3 (nrow, ncol, nlyr) ## resolution : 10, 10 (x, y) ## extent : 711962.7, 715792.7, 9556862, 9561012 (xmin, xmax, ymin, ymax) ## coord. ref. : WGS 84 / UTM zone 17S (EPSG:32717) ## source : memory ## names : slope, aspect, TPI ## min values : 0.00000, 4.752037e-04, -21.59433 ## max values : 74.14679, 3.599997e+02, 13.26920 To fit models, we now need to add the new variables to the landslides data. To add the raster values to the sample points we have to convert the points, which at this stage are not yet spatially explicit, into sf format and then to terra's vector format SpatVector with vect(). We create lsl_terrain a copy of lsl which we use for this demonstration only. Afterward, we will fit the models with the original and unaltered lsl. lsl_terrain &lt;- lsl lsl_terrain %&lt;&gt;% st_as_sf(coords = c(&quot;x&quot;, &quot;y&quot;), crs=&quot;EPSG:32717&quot;) %&gt;% vect() We extract the values of the raster cells at the location of the landslide observations with the extract() function. lsl2 &lt;- extract(y = lsl_terrain, x = ta2) Now we can bring the landslide data back to the sf format and combine it with the extracted terrain analysis data. In all of this the variable lslpts is reclassified as character at some point. With the last line of the code below, we turn it back into a bolean. lsl_terrain %&lt;&gt;% st_as_sf() %&gt;% bind_cols(lsl2) %&gt;% mutate(lslpts = as.logical(lslpts)) ## New names: ## • `slope` -&gt; `slope...2` ## • `slope` -&gt; `slope...9` lsl_terrain ## Simple feature collection with 350 features and 10 fields ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: 712197.7 ymin: 9556947 xmax: 715737.7 ymax: 9560807 ## Projected CRS: WGS 84 / UTM zone 17S ## First 10 features: ## lslpts slope...2 cplan cprof elev log10_carea ID slope...9 aspect TPI geometry ## 1 FALSE 33.75185 0.023180449 0.003193061 2422.810 2.784319 1 34.29864 238.95861 0.2219849 POINT (713887.7 9558537) ## 2 FALSE 39.40821 -0.038638908 -0.017187813 2051.771 4.146013 2 38.21037 33.21008 -2.3615265 POINT (712787.7 9558917) ## 3 FALSE 37.45409 -0.013329108 0.009671087 1957.832 3.643556 3 37.70793 312.93348 0.1600342 POINT (713407.7 9560307) ## 4 FALSE 31.49607 0.040931452 0.005888638 1968.621 2.268703 4 30.82579 12.59296 1.4586945 POINT (714887.7 9560237) ## 5 FALSE 44.07456 0.009686948 0.005149810 3007.774 3.003426 5 43.54292 295.99650 0.2507935 POINT (715247.7 9557117) ## 6 FALSE 29.85981 -0.009047707 -0.005738329 1736.887 3.174073 6 31.43264 39.48969 -0.9816742 POINT (714927.7 9560777) ## 7 FALSE 31.57465 0.055624146 0.021838507 2583.551 2.251919 7 32.18023 231.95105 2.2877808 POINT (714287.7 9558367) ## 8 FALSE 53.42223 0.005728012 0.001018965 2522.235 2.583303 8 53.35639 285.90275 0.6058044 POINT (714147.7 9558467) ## 9 FALSE 32.60400 0.024040293 -0.016939975 1929.097 2.836454 9 33.26295 355.05564 -0.5811005 POINT (713717.7 9560657) ## 10 FALSE 37.45409 -0.013329108 0.009671087 1957.832 3.643556 10 37.70793 312.93348 0.1600342 POINT (713407.7 9560307) We can now remove the data as we will continue our work with the origina ones. 4.2 Ploting the data Next we will look at DEM and landslides in parallel. First, we will mask the raster to just the study area. That means we remove all cells of the raster that are not within the study area. Partly, because it looks nicer and partly because it removes the NaN in the raster that would cause problems later on. First, we turn the lsl data into sf format. lsl %&lt;&gt;% st_as_sf(coords = c(&quot;x&quot;, &quot;y&quot;), crs=&quot;EPSG:32717&quot;) Then we derive its convex hull. We need to call st_union() on the points before, because otherwise st_convex_hull() would try to create a separate convex hull for each point. mask &lt;- lsl |&gt; st_union() |&gt; st_convex_hull() When we call the mask function we need to use vect() to transform the mask into the SpatVector class. dem2 &lt;- mask(dem, vect(mask)) Now we can create a nice plot. # create hill shade hs &lt;- shade(slope = dem2$slope * pi / 180, terrain(dem2$elev, v = &quot;aspect&quot;, unit = &quot;radians&quot;)) # tmaptools does not support terra yet. bbx = tmaptools::bb(raster::raster(hs), xlim = c(-0.0001, 1), ylim = c(-0.0001, 1), relative = TRUE) map = tm_shape(hs, bbox = bbx) + tm_grid(col = &quot;black&quot;, n.x = 1, n.y = 1, labels.inside.frame = FALSE, labels.rot = c(0, 90), lines = FALSE) + tm_raster(palette = gray(0:100 / 100), n = 100, legend.show = FALSE) + tm_shape(dem2$elev) + tm_raster(alpha = 0.5, palette = terrain.colors(10), legend.show = FALSE) + tm_shape(lsl) + tm_bubbles(&quot;lslpts&quot;, size = 0.2, palette = &quot;-RdYlBu&quot;, title.col = &quot;Landslide: &quot;) + tm_layout(inner.margins = 0, legend.outside = TRUE) + tm_legend(bg.color = &quot;white&quot;) map 4.3 Logistic regression With the glm() function we can fit a logistic regression model that tries to predict the probability of a landslide with planar curvature, profile curvature, elevation, the decadal logarithm of the cathment area. The call looks similar to the linear regression models you already know except for the family = binomial() argument, which specifies the distribution we assume for the residuals. glm1 = glm(lslpts ~ slope + cplan + cprof + elev + log10_carea, family = binomial(), data = lsl) Just like linear regressions, GLMs have a number of assumptions about the data that need to be met in order for the model to be meaningful. These assumptions are: 1. correct distribution function, 2. correct link function, 3. linearity, 4. lack of outliers, and 5. independence of observations. Your first step after fitting a model should always be to see if it fits the data well and if assumptions are met. Some of them can be checked in tandem: link function, linearity and distribution would, if wrongly specified, all lead to patterns in the residuals that we can investigate with residual plots. For linear models we use the response residuals, i.e. the difference between the observed point and the predicted point \\(y_i - \\hat{\\mu}\\). For GLMs this is not possible, because the variance of the residuals changes with the mean. That means we assume that a pattern in the residuals and visually testing whether our data deviate from the expected pattern is a lot harder than looking for any pattern at all. Thus we use other types of residuals which should indeed show not pattern but are a little bit more difficult to compute. The details of this was discussed in the lecutre and plenty of material is provided in the presentation notes. Here we will use Quantile residuals which we can compute with the statmodpackage (Dunn and Smyth 1996). quantile_residuals &lt;- statmod::qresiduals(glm1) plot(quantile_residuals ~ glm1$fitted.values) abline(h = mean(quantile_residuals)) investigating the QQ Plot (points should be on diagonal line), or qqnorm(quantile_residuals); qqline(quantile_residuals, col = 2) residuals versus row number (again optimally no pattern). scatter.smooth(quantile_residuals) Outliers or influential observations can be identified with Cook’s distance. This metric gives the degree to which the model is altered by removing any one observation. Larger values imply larger changes, i.e. a larger influence on the model. There are some rule of thumbs what values are considered problematic: \\(4/(n-p-1)\\) (Fox 2002) or 1 (Braun 2018). Here, we simply can see that all values are below these thresholds. Nonetheless, we have a look at the largest value. How do regression parameters change if we drop this observation. lsl.cd &lt;- cooks.distance(glm1) plot(lsl.cd) infl &lt;- which.max(lsl.cd) glm2 &lt;- update(glm1, subset = (-infl)) coef(glm1) ## (Intercept) slope cplan cprof elev log10_carea ## 2.511364e+00 7.901064e-02 -2.894196e+01 -1.756360e+01 1.789238e-04 -2.274877e+00 coef(glm2) ## (Intercept) slope cplan cprof elev log10_carea ## 2.642992e+00 8.082282e-02 -3.003641e+01 -8.765858e+00 1.641055e-04 -2.342504e+00 Parameters barely change. Our data are spatial and spatial data often contain spatial autocorrelation. If our data would be spatially auto correlated the assumption of independent observations would be violated. We can check the Moran’s I of the residuals. knn &lt;- knearneigh(lsl, k = 1) nb &lt;- knn2nb(knn) listw &lt;- nb2listw(nb) quantile_residuals &lt;- qres.binom(glm1) moran.test(x = quantile_residuals, listw = listw) ## ## Moran I test under randomisation ## ## data: quantile_residuals ## weights: listw ## ## Moran I statistic standard deviate = 2.793, p-value = 0.002611 ## alternative hypothesis: greater ## sample estimates: ## Moran I statistic Expectation Variance ## 0.188820955 -0.002865330 0.004710304 Indeed, there seems to be a weak but statistically significant autocorrelation in our residuals. At this point we will not address possible fixes for this problem as they extend beyond the scope of this lecture. With summary() we get a quick overview of the results. summary(glm1) ## ## Call: ## glm(formula = lslpts ~ slope + cplan + cprof + elev + log10_carea, ## family = binomial(), data = lsl) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.91888 -0.86223 0.09279 0.86374 2.76299 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 2.511e+00 2.035e+00 1.234 0.217 ## slope 7.901e-02 1.506e-02 5.248 1.54e-07 *** ## cplan -2.894e+01 4.746e+00 -6.098 1.07e-09 *** ## cprof -1.756e+01 1.083e+01 -1.622 0.105 ## elev 1.789e-04 5.492e-04 0.326 0.745 ## log10_carea -2.275e+00 4.848e-01 -4.692 2.70e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 485.20 on 349 degrees of freedom ## Residual deviance: 372.83 on 344 degrees of freedom ## AIC: 384.83 ## ## Number of Fisher Scoring iterations: 4 Using the model we can predict the probability of a landslide for each cell of the raster terrain_analysis. pred_glm &lt;- terra::predict(object = dem, model = glm1, type = &quot;response&quot;) map = tm_shape(hs, bbox = bbx) + tm_grid(col = &quot;black&quot;, n.x = 1, n.y = 1, labels.inside.frame = FALSE, labels.rot = c(0, 90), lines = FALSE) + tm_raster(palette = gray(0:100 / 100), n = 100, legend.show = FALSE) + tm_shape(mask(pred_glm, vect(mask))) + tm_raster(alpha = 0.5, palette = &quot;Reds&quot;, n = 6, legend.show = TRUE, title = &quot;Probability of a Landslide: &quot;) + tm_layout(inner.margins = 0, legend.outside = TRUE) + tm_legend(bg.color = &quot;white&quot;) map A common metric to evaluate the predictive capacity of a model is the area under the receiver operating characteristic curve (AUROC or ROC). This is a value between 0.5 and 1.0, with 0.5 indicating a model that is no better than random and 1.0 indicating perfect prediction of the two classes. Thus, the higher the AUROC, the better the model’s predictive power. The following code chunk computes the AUROC value of the model with roc(), which takes the response and the predicted values as inputs. auc() returns the area under the curve. pROC::auc(pROC::roc(lsl$lslpts, fitted(glm1))) ## Area under the curve: 0.8216 4.4 mlr3 Now we will turn to the machine learning technique random forest. There are different frameworks for machine learning in R. We will focus on mlr3 (Lang et al. 2019) which is a versatile and popular framework. mlr3 follows a logic which is shown in Figure 4.2. If your looking for an in depth introduction to the package, you can find a book length introduction to mlr3 here. Figure 4.2: flow diagram of mlr3 4.4.1 Creating a task First we need to create a task. A task contains the data as well as some information on how we want to model the data, like the column name of the dependent variable. There are different types of tasks which differ in the kinds of dependent variables they support. For example, classification tasks are for cases where our dependent variable consists of binary or nominal data. A regression task is designed for continuous numeric quantities. The mlr3spatiotempcv(Schratz and Becker 2022) package introduced a special spatial task type we will look at later. Here we will create a classification task for our landslides data. This is the optimal task type because the dependent variable is binary (landslide or no landslide). First we need to turn the lslpts column into a factor column. For this first example we will drop the geometry column and add the coordinates as individual columns. lsl2 &lt;- st_drop_geometry(lsl) lsl2 %&lt;&gt;% mutate(x = st_coordinates(lsl)[,1], y = st_coordinates(lsl)[,2], lslpts = factor(lslpts)) The spatial classification task is defined by TaskClassifST$new(). The function takes the argument backend, the data set, target, the name of the dependent variable, and id, the name of a column the can be used to identify each observation. Additionally we provide the names of the coordinate columns (coordinate_names), tell the model not to use the coordinates as features (coords_as_features = FALSE) and provide the coordinate reference system (crs). task = mlr3spatiotempcv::TaskClassifST$new( id = &quot;ecuador_lsl&quot;, backend = mlr3::as_data_backend(lsl2), target = &quot;lslpts&quot;, positive = &quot;TRUE&quot;, coordinate_names = c(&quot;x&quot;, &quot;y&quot;), coords_as_features = FALSE, crs = &quot;EPSG:32717&quot; ) The new object has the class TaskClassifST and we can get a short summary of the tasks if we print it to the console. class(task) ## [1] &quot;TaskClassifST&quot; &quot;TaskClassif&quot; &quot;TaskSupervised&quot; &quot;Task&quot; &quot;R6&quot; print(task) ## &lt;TaskClassifST:ecuador_lsl&gt; (350 x 6) ## * Target: lslpts ## * Properties: twoclass ## * Features (5): ## - dbl (5): cplan, cprof, elev, log10_carea, slope ## * Coordinates: ## x y ## 1: 713887.7 9558537 ## 2: 712787.7 9558917 ## 3: 713407.7 9560307 ## 4: 714887.7 9560237 ## 5: 715247.7 9557117 ## --- ## 346: 714877.2 9558362 ## 347: 714909.5 9558581 ## 348: 713713.6 9558849 ## 349: 715253.2 9558797 ## 350: 713825.6 9559078 The task registers all variables that are not the target as predictors or, as they are commonly called in the machine learning literature, features. We want to use all six features here but in case we would just want to a subset this would be done with the following code task$select(c(&quot;slope&quot;, &quot;cplan&quot;)) We can use the autoplot() function to get a visual summary of the data. autoplot(task, type = &quot;pairs&quot;) 4.4.2 The learner The learner includes the machine learning algorithm we want to use as well as some information on hyperparameters. Hyperparameters are parameters that you have to determine before running the model. They are not estimated in the fitting procedure. Think of them as different settings for the methods. We will encounter some hyperparameters and discuss how you should choose them later. The learner works in a two-stage procedure: First, a randomly selected subset of the data (the training set, see Fig. 4.2) is used to train the specified algorithm and the trained model is stored in the learner. Second, the trained model is used to predict the target in the test set (i.e. all observations that are not in the training set). The predictions can be compared to the actual values to determine how well the model fares on data it has not seen before. This is also known as cross-validation. If a model performs well in cross-validation we can have a higher believe that it might generalize to unobserved data from the same context (e.g. landslide probability in the area). However, cross validation can not inform us on transferability, i.e., whether the model is adequate to estimate landslide probabilities in other regions of the world. Cross validation usually splits in more than just two groups. The number of groups in a CV is called folds. With five folds we have five equally sized groups. The model is fit on four of the groups and predicts the fifth group. This is repeated for each group. Then the distribution of observations into groups is repeated. In the example below, we split the data into five groups 30 times. Please note that 30 is a rather low number of repetitions, I choose it to keep computational load light for this demonstration. In real world applications you should consider using more repetitions. # non spatial resampling approach ns_resampling &lt;- rsmp(&quot;repeated_cv&quot;, folds = 5, repeats = 30) CV assumes that the observations are independent which, as we have seen, is not the case for our data. A way to met this assumption is to use spatial or blocked CV. Instead of randomly selection point for the folds, all the points in a fold will be close to each other (see Figure 4.3 as an example). Figure 4.3: Spatial vs non-spatial cross validation Thanks to mlr3spatiotempcv (Schratz and Becker 2022) we can select such blocked cross validation approaches as resampling scheme. # spatial resampling approach sp_resampling = rsmp(&quot;repeated_spcv_coords&quot;, folds = 5, repeats = 30) mlr3 contains many algorithms to choose for you learner. A complete overview can be found here. We want to choose three different learners here: A logistic GLM A Random Forest A support vector machine The random forest algorithm is based on the implementation in the ranger package (Wright and Ziegler 2017) and the support vector machine on the implementation in the e1071 package (Meyer et al. 2022). In the codeblock below, we also define fallback learners that are used if the original learner (random forest or support vector machine) return an error. The featureless classifier does not use any features and simply always predicts the more common result. lrnr_glm &lt;- lrn(&quot;classif.log_reg&quot;, predict_type = &quot;prob&quot;) lrnr_rf &lt;- lrn(&quot;classif.ranger&quot;, predict_type = &quot;prob&quot;) lrnr_svm &lt;- lrn(&quot;classif.svm&quot;, predict_type = &quot;prob&quot;) lrnr_rf$fallback &lt;- lrn(&quot;classif.featureless&quot;, predict_type = &quot;prob&quot;) lrnr_svm$fallback &lt;- lrn(&quot;classif.featureless&quot;, predict_type = &quot;prob&quot;) Now we can run the resampling. We will run spatial and non spatial resampling for all three learners and see how their results differ. ## non spatial CV rr_nscv_glm &lt;- mlr3::resample(task = task, learner = lrnr_glm, resampling = ns_resampling) rr_nscv_rf &lt;- mlr3::resample(task = task, learner = lrnr_rf, resampling = ns_resampling) rr_nscv_svm &lt;- mlr3::resample(task = task, learner = lrnr_svm, resampling = ns_resampling) ## with spatial CV rr_spcv_glm &lt;- mlr3::resample(task = task, learner = lrnr_glm, resampling = sp_resampling) rr_spcv_rf &lt;- mlr3::resample(task = task, learner = lrnr_rf, resampling = sp_resampling) rr_spcv_svm &lt;- mlr3::resample(task = task, learner = lrnr_svm, resampling = sp_resampling) We can extract the AUROC and the Brier score from the score element of the resampling object. For a full list of available performance measures see here. auroc_nsp_glm &lt;- rr_nscv_glm$score(measure = mlr3::msr(&quot;classif.auc&quot;)) auroc_nsp_rf &lt;- rr_nscv_rf$score(measure = mlr3::msr(&quot;classif.auc&quot;)) auroc_nsp_svm &lt;- rr_nscv_svm$score(measure = mlr3::msr(&quot;classif.auc&quot;)) auroc_sp_glm &lt;- rr_spcv_glm$score(measure = mlr3::msr(&quot;classif.auc&quot;)) auroc_sp_rf &lt;- rr_spcv_rf$score(measure = mlr3::msr(&quot;classif.auc&quot;)) auroc_sp_svm &lt;- rr_spcv_svm$score(measure = mlr3::msr(&quot;classif.auc&quot;)) brier_nsp_glm &lt;- rr_nscv_glm$score(measure = mlr3::msr(&quot;classif.bbrier&quot;)) brier_nsp_rf &lt;- rr_nscv_rf$score(measure = mlr3::msr(&quot;classif.bbrier&quot;)) brier_nsp_svm &lt;- rr_nscv_svm$score(measure = mlr3::msr(&quot;classif.bbrier&quot;)) brier_sp_glm &lt;- rr_spcv_glm$score(measure = mlr3::msr(&quot;classif.bbrier&quot;)) brier_sp_rf &lt;- rr_spcv_rf$score(measure = mlr3::msr(&quot;classif.bbrier&quot;)) brier_sp_svm &lt;- rr_spcv_svm$score(measure = mlr3::msr(&quot;classif.bbrier&quot;)) These tables contain more columns than we need. We drop the unnecessary columns to get clearer table. We also prepare the table for joining them later. auroc_nsp_glm %&lt;&gt;% dplyr::select(resampling_id, value = classif.auc) %&gt;% dplyr::mutate (measure = &quot;auroc&quot;, model = &quot;glm&quot;) auroc_nsp_rf %&lt;&gt;% dplyr::select(resampling_id, value = classif.auc) %&gt;% dplyr::mutate (measure = &quot;auroc&quot;, model = &quot;rf&quot;) auroc_nsp_svm %&lt;&gt;% dplyr::select(resampling_id, value = classif.auc) %&gt;% dplyr::mutate (measure = &quot;auroc&quot;, model = &quot;svm&quot;) auroc_sp_glm %&lt;&gt;% dplyr::select(resampling_id, value = classif.auc) %&gt;% dplyr::mutate (measure = &quot;auroc&quot;, model = &quot;glm&quot;) auroc_sp_rf %&lt;&gt;% dplyr::select(resampling_id, value = classif.auc) %&gt;% dplyr::mutate (measure = &quot;auroc&quot;, model = &quot;rf&quot;) auroc_sp_svm %&lt;&gt;% dplyr::select(resampling_id, value = classif.auc) %&gt;% dplyr::mutate (measure = &quot;auroc&quot;, model = &quot;svm&quot;) brier_nsp_glm %&lt;&gt;% dplyr::select(resampling_id, value = classif.bbrier) %&gt;% dplyr::mutate (measure = &quot;brier&quot;, model = &quot;glm&quot;) brier_nsp_rf %&lt;&gt;% dplyr::select(resampling_id, value = classif.bbrier) %&gt;% dplyr::mutate (measure = &quot;brier&quot;, model = &quot;rf&quot;) brier_nsp_svm %&lt;&gt;% dplyr::select(resampling_id, value = classif.bbrier) %&gt;% dplyr::mutate (measure = &quot;brier&quot;, model = &quot;svm&quot;) brier_sp_glm %&lt;&gt;% dplyr::select(resampling_id, value = classif.bbrier) %&gt;% dplyr::mutate (measure = &quot;brier&quot;, model = &quot;glm&quot;) brier_sp_rf %&lt;&gt;% dplyr::select(resampling_id, value = classif.bbrier) %&gt;% dplyr::mutate (measure = &quot;brier&quot;, model = &quot;rf&quot;) brier_sp_svm %&lt;&gt;% dplyr::select(resampling_id, value = classif.bbrier) %&gt;% dplyr::mutate (measure = &quot;brier&quot;, model = &quot;svm&quot;) Now we combine the two auroc data sets and the two brier score data sets. results_cross_validation &lt;- bind_rows( auroc_nsp_glm, auroc_nsp_rf, auroc_nsp_svm, auroc_sp_glm, auroc_sp_rf, auroc_sp_svm, brier_nsp_glm, brier_nsp_rf, brier_nsp_svm, brier_sp_glm, brier_sp_rf, brier_sp_svm ) We display the results using violin plots. They are similar to boxplots, the line in the middle indicates the median. However instead of an uninformative box we get the distribution of values as a shape. Remember the higher the AUC the better and the lower the Brier score to better the model. ggplot(results_cross_validation, aes(y = value, x = resampling_id)) + geom_violin(draw_quantiles = .5) + geom_jitter(alpha = 0.1, height = 0, width = 0.03) + facet_wrap(model~ measure, scales = &quot;free&quot;) 4.4.3 Hyperparameter Tuning We will use a spatial nested cross validation scheme to tune the hyperparameters of the random forest. As in the cross validation example above, we will use a relatively small numbers of folds and repetitions just to keep the computational demand reasonable. Please note, that for real analyses you should use more folds and repetitions. Try to find norms in your respective filed of study from published papers and see if performance estimates stabilize at a certain number of folds and repetition. For the hyperparameter tuning we first establish the search space, that is the space in which we look for possible parameter values. This is done with ps() function from the paradox package (Lang et al. 2022). Here, we tune the parameter max.depth, that is the maximal depth, the number of splits, in a single tree. Low depth leads to underfitting, large depth to overfitting. We tell the ps() function that the value should be an integer (p_int(), enter paradox::p_ into the console to see alternatives) between 1 and 10. search_space = ps(max.depth = p_int(lower = 1, upper = 10)) We also specify the resampling scheme, the performance measure, and when the cross validation should end. resampling = rsmp(&quot;spcv_coords&quot;) measure = msr(&quot;classif.auc&quot;) terminator = trm(&quot;evals&quot;, n_evals = 30) With all of those, we specify a tuning algorithm for one criterion with TuningInstanceSingleCrit$new(). instance = TuningInstanceSingleCrit$new( task = task, learner = lrnr_rf, resampling = resampling, measure = measure, search_space = search_space, terminator = terminator ) We specify the number of hyperparemeter vaules to evaluate in the tnr() function with the resolution argument. tuner = tnr(&quot;grid_search&quot;, resolution = 10) tuner$optimize(instance) parameter_tuning_results &lt;- as.data.table(instance$archive) Lets have a look at the results. ggplot(parameter_tuning_results, aes(y = classif.auc, x = max.depth)) + geom_line() + geom_point() + geom_point(data =filter(parameter_tuning_results, classif.auc == min(classif.auc)), col = &quot;red&quot;, size = 4) + geom_label(data =filter(parameter_tuning_results, classif.auc == min(classif.auc)), aes(label = max.depth), nudge_x = .5) Our hyperparameter tuning determined that four is the optimal maximal depth for the trees. We can also tune multiple parameters at the same time. Here we additionally tune the minimum nodes size, i.e., the minimal number of observations that a single node should have. search_space = ps(max.depth = p_int(lower = 1, upper = 100), min.node.size = p_int(lower = 1, upper = 100)) instance = TuningInstanceSingleCrit$new( task = task, learner = lrnr_rf, resampling = resampling, measure = measure, search_space = search_space, terminator = terminator ) tuner = tnr(&quot;grid_search&quot;, resolution = 30) tuner$optimize(instance) parameter_tuning_results &lt;- as.data.table(instance$archive) This is a little bit more difficult to visualize. In the following plot each axis is one hyperparameter. Larger circles and brighter color indicate a higher AUC. The combination with the highest AUC is marked with a red dot. ggplot(parameter_tuning_results, aes(x = max.depth, y = min.node.size)) + geom_point(aes(fill = classif.auc, size = classif.auc), shape = 21) + geom_point( data = filter(parameter_tuning_results, classif.auc == max(classif.auc)), col = &quot;red&quot;, size = 3 ) + theme(legend.position = &quot;none&quot;) We can extract the optimal solution with instance$result_learner_param_vals ## $num.threads ## [1] 1 ## ## $max.depth ## [1] 100 ## ## $min.node.size ## [1] 11 and set the parameters accordingly. lrnr_rf$param_set$values$max.depth &lt;- 1 lrnr_rf$param_set$values$min.node.size &lt;- 32 Now we turn to nested cross validation. We start out by defining the inner resampling. Most of the functions here we have seen before. The only new one is the AutoTuner$new() function. The AutoTuner is a learner which wraps another learner (in our case lrnr_rf). Wrapping here means that it covers it and calls it when it is called. A wrapping function is a function that calls another function. So the AutoTuner calls our learner and tunes its hyperparameters with the specified resampling procedure, search space, terminator, tuner, and measure. The best hyperparameters are set as parameters for a final model which is fit to the full data. resampling = rsmp(&quot;spcv_coords&quot;, folds = 4) measure = msr(&quot;classif.auc&quot;) terminator = trm(&quot;evals&quot;, n_evals = 5) tuner = tnr(&quot;grid_search&quot;, resolution = 10) at = AutoTuner$new(lrnr_rf, resampling, measure, terminator, tuner, search_space) Now we can pass this inner to a resampling scheme for the outer resampling. outer_resampling = rsmp(&quot;spcv_coords&quot;, folds = 3) rr = mlr3::resample(task, at, outer_resampling, store_models = TRUE) The aggregated performance over all nested instances can be determined with: mean(rr$score(measure = msr(&quot;classif.auc&quot;))$classif.auc) ## [1] 0.7942331 4.4.4 Predictions We can also use the AutoTuner to fit the final model we want to use for prediction. at$train(task) If you do not want to use the AutoTuner but instead train the model without tuning the hyperparameters with hyperparamters values you determined before you can use: lrnr_rf$train(task) at$model ## $learner ## &lt;LearnerClassifRanger:classif.ranger&gt; ## * Model: ranger ## * Parameters: num.threads=1, max.depth=34, min.node.size=45 ## * Packages: mlr3, mlr3learners, ranger ## * Predict Types: response, [prob] ## * Feature Types: logical, integer, numeric, character, factor, ordered ## * Properties: hotstart_backward, importance, multiclass, oob_error, twoclass, weights ## ## $tuning_instance ## &lt;TuningInstanceSingleCrit&gt; ## * State: Optimized ## * Objective: &lt;ObjectiveTuning:classif.ranger_on_ecuador_lsl&gt; ## * Search Space: ## id class lower upper nlevels ## 1: max.depth ParamInt 1 100 100 ## 2: min.node.size ParamInt 1 100 100 ## * Terminator: &lt;TerminatorEvals&gt; ## * Result: ## max.depth min.node.size classif.auc ## 1: 34 45 0.748318 ## * Archive: ## max.depth min.node.size classif.auc ## 1: 34 45 0.7483180 ## 2: 12 1 0.7321165 ## 3: 67 1 0.7301726 ## 4: 34 34 0.7440140 ## 5: 12 34 0.7480052 lrnr_rf$model ## Ranger result ## ## Call: ## ranger::ranger(dependent.variable.name = task$target_names, data = task$data(), probability = self$predict_type == &quot;prob&quot;, case.weights = task$weights$weight, num.threads = 1L, max.depth = 1L, min.node.size = 32L) ## ## Type: Probability estimation ## Number of trees: 500 ## Sample size: 350 ## Number of independent variables: 5 ## Mtry: 2 ## Target node size: 32 ## Variable importance mode: none ## Splitrule: gini ## OOB prediction error (Brier s.): 0.2114862 We can use the predict landslide probability for the whole region. ta2 &lt;- data.table(slope = values(dem$slope)[,1], cplan = values(dem$cplan)[,1], cprof = values(dem$cprof)[,1], log10_carea = values(dem$log10_carea)[,1], elev = values(dem$elev)[,1]) ta2[is.na(slope), c(&quot;slope&quot;, &quot;cplan&quot;, &quot;cprof&quot;, &quot;log10_carea&quot;, &quot;elev&quot;) := -10] # predict new values y &lt;- at$predict_newdata(ta2) x &lt;- lrnr_rf$predict_newdata(ta2) # replace predictions for -10 placeholders y &lt;- as.data.frame(y$data$prob) y[which(ta2$slope == -10), &quot;TRUE&quot;] &lt;- NA x &lt;- as.data.frame(x$data$prob) x[which(ta2$slope == -10), &quot;TRUE&quot;] &lt;- NA dem$prediction_rf &lt;- x$&#39;TRUE&#39; dem3 &lt;- terra::mask(dem, vect(mask)) Create map with predictions. map = tm_shape(hs, bbox = bbx) + tm_grid(col = &quot;black&quot;, n.x = 1, n.y = 1, labels.inside.frame = FALSE, labels.rot = c(0, 90), lines = FALSE) + tm_raster(palette = gray(0:100 / 100), n = 100, legend.show = FALSE) + tm_shape(dem3$prediction_rf) + tm_raster(alpha = 0.5, palette = &quot;Reds&quot;, n = 6, legend.show = TRUE, title = &quot;Probability of a Landslide: &quot;) + tm_layout(inner.margins = 0, legend.outside = TRUE) + tm_legend(bg.color = &quot;white&quot;) map References "],["graph-analysis.html", "Chapter 5 Graph Analysis 5.1 The igraph package 5.2 From data to graph 5.3 Centrality measures 5.4 Visualization 5.5 Network Models 5.6 Optimal Channel Networks 5.7 sfnetworks", " Chapter 5 Graph Analysis In this script, we will work with network data in R. To this end will, we will use different packages: igraph, tidygraph, and network for representing networks in R through specific object classes. OCN for the creation of optimal channel networks, and sfnetworks for spatial networks. 5.1 The igraph package We will start with the igraphpackage (Csardi and Nepusz 2006). It is a very popular implementation of networks in R, which is also available for Python and C++. library(dplyr) library(purrr) ## ## Attache Paket: &#39;purrr&#39; ## Das folgende Objekt ist maskiert &#39;package:data.table&#39;: ## ## transpose ## Das folgende Objekt ist maskiert &#39;package:magrittr&#39;: ## ## set_names library(magrittr) library(igraph) ## ## Attache Paket: &#39;igraph&#39; ## Die folgenden Objekte sind maskiert von &#39;package:purrr&#39;: ## ## compose, simplify ## Das folgende Objekt ist maskiert &#39;package:raster&#39;: ## ## union ## Die folgenden Objekte sind maskiert von &#39;package:terra&#39;: ## ## blocks, union ## Die folgenden Objekte sind maskiert von &#39;package:dplyr&#39;: ## ## as_data_frame, groups, union ## Die folgenden Objekte sind maskiert von &#39;package:spatstat.geom&#39;: ## ## diameter, edges, is.connected, vertices ## Die folgenden Objekte sind maskiert von &#39;package:stats&#39;: ## ## decompose, spectrum ## Das folgende Objekt ist maskiert &#39;package:base&#39;: ## ## union First, we create a simple graph with three nodes (n). Thee first node is connected to the second node, the second to the third, and the third to the first. This specified in the edges argument, which takes a vector. Each pair of entries in the vector gives the number or name of the originating and the terminating node. For the network I described above this works out to c(1,2,2,3,3,1). g &lt;- graph(edges = c(1,2,2,3,3,1), n = 3) plot(g) The base plot function returns an image where nodes are represented by orange circles with their names inscribed. The edges are arrows, so we know that the default option of the graph() function is to create directed graphs. We can create a non-directed network by setting directed=FALSE. g &lt;- graph(edges = c(1,2,2,3,3,1), n = 3, directed = FALSE) plot(g) If we have more nodes in the network than are indicated in the edge list, the additional nodes are unconnected. g &lt;- graph(edges = c(1,2,2,3,3,1), n = 6, directed = FALSE) plot(g) We can also create complete, undirected graphs that form predefined geometries like tetrahedrons. See the help file of make_graph for more geometries. g &lt;- make_graph(&quot;Tetrahedron&quot;) plot(g) A further alternative is to use a literal description of the graph, where named nodes are connected via -. g &lt;- graph_from_literal(Fred-Daphne, Velma-Shaggy, Shaggy-Fred) plot(g) We can see that - is an undirected edge. You can use as many - as you like, which might improve legibility in some cases. g &lt;- graph_from_literal(Fred--Daphne, Velma----Shaggy, Shaggy----Fred) plot(g) The : colon operator links vertex sets, so that all vertices from both sets are connected. g &lt;- graph_from_literal(Fred-Daphne:Velma-Shaggy, Fred-Shaggy-Scooby) plot(g) In directed graphs, you can indicate the direction with -+ where the + marks the head of the arrow. g &lt;- graph_from_literal(Fred-+Daphne:Velma+-Shaggy, Fred+-Shaggy+-+Scooby) plot(g) Now lets have a look at how the graph is represented non-graphically to us: g ## IGRAPH 415271c DN-- 5 7 -- ## + attr: name (v/c) ## + edges from 415271c (vertex names): ## [1] Fred -&gt;Daphne Fred -&gt;Velma Shaggy-&gt;Fred Shaggy-&gt;Daphne Shaggy-&gt;Velma Shaggy-&gt;Scooby Scooby-&gt;Shaggy The output always starts with IGRAPH, telling us the we have an igraph object. Next, is a seven character string. For me, while I write this, it is c98a3fb. For you it will be different. It will even be different when you read this, because the number changes every time I compile this document. This character string is the ID of the graph. After the ID follow four letters: 1. D or U for directed or undirected 2. N or - for named or unnamed vertices 3. W or - for weighted or unweighted edges 4. B or - for bipartite and unipartite networks. After the four letter code, we get the number of nodes (5) and the number of edges (7, the edge between Shaggy and Scooby is counted double). If the whole graph has a name that is printed after the the numbers. All of these properties can also be queried independently. # number of nodes vcount(g) ## [1] 5 # number of edges ecount(g) ## [1] 7 # is the graph directed? is.directed(g) ## [1] TRUE # is the graph bipartite is.bipartite(g) ## [1] FALSE # is the graph weighted is.weighted(g) ## [1] FALSE # is the graph named is.named(g) ## [1] TRUE Below that the attributes are given under +attr :. For our graph, we get one attribute: name(v/c). The code after the attribute name (in our case the name is name), tells us whether the attribute is of a node (v), edge (e), or whole graph (g) and whether the attribute is a character (c), numeric (n), logical (l), or other (x). So our attribute names concerns nodes and is a character. Lastly, the edges are listed. We can change the attributes of nodes by accessing the nodes of the graph with V(). Then we can change and add attributes in the following way: # name the nodes Bill, Joe, Josy, Laura, and Tyler V(g)$name &lt;- c(&quot;Bill&quot;, &quot;Joe&quot;, &quot;Josy&quot;, &quot;Laura&quot;, &quot;Tyler&quot;) We can see the change in the summary and the plot. g ## IGRAPH 415271c DN-- 5 7 -- ## + attr: name (v/c) ## + edges from 415271c (vertex names): ## [1] Bill -&gt;Joe Bill -&gt;Josy Laura-&gt;Bill Laura-&gt;Joe Laura-&gt;Josy Laura-&gt;Tyler Tyler-&gt;Laura plot(g) We can also add a new attribute called math grade. V(g)$math_grade &lt;- c(4,2,3,1,1) g ## IGRAPH 415271c DN-- 5 7 -- ## + attr: name (v/c), math_grade (v/n) ## + edges from 415271c (vertex names): ## [1] Bill -&gt;Joe Bill -&gt;Josy Laura-&gt;Bill Laura-&gt;Joe Laura-&gt;Josy Laura-&gt;Tyler Tyler-&gt;Laura cols &lt;- c(&quot;blue&quot;,&quot;red&quot;,&quot;black&quot;,&quot;magenta&quot;) plot(g, vertex.color = cols[V(g)$math_grade]) We can access the edges in the same way. E(g) ## + 7/7 edges from 415271c (vertex names): ## [1] Bill -&gt;Joe Bill -&gt;Josy Laura-&gt;Bill Laura-&gt;Joe Laura-&gt;Josy Laura-&gt;Tyler Tyler-&gt;Laura E(g)$likes &lt;- sample(1:4, ecount(g), replace = TRUE) plot(g, edge.color = cols[E(g)$likes]) 5.2 From data to graph In many instances you start out with an adjacency matrix or an edge list which we might want to turn into a graph. To illustrate how this works, I will simulate an example for each of those data types. First, we create an adjacency matrix. Remember, an adjacency matrix is a matrix filled with zeros and ones. A one in the j\\(^{th}\\) column of the i\\(^{th}\\) row implies that the j\\(^{th}\\) node is connected to the i\\(^{th}\\) node, while a zero would imply that they are not connected. Here, we simulate a 20x20 matrix that is randomly populated with zeros and ones. For this, we use random draws from the binomial distribution rbinom() . The binomial distribution is what you might use to simulate a coin toss or any other event that can lead to two different outcomes in each trial. In this case, we draw 400 numbers (20^2), each draw only consist of one trail and a likelihood of success (i.e. a 1) of 0.1. adjacency &lt;- matrix(rbinom(20^2, 1, .1), ncol = 20) # turn all values on the diagonal to zero as a node is not connected to itself. diag(adjacency) &lt;- 0 ga &lt;- graph_from_adjacency_matrix(adjacency, mode = &quot;undirected&quot;) plot(ga) We can also go the other way around and turn a graph into a adjacency matrix. as_adjacency_matrix(ga) ## 20 x 20 sparse Matrix of class &quot;dgCMatrix&quot; ## ## [1,] . . . 1 . . . . . . . . 1 . . 1 . . . . ## [2,] . . 1 . 1 . . 1 1 . . . . . . . . . . . ## [3,] . 1 . . . . . . . . 1 . . . . . . . . . ## [4,] 1 . . . . 1 . . . . . 1 . . . . . . . . ## [5,] . 1 . . . . . . 1 1 . . . . . . . . . . ## [6,] . . . 1 . . 1 . . . . . . 1 . . . . 1 . ## [7,] . . . . . 1 . 1 . . . . . 1 . 1 . . 1 . ## [8,] . 1 . . . . 1 . . . . . 1 1 1 . . . . . ## [9,] . 1 . . 1 . . . . 1 . 1 1 . . . . 1 1 . ## [10,] . . . . 1 . . . 1 . . 1 . . . . 1 . . . ## [11,] . . 1 . . . . . . . . 1 . . . . . 1 . . ## [12,] . . . 1 . . . . 1 1 1 . . . . . . . . . ## [13,] 1 . . . . . . 1 1 . . . . . 1 . . . 1 1 ## [14,] . . . . . 1 1 1 . . . . . . . . 1 . . . ## [15,] . . . . . . . 1 . . . . 1 . . 1 . . . . ## [16,] 1 . . . . . 1 . . . . . . . 1 . . . . . ## [17,] . . . . . . . . . 1 . . . 1 . . . . . . ## [18,] . . . . . . . . 1 . 1 . . . . . . . 1 1 ## [19,] . . . . . 1 1 . 1 . . . 1 . . . . 1 . . ## [20,] . . . . . . . . . . . . 1 . . . . 1 . . You see that the format is unusual for matrices in R. As it says above the output, this is a sparse matrix. Most entries are zero and therefore zeros are not explicitly represented in the matrix to save computer memory. Next, we will create an edge list and derive a graph from it. letters is a vector that contains the alphabet we and take a sample of 20 letters with sample(). The replace = TRUE argument enables us to sample the same letter multiple time. After we have two vectors with randomly drawn letters (letters1 and letters2) we use them as columns in a matrix with the cbind() function. This matrix is the edge list with which we can create a graph using the graph_from_edgelist() function. letters1 &lt;- sample(letters, 20, replace = TRUE) letters2 &lt;- sample(letters, 20, replace = TRUE) el &lt;- cbind(letters1, letters2) el ## letters1 letters2 ## [1,] &quot;q&quot; &quot;u&quot; ## [2,] &quot;v&quot; &quot;y&quot; ## [3,] &quot;n&quot; &quot;e&quot; ## [4,] &quot;n&quot; &quot;k&quot; ## [5,] &quot;i&quot; &quot;b&quot; ## [6,] &quot;o&quot; &quot;v&quot; ## [7,] &quot;z&quot; &quot;f&quot; ## [8,] &quot;h&quot; &quot;c&quot; ## [9,] &quot;f&quot; &quot;l&quot; ## [10,] &quot;c&quot; &quot;e&quot; ## [11,] &quot;t&quot; &quot;v&quot; ## [12,] &quot;s&quot; &quot;l&quot; ## [13,] &quot;y&quot; &quot;x&quot; ## [14,] &quot;r&quot; &quot;t&quot; ## [15,] &quot;h&quot; &quot;x&quot; ## [16,] &quot;z&quot; &quot;a&quot; ## [17,] &quot;l&quot; &quot;f&quot; ## [18,] &quot;i&quot; &quot;x&quot; ## [19,] &quot;c&quot; &quot;r&quot; ## [20,] &quot;d&quot; &quot;p&quot; ge &lt;- graph_from_edgelist(el, directed = FALSE) plot(ge) Until now all networks we have created have been unipartite. There has been one set of nodes and in principle each node could be connected to every other node. In bipartite networks, there are two distinct sets of nodes. Each node is only connected to nodes from the other set. Below, I create an example of an bipartite network. We have six students (S1-S6) that can register in four courses (C1-C4). The affiliation matrix has one column per class and one row per student. If a student is part of a class than the respective cell is a one. If a student does not visit a class then it is a zero. Both classes and students are nodes but no student is directly connected to another student and classes are not connected among themselves either. When we print the graph to the console, we can see that the graph is bipartite by looking at the four letter code: UN-B C1 &lt;- c(1,1,1,0,0,0) C2 &lt;- c(0,1,1,1,0,0) C3 &lt;- c(0,0,1,1,1,0) C4 &lt;- c(0,0,0,0,1,1) aff.df &lt;- data.frame(C1,C2,C3,C4) row.names(aff.df) &lt;- c(&quot;S1&quot;,&quot;S2&quot;,&quot;S3&quot;,&quot;S4&quot;,&quot;S5&quot;,&quot;S6&quot;) bn &lt;- graph.incidence(aff.df) bn ## IGRAPH 41c1979 UN-B 10 11 -- ## + attr: type (v/l), name (v/c) ## + edges from 41c1979 (vertex names): ## [1] S1--C1 S2--C1 S2--C2 S3--C1 S3--C2 S3--C3 S4--C2 S4--C3 S5--C3 S5--C4 S6--C4 plt.x &lt;- c(rep(2, 6), rep(4, 4)) plt.y &lt;- c(7:2, 6:3) lay &lt;- as.matrix(cbind(plt.x, plt.y)) shapes &lt;- c(&quot;circle&quot;, &quot;square&quot;) colors &lt;- c(&quot;blue&quot;, &quot;red&quot;) plot( bn, vertex.color = colors[V(bn)$type + 1], vertex.shape = shapes[V(bn)$type + 1], vertex.size = 10, vertex.label.degree = -pi / 2, vertex.label.dist = 1.2, vertex.label.cex = 0.9, layout = lay ) 5.3 Centrality measures Next, we will turn to the centrality measures we discussed in the lecture. The most basic measure is the degree centrality which is just the degree of a node. We can get the degree with the function degree(). degree(ga) ## [1] 3 4 2 3 3 4 5 5 7 4 3 4 6 4 3 3 2 4 5 2 We can assign each node its degree as an attribute and plot the network with accordingly colored nodes. For this we need two new packages: ggraph (Pedersen 2022a) and tidygraph (Pedersen 2022b). We will look at both packages a little more in depth later in this tutorial. library(tidygraph) ## ## Attache Paket: &#39;tidygraph&#39; ## Das folgende Objekt ist maskiert &#39;package:igraph&#39;: ## ## groups ## Das folgende Objekt ist maskiert &#39;package:raster&#39;: ## ## select ## Das folgende Objekt ist maskiert &#39;package:stats&#39;: ## ## filter library(ggraph) ## ## Attache Paket: &#39;ggraph&#39; ## Das folgende Objekt ist maskiert &#39;package:sp&#39;: ## ## geometry ## Das folgende Objekt ist maskiert &#39;package:spatstat.geom&#39;: ## ## square V(ga)$degree &lt;- degree(ga) ga ## IGRAPH 419db31 U--- 20 38 -- ## + attr: degree (v/n) ## + edges from 419db31: ## [1] 1-- 4 1--13 1--16 2-- 3 2-- 5 2-- 8 2-- 9 3--11 4-- 6 4--12 5-- 9 5--10 6-- 7 6--14 6--19 7-- 8 7--14 7--16 7--19 ## [20] 8--13 8--14 8--15 9--10 9--12 9--13 9--18 9--19 10--12 10--17 11--12 11--18 13--15 13--19 13--20 14--17 15--16 18--19 18--20 ga2 &lt;- as_tbl_graph(ga) ggraph(ga2) + geom_edge_link(lwd = 0.4) + geom_node_point(aes(col = degree), size = 3) ## Using &quot;stress&quot; as default layout ## Warning: Ignoring unknown parameters: size Other centrality metrics can also be computed with the epynomous functions from the igraph package. V(ga)$closeness &lt;- closeness(ga) V(ga)$betweenness &lt;- betweenness(ga) Another nice way to display these data is through radial plots. However, for this we again need two additional packages: sna (Butts 2022) and network (Butts 2008) . sna makes the plots and network provides the object class that sna uses. library(network) ## ## &#39;network&#39; 1.17.2 (2022-05-20), part of the Statnet Project ## * &#39;news(package=&quot;network&quot;)&#39; for changes since last version ## * &#39;citation(&quot;network&quot;)&#39; for citation information ## * &#39;https://statnet.org&#39; for help, support, and other information ## ## Attache Paket: &#39;network&#39; ## Die folgenden Objekte sind maskiert von &#39;package:igraph&#39;: ## ## %c%, %s%, add.edges, add.vertices, delete.edges, delete.vertices, get.edge.attribute, get.edges, get.vertex.attribute, ## is.bipartite, is.directed, list.edge.attributes, list.vertex.attributes, set.edge.attribute, set.vertex.attribute library(sna) ## Lade nötiges Paket: statnet.common ## ## Attache Paket: &#39;statnet.common&#39; ## Die folgenden Objekte sind maskiert von &#39;package:base&#39;: ## ## attr, order ## sna: Tools for Social Network Analysis ## Version 2.7 created on 2022-05-09. ## copyright (c) 2005, Carter T. Butts, University of California-Irvine ## For citation information, type citation(&quot;sna&quot;). ## Type help(package=&quot;sna&quot;) to get started. ## ## Attache Paket: &#39;sna&#39; ## Die folgenden Objekte sind maskiert von &#39;package:igraph&#39;: ## ## betweenness, bonpow, closeness, components, degree, dyad.census, evcent, hierarchy, is.connected, neighborhood, ## triad.census ## Das folgende Objekt ist maskiert &#39;package:nlme&#39;: ## ## gapply ## Die folgenden Objekte sind maskiert von &#39;package:spatstat.geom&#39;: ## ## is.connected, maxflow In the following plots, nodes that are closer to the center have higher centrality. A &lt;- as_adjacency_matrix(ga, sparse=FALSE) ga3 &lt;- as.network.matrix(A) gplot.target( ga3, degree(ga3, gmode = &quot;graph&quot;), main = &quot;Degree&quot;, circ.lab = FALSE, circ.col = &quot;skyblue&quot;, usearrows = FALSE, edge.col = &quot;darkgray&quot; ) gplot.target( ga3, closeness(ga3, gmode = &quot;graph&quot;), main = &quot;Closeness&quot;, circ.lab = FALSE, circ.col = &quot;skyblue&quot;, usearrows = FALSE, edge.col = &quot;darkgray&quot; ) gplot.target( ga3, betweenness(ga3, gmode = &quot;graph&quot;), main = &quot;Betweenness&quot;, circ.lab = FALSE, circ.col = &quot;skyblue&quot;, usearrows = FALSE, edge.col = &quot;darkgray&quot; ) 5.4 Visualization For non-spatial networks, there is no correct position for any one node. They can be freely arranged in space. However, certain arrangements will make it easier to understand the network while others will simply be confusing. Visualization is as much art as science but there are some common sense rules that usually result in a less confusing figure: Minimize edge crossings Maximize the symmetry of the layout of nodes Minimize the variability of the edge lengths Maximize the angle between edges when they cross or join nodes Minimize the total space used for the network display These were taken from Luke (2015) We will quickly go through some options that igraph gives you and then cover the basics of the ggraph package. As a baseline, we start with the default plot from an igraph object. plot(ga) We can change the color of nodes with vertex.color and the color of edges with edge.color. par(mfrow = c(1,2)) plot(ga, vertex.color=&quot;cyan&quot;) plot(ga, edge.color=&quot;cyan&quot;) We can add a title with plot(ga, main=&quot;Random layout&quot;) Next we can change the layout of the graph, that means the rules, the algorithm, which is used to optimize the position of the nodes. par(mfrow = c(1,3)) plot(ga, layout=layout.circle, main = &quot;circle&quot;) plot(ga, layout=layout.fruchterman.reingold, main = &quot;fruchterman&quot;) plot(ga, layout=layout.random, main = &quot;random&quot;) We also have the option to create an interactive 2d and 3d graph with tkplot() and rglplot(). It is not possible to embed the result in this html file but you can use the code here to try it out yourself. tkplot(ga) rglplot(ga) With that we come to ggraph, which is an effort to place network visualization inside the ggplot frame work. As we have seen above we need a tidygraph object to use ggraph. Here, we can use the same object we used above ga2. Each ggraph plot starts with the same function: ggraph(). The only argument in this function is the object we wish to display. As in ggplot2 this object is not a working plot yet. ga2 %&gt;% ggraph() ## Using &quot;stress&quot; as default layout All we see is a gray background. To visualize the data we need to add geoms. First, we will add the edges. There are mutiple options here and I will show some of them. ga2 %&gt;% ggraph() + geom_edge_link() ## Using &quot;stress&quot; as default layout ga2 %&gt;% ggraph() + geom_edge_bend() ## Using &quot;stress&quot; as default layout ga2 %&gt;% ggraph() + geom_edge_arc() ## Using &quot;stress&quot; as default layout Next, we have a look at the nodes. ga2 %&gt;% ggraph() + geom_node_point() ## Using &quot;stress&quot; as default layout ga2 %&gt;% ggraph() + geom_node_label(aes(label = degree)) ## Using &quot;stress&quot; as default layout ga2 %&gt;% ggraph() + geom_node_tile(width = 1, height = 1) ## Using &quot;stress&quot; as default layout Of course, we can also show both in one plot. ga2 |&gt; ggraph() + geom_edge_link() + geom_node_point() ## Using &quot;stress&quot; as default layout ga2 |&gt; ggraph() + geom_edge_arc() + geom_node_tile( width = .1, height = .1, aes(fill = degree) ) ## Using &quot;stress&quot; as default layout 5.5 Network Models We can simulate network models with igraph . A poisson random graph is simulated with erdos.renyi.game() where n is the number of nodes, m the number of edges , p the probability of two nodes sharing an edge and the type is either gnm or gnp, depending on whether you supply the number of edges or the probability of adjacency. prg1 &lt;- erdos.renyi.game(n=12,10,type=&#39;gnm&#39;) prg2 &lt;- erdos.renyi.game(n=12,0.1,type=&#39;gnp&#39;) par(mfrow = c(1,2)) plot(prg1) plot(prg2) A small world network is created with watts.strogatz.game(), where dim is the dimension of the starting lattice, size the size of the lattice along each dimension, nei the neighborhood of the lattice in which nodes are connected and p the rewiring probability. watts.strogatz.game(dim=1,size=100,nei=2,p=0.5) |&gt; as_tbl_graph() |&gt; ggraph() + geom_edge_link() + geom_node_point() ## Using &quot;stress&quot; as default layout Preferential attachment models are created with barabasi.game(), where n is the number of nodes, m the number of nodes added in each round and zero.appeal a factor that raises the likelihood of binding to a node that has no edges so far above zero. g &lt;- barabasi.game(n = 100, m = 1, directed = FALSE, zero.appeal = .5) g &lt;- as_tbl_graph(g) ggraph(g, layout = &quot;igraph&quot;, algorithm = &quot;kk&quot;) + geom_edge_link() + geom_node_point() 5.6 Optimal Channel Networks Next we will look at optimal channel networks and their R implementation in the OCNet Package (Carraro et al. 2020). As always, we first load the package. library(OCNet) Creating a OCN starts with a call to create_OCN(). This function establishes a lattice with dimX points in the x direction and dimY points in the y direction. In this grid it also establishes the flow direction of each point. The function has many more arguments but usually we do not have to worry about them and can keep the default settings. Here, we create an OCN in a 20x20 grid. OCN &lt;- create_OCN(dimX = 20, dimY = 20) The function produces a list with A a vector of drainage area values (i.e., how many cells does this cell drain) OCN$FD$A ## [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 1 1 5 1 3 2 1 1 5 1 3 1 1 4 ## [34] 1 3 1 1 8 1 1 1 1 11 1 6 1 1 1 10 1 3 5 1 5 1 1 13 1 1 1 1 2 1 21 1 1 ## [67] 1 3 13 1 9 1 1 8 1 16 1 1 4 1 2 1 26 1 1 1 1 1 130 111 99 96 93 1 19 1 1 6 1 ## [100] 1 1 30 1 1 1 1 2 142 1 1 2 1 1 83 1 1 18 1 1 1 400 366 1 4 1 1 146 1 8 1 1 1 ## [133] 3 1 58 21 1 9 2 1 1 1 363 1 1 1 160 1 1 6 1 1 1 1 33 1 1 1 5 1 1 3 1 353 1 ## [166] 165 1 10 1 1 3 1 1 4 1 27 3 1 1 1 1 1 8 1 341 1 2 1 6 2 1 1 1 1 1 21 1 1 ## [199] 1 1 1 5 1 1 172 1 1 1 1 1 1 1 1 1 1 1 18 1 4 1 1 2 1 3 1 167 2 1 1 1 3 ## [232] 1 6 2 1 1 1 13 1 1 1 1 1 1 1 161 1 1 1 74 1 8 1 1 1 1 1 1 5 1 1 1 3 1 ## [265] 44 1 115 1 78 1 67 52 49 1 4 1 1 1 1 1 1 3 1 40 1 1 1 110 1 1 5 1 1 47 1 3 1 ## [298] 2 1 1 1 1 18 1 16 1 1 28 1 1 1 3 1 1 40 1 4 1 5 1 1 11 1 1 15 1 2 1 24 1 ## [331] 1 1 1 2 1 31 1 6 1 1 1 6 1 1 7 4 1 1 1 19 12 1 1 1 1 1 23 7 1 1 1 3 1 ## [364] 4 1 1 3 1 5 1 1 8 3 1 1 5 1 3 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## [397] 1 1 1 1 and W a sparse adjacency matrix. OCN$FD$W ## [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## [67] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## [133] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## [199] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## [265] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## [331] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## [397] 1 1 1 ## Class &#39;spam&#39; (32-bit) The following functions all take the OCN object as argument. Instead of creating a new object they attach new levels to the list. create_OCN() created the list at the flow direction level (FD). landscape_OCN() creates the DEM of the river network and adds the catchment level (CM) to the list. The DEM is stored in OCN$FD$Z. library(terra) OCN &lt;- landscape_OCN(OCN) OCN$FD$Z |&gt; matrix(ncol = 20) |&gt; rast() |&gt; plot() Now we need to aggregate the flow network. This means we set a threshold of how many cells need to flow into one cell for this cell to become a river. This is done with aggregate_OCN(). This function builds the network at the river network (RN), aggregated (AG), subcatchment (SC) and catchment levels. We use the default threshold here, so the only argument we need is OCN. OCN &lt;- aggregate_OCN(OCN) draw_contour_OCN(OCN) draw_elev2D_OCN(OCN) draw_elev3D_OCN(OCN) We can also take the OCN and transform it into an igraph object. river_graph &lt;- OCN_to_igraph(OCN, level = &quot;AG&quot;) river_graph |&gt; as_tbl_graph() |&gt; ggraph(layout = &quot;igraph&quot;, algorithm = &quot;kk&quot;) + geom_edge_link(color = &quot;blue&quot;) + geom_node_point(size = 0) 5.7 sfnetworks Lastly, we will turn towards the sfnetworks package. This R package enables us to turn spatial sf objects into networks. Unlike sfnetworks uses the tidygraph package instead of igraph. However, tidygraphis intimately linked to igraph. It takes most of the functions from igraph and changes them so they adhere to the tidy principles. There is an implementation of spatial networks in R which works directly with igraph, spNetworks(Gelb 2021) but we will not cover this package here. Since tidygraph adheres to the tidy principles, we can use dplyr verbs and piping. So before we work with sfnetworks, lets have a short look at tidygraph. To represent networks while following tidy principles (i.e., one row = one observation), tidygraph uses two tables for each graph. One table represents the nodes and the other one the edges. As an example, we will use the OCN we just created. river_tidy &lt;- as_tbl_graph(river_graph) river_tidy ## # A tbl_graph: 382 nodes and 381 edges ## # ## # A rooted tree ## # ## # Node Data: 382 × 0 (active) ## # … with 376 more rows ## # ## # Edge Data: 381 × 2 ## from to ## &lt;int&gt; &lt;int&gt; ## 1 20 19 ## 2 39 19 ## 3 1 22 ## # … with 378 more rows river_tidy has the class tbl_graph. We also see the number of nodes and edges and two tibbles: Node Data without columns, because the nodes do not have attributes yet and Edge Data which is an edge list. plot(river_tidy) Plotting returns the same default output as for an igraph object and, in fact, a look at class of river_tidy reveals that it has inherited the igraph class from river_graph. class(river_tidy) ## [1] &quot;tbl_graph&quot; &quot;igraph&quot; If we want to work with either one of the tables we use the activate() function. In the following example, I add an attribute, letter, to the nodes. river_tidy %&gt;% activate(nodes) %&gt;% mutate(letter = sample(x = letters, size = vcount(river_graph), replace = TRUE)) ## # A tbl_graph: 382 nodes and 381 edges ## # ## # A rooted tree ## # ## # Node Data: 382 × 1 (active) ## letter ## &lt;chr&gt; ## 1 b ## 2 e ## 3 b ## 4 o ## 5 j ## 6 f ## # … with 376 more rows ## # ## # Edge Data: 381 × 2 ## from to ## &lt;int&gt; &lt;int&gt; ## 1 20 19 ## 2 39 19 ## 3 1 22 ## # … with 378 more rows We can activate the edges in the same way and we can activate edges and nodes in a single pipe. river_tidy %&lt;&gt;% activate(nodes) %&gt;% mutate(degree = centrality_degree()) %&gt;% activate(edges) %&gt;% mutate(centrality = centrality_edge_betweenness()) %&gt;% arrange(centrality) This will be enough tidygraph for now. The package has almost 300 functions, so we will not be able to cover the package extensively. Another short introduction by the package’s author is available here. Now we turn back to sfnetworks. library(sf) library(sfnetworks) library(mapview) To demonstrate the functionality, we use a demo data set that is included in the Package. The roxel data contain the road network (including bike lanes and footpaths) of the Roxel neighborhod in the German city of Münster. data(&quot;roxel&quot;) mapview(roxel) There are three options of creating a network from sf data with sfnetwork. Either you provide a point data set as nodes and a line data set as edges or you just provide a line data set and the nodes are automatically created at the ends of the lines or you just provide a point data set the the edges are drawn as straight lines between the nodes. We will try out the just linestrings and just points versions here. We will start with the just linestrings option. roxel_nw1 &lt;- as_sfnetwork(roxel, directed = FALSE) The object looks similar to the tidygraph object we saw before. New are the coordinate reference system and the coordinates of the nodes and edges. roxel_nw1 ## # A sfnetwork with 701 nodes and 851 edges ## # ## # CRS: EPSG:4326 ## # ## # An undirected multigraph with 14 components with spatially explicit edges ## # ## # Node Data: 701 × 1 (active) ## # Geometry type: POINT ## # Dimension: XY ## # Bounding box: xmin: 7.522622 ymin: 51.94151 xmax: 7.546705 ymax: 51.9612 ## geometry ## &lt;POINT [°]&gt; ## 1 (7.533722 51.95556) ## 2 (7.533461 51.95576) ## 3 (7.532442 51.95422) ## 4 (7.53209 51.95328) ## 5 (7.532709 51.95209) ## 6 (7.532869 51.95257) ## # … with 695 more rows ## # ## # Edge Data: 851 × 5 ## # Geometry type: LINESTRING ## # Dimension: XY ## # Bounding box: xmin: 7.522594 ymin: 51.94151 xmax: 7.546705 ymax: 51.9612 ## from to name type geometry ## &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;fct&gt; &lt;LINESTRING [°]&gt; ## 1 1 2 Havixbecker Strasse residential (7.533722 51.95556, 7.533461 51.95576) ## 2 3 4 Pienersallee secondary (7.532442 51.95422, 7.53236 51.95377, 7.53209 51.95328) ## 3 5 6 Schulte-Bernd-Strasse residential (7.532709 51.95209, 7.532823 51.95239, 7.532869 51.95257) ## # … with 848 more rows plot(roxel_nw1) Now we can extract our new nodes as sf point object. roxel_point &lt;- st_as_sf(activate(roxel_nw1, nodes), crs = 4326) roxel_nw2 &lt;- roxel_point |&gt; as_sfnetwork() plot(roxel_nw2) Working with the spatial network works the same as the non-spatial tidygraph networks. roxel_nw1 %&gt;% activate(&quot;edges&quot;) %&gt;% mutate(weight = edge_length()) %&gt;% activate(&quot;nodes&quot;) %&gt;% mutate(bc = centrality_betweenness(weights = weight, directed = FALSE)) ## Warning in betweenness(graph = graph, v = V(graph), directed = directed, : &#39;nobigint&#39; is deprecated since igraph 1.3 and will be removed ## in igraph 1.4 ## # A sfnetwork with 701 nodes and 851 edges ## # ## # CRS: EPSG:4326 ## # ## # An undirected multigraph with 14 components with spatially explicit edges ## # ## # Node Data: 701 × 2 (active) ## # Geometry type: POINT ## # Dimension: XY ## # Bounding box: xmin: 7.522622 ymin: 51.94151 xmax: 7.546705 ymax: 51.9612 ## geometry bc ## &lt;POINT [°]&gt; &lt;dbl&gt; ## 1 (7.533722 51.95556) 13808 ## 2 (7.533461 51.95576) 9777 ## 3 (7.532442 51.95422) 35240 ## 4 (7.53209 51.95328) 31745 ## 5 (7.532709 51.95209) 7174 ## 6 (7.532869 51.95257) 9081 ## # … with 695 more rows ## # ## # Edge Data: 851 × 6 ## # Geometry type: LINESTRING ## # Dimension: XY ## # Bounding box: xmin: 7.522594 ymin: 51.94151 xmax: 7.546705 ymax: 51.9612 ## from to name type geometry weight ## &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;fct&gt; &lt;LINESTRING [°]&gt; [m] ## 1 1 2 Havixbecker Strasse residential (7.533722 51.95556, 7.533461 51.95576) 28.8 ## 2 3 4 Pienersallee secondary (7.532442 51.95422, 7.53236 51.95377, 7.53209 51.95328) 108. ## 3 5 6 Schulte-Bernd-Strasse residential (7.532709 51.95209, 7.532823 51.95239, 7.532869 51.95257) 54.3 ## # … with 848 more rows We can also use spatial operations like spatial filtering on the network. bbox &lt;- st_bbox(roxel) bbox |&gt; st_as_sfc() |&gt; st_as_sf(crs=4326) |&gt; st_transform(3035) -&gt; bbox bbox &lt;- st_buffer(x = bbox, dist = -700, joinStyle = &quot;MITRE&quot;, mitreLimit = 2) bbox %&lt;&gt;% st_transform(4326) plot(roxel_nw1, col = &quot;grey&quot;) plot(bbox, border = &quot;red&quot;, lty = 4, lwd = 4, add = TRUE) filtered = roxel_nw1 %&gt;% activate(&quot;edges&quot;) %&gt;% st_filter(bbox) %&gt;% activate(&quot;nodes&quot;) %&gt;% st_filter(bbox) plot(roxel_nw1, col = &quot;grey&quot;) plot(filtered, add = TRUE) We can also filter to observations around a specified point. point = st_centroid(st_combine(roxel_nw1)) filtered = roxel_nw1 %&gt;% activate(&quot;nodes&quot;) %&gt;% st_filter(point, .predicate = st_is_within_distance, dist = 500) plot(roxel_nw1, col = &quot;grey&quot;) plot(filtered, add = TRUE) plot(point, col = &quot;red&quot;, add = TRUE) 5.7.1 Finding paths in the network Within our network we can find the shortest way to get from point A to point B with the function st_network_paths(). The function uses the most common routing algorithm (i.e., algorithm to find the shortest path): Dijkstras algorithm. Before we call it, we prepare the data: We transform it to a projected coordinate system. Now the unit of edge lengths are meters and not fractions of degrees. Add the edge length as attribute to the edges. In a spatial context the length of edges has a natural interpretation as the spatial distance between points. net &lt;- roxel_nw1 %&gt;% # transform so that distances are in meters st_transform(3035) %&gt;% activate(&quot;edges&quot;) %&gt;% mutate(weight = edge_length()) We compute the shortest paths from the 38\\(^{th}\\) to the 200\\(^{th}\\) and the 517\\(^{th}\\) node. The length of the paths is weighted be the weight variable which we computed in the code block above. paths = st_network_paths(net, from = 38, to = c(200, 517), weights = &quot;weight&quot;) Now what is the output of this function? paths ## # A tibble: 2 × 2 ## node_paths edge_paths ## &lt;list&gt; &lt;list&gt; ## 1 &lt;int [25]&gt; &lt;int [24]&gt; ## 2 &lt;int [22]&gt; &lt;int [21]&gt; A tibble with two list columns, two rows long. Each row is one path, i.e., the first row is the path from 38 to 200 and the second row is the path from 38 to 517. In the first column we get the node ids on the respective path and in the second column we get the edge ids. To visualize this we will write a small function that plots each node in the node paths (i.e., column 1). plot_path = function(node_path) { net %&gt;% activate(&quot;nodes&quot;) %&gt;% slice(node_path) %&gt;% plot(cex = 1.5, lwd = 1.5, add = TRUE) } Now we apply this function to all paths and also add symbols to the start and ending points of our paths. colors = sf.colors(3, categorical = TRUE) plot(net, col = &quot;grey&quot;) paths %&gt;% pull(node_paths) %&gt;% walk(plot_path) net %&gt;% activate(&quot;nodes&quot;) %&gt;% st_as_sf() %&gt;% slice(c(38, 200, 517)) %&gt;% plot(col = colors, pch = 8, cex = 2, lwd = 2, add = TRUE) If you need an overview of the distances between all nodes use st_network_cost(). cost &lt;- st_network_cost(net) hist(cost) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
