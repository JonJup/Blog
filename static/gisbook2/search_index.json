[["introduction-to-the-sf-package.html", "Spatial Data Science in R Chapter 1 Introduction to the sf package 1.1 Loading spatial data into R 1.2 Creating spatial data yourself 1.3 Basic operations 1.4 Useful functions", " Spatial Data Science in R Jonathan Jupke 2024-06-12 Chapter 1 Introduction to the sf package The most common way to handle spatial data in R is with the sf package (Pebesma 2018). In this first chapter, you will learn the basics of sf - everything you need to load, visualize, alter, and analyze spatial data in R. If you are looking more comprehensive introduction to sf and some additional packages, a great and free resource are the books Geocomputation in R (Lovelace, Nowosad, and Muenchow 2019) available here and Spatial Data Science (Pebesma and Bivand 2022) available here. 1.1 Loading spatial data into R First we need to load the sf package. When we do so, we get the following message: library(sf) ## Linking to GEOS 3.12.1, GDAL 3.8.4, PROJ 9.3.1; sf_use_s2() is TRUE So sf establishes a connection between the running R instance and three other programs: GEOS, GDAL and PROJ. GEOS and GDAL are collections of functions to read, modify, and write geodata; PROJ transforms geodata from one coordinate reference system to another. Before we can load a file you need to set your working directory. setwd(&quot;~/Uni/teaching/GIS/&quot;) You have to adjust the working directory to your folder structure. If you are not sure how to write the path to your desired folder you can use the file.choose() function. After executing the function a window prompt will open where you can select the desired folder in a point-and-click fashion. In the function you have to choose a file by double-clicking it. R will now print the path to the selected file in the console. The argument to setwd() needs to be the path to a folder. Therefore, to use the path file.choose() provided you need to remove the file name from the path. Exercise 1.1 Create a folder where you will store all your files for this class set the working directory to that folder With the command st_read(), we read the geopackage file gadm36_DEU_3_pk.gpkg (download here). The function starts, like all functions in sf, with st_, short for spatiotemporal. germany &lt;- st_read(&quot;data/gadm36_DEU_3_pk.gpkg&quot;) ## Reading layer `gadm36_DEU_3_pk&#39; from data source ## `C:\\Users\\jonat\\Documents\\001_Uni\\002_teaching\\online books\\book_spatial_data_science_in_R\\data\\gadm36_DEU_3_pk.gpkg&#39; ## using driver `GPKG&#39; ## Simple feature collection with 4680 features and 16 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: 5.866251 ymin: 47.27012 xmax: 15.04181 ymax: 55.05653 ## Geodetic CRS: WGS 84 After the file is loaded, we automatically get a set of information. These are from left to right and top to bottom following the yellow boxes: The layer name of the loaded file. This is not the object name in R, but the name stored in the geopackage (gpkg) file; where the file is located on disk; the file format (here GPKG); the number of rows (features, 4680) and columns (fields, 16); the feature type (GeometryType); the bounding box, i.e., minimum and maximum x and y coordinate data or longitude and latitude, and finally the coordinate reference system. If we now open the object in R we see the following: germany ## Simple feature collection with 4680 features and 16 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: 5.866251 ymin: 47.27012 xmax: 15.04181 ymax: 55.05653 ## Geodetic CRS: WGS 84 ## First 10 features: ## GID_0 NAME_0 GID_1 NAME_1 NL_NAME_1 GID_2 NAME_2 NL_NAME_2 GID_3 ## 1 DEU Germany DEU.1_1 Baden-Württemberg &lt;NA&gt; DEU.1.1_1 Alb-Donau-Kreis &lt;NA&gt; DEU.1.1.1_1 ## 2 DEU Germany DEU.1_1 Baden-Württemberg &lt;NA&gt; DEU.1.1_1 Alb-Donau-Kreis &lt;NA&gt; DEU.1.1.2_1 ## 3 DEU Germany DEU.1_1 Baden-Württemberg &lt;NA&gt; DEU.1.1_1 Alb-Donau-Kreis &lt;NA&gt; DEU.1.1.3_1 ## 4 DEU Germany DEU.1_1 Baden-Württemberg &lt;NA&gt; DEU.1.1_1 Alb-Donau-Kreis &lt;NA&gt; DEU.1.1.4_1 ## 5 DEU Germany DEU.1_1 Baden-Württemberg &lt;NA&gt; DEU.1.1_1 Alb-Donau-Kreis &lt;NA&gt; DEU.1.1.5_1 ## 6 DEU Germany DEU.1_1 Baden-Württemberg &lt;NA&gt; DEU.1.1_1 Alb-Donau-Kreis &lt;NA&gt; DEU.1.1.6_1 ## 7 DEU Germany DEU.1_1 Baden-Württemberg &lt;NA&gt; DEU.1.1_1 Alb-Donau-Kreis &lt;NA&gt; DEU.1.1.7_1 ## 8 DEU Germany DEU.1_1 Baden-Württemberg &lt;NA&gt; DEU.1.1_1 Alb-Donau-Kreis &lt;NA&gt; DEU.1.1.8_1 ## 9 DEU Germany DEU.1_1 Baden-Württemberg &lt;NA&gt; DEU.1.1_1 Alb-Donau-Kreis &lt;NA&gt; DEU.1.1.9_1 ## 10 DEU Germany DEU.1_1 Baden-Württemberg &lt;NA&gt; DEU.1.1_1 Alb-Donau-Kreis &lt;NA&gt; DEU.1.1.10_1 ## NAME_3 VARNAME_3 NL_NAME_3 TYPE_3 ENGTYPE_3 CC_3 HASC_3 ## 1 Allmendingen &lt;NA&gt; &lt;NA&gt; Verwaltungsgemeinschaft Municipality 084255001 &lt;NA&gt; ## 2 Blaubeuren &lt;NA&gt; &lt;NA&gt; Verwaltungsgemeinschaft Municipality 084255002 &lt;NA&gt; ## 3 Blaustein &lt;NA&gt; &lt;NA&gt; Einheitsgemeinde Municipality 084250141 &lt;NA&gt; ## 4 Dietenheim &lt;NA&gt; &lt;NA&gt; Verwaltungsverband Municipality 084255003 &lt;NA&gt; ## 5 Dornstadt &lt;NA&gt; &lt;NA&gt; Verwaltungsgemeinschaft Municipality 084255004 &lt;NA&gt; ## 6 Ehingen (Donau) &lt;NA&gt; &lt;NA&gt; Verwaltungsgemeinschaft Municipality 084255005 &lt;NA&gt; ## 7 Erbach &lt;NA&gt; &lt;NA&gt; Einheitsgemeinde Municipality 084250039 &lt;NA&gt; ## 8 Kirchberg-Weihungstal &lt;NA&gt; &lt;NA&gt; Verwaltungsverband Municipality 084255006 &lt;NA&gt; ## 9 Laichinger Alb &lt;NA&gt; &lt;NA&gt; Verwaltungsverband Municipality 084255007 &lt;NA&gt; ## 10 Langenau &lt;NA&gt; &lt;NA&gt; Verwaltungsverband Municipality 084255008 &lt;NA&gt; ## geom ## 1 MULTIPOLYGON (((9.777709 48... ## 2 MULTIPOLYGON (((9.775818 48... ## 3 MULTIPOLYGON (((9.827528 48... ## 4 MULTIPOLYGON (((10.00971 48... ## 5 MULTIPOLYGON (((9.972806 48... ## 6 MULTIPOLYGON (((9.803958 48... ## 7 MULTIPOLYGON (((9.837822 48... ## 8 MULTIPOLYGON (((10.02729 48... ## 9 MULTIPOLYGON (((9.708008 48... ## 10 MULTIPOLYGON (((9.946911 48... The header is similar to what we have already seen when reading the file. Below are the first ten rows of all columns. The object has the classes: class(germany) ## [1] &quot;sf&quot; &quot;data.frame&quot; For the most part, we can handle sf objects like normal data frames. We can use the [ subsetting operations. germany[,1] ## Simple feature collection with 4680 features and 1 field ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: 5.866251 ymin: 47.27012 xmax: 15.04181 ymax: 55.05653 ## Geodetic CRS: WGS 84 ## First 10 features: ## GID_0 geom ## 1 DEU MULTIPOLYGON (((9.777709 48... ## 2 DEU MULTIPOLYGON (((9.775818 48... ## 3 DEU MULTIPOLYGON (((9.827528 48... ## 4 DEU MULTIPOLYGON (((10.00971 48... ## 5 DEU MULTIPOLYGON (((9.972806 48... ## 6 DEU MULTIPOLYGON (((9.803958 48... ## 7 DEU MULTIPOLYGON (((9.837822 48... ## 8 DEU MULTIPOLYGON (((10.02729 48... ## 9 DEU MULTIPOLYGON (((9.708008 48... ## 10 DEU MULTIPOLYGON (((9.946911 48... germany[1,] ## Simple feature collection with 1 feature and 16 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: 9.61846 ymin: 48.29861 xmax: 9.822202 ymax: 48.36533 ## Geodetic CRS: WGS 84 ## GID_0 NAME_0 GID_1 NAME_1 NL_NAME_1 GID_2 NAME_2 NL_NAME_2 GID_3 ## 1 DEU Germany DEU.1_1 Baden-Württemberg &lt;NA&gt; DEU.1.1_1 Alb-Donau-Kreis &lt;NA&gt; DEU.1.1.1_1 ## NAME_3 VARNAME_3 NL_NAME_3 TYPE_3 ENGTYPE_3 CC_3 HASC_3 ## 1 Allmendingen &lt;NA&gt; &lt;NA&gt; Verwaltungsgemeinschaft Municipality 084255001 &lt;NA&gt; ## geom ## 1 MULTIPOLYGON (((9.777709 48... However, when we subset the columns of an sf object, we always keep the geom or geometry column. Usually, this is convenient because this column contains the spatial coordinates of the objects. The geom column is a so-called sticky column. However, if you want to remove this column explicitly, you can use st_drop_geometry(). The geom column is different in another way - it is a list column. Unlike other columns that are vectors, it is a list. The geom column has the class sfc. class(germany$geom) ## [1] &quot;sfc_MULTIPOLYGON&quot; &quot;sfc&quot; sfc is short for simple feature column, i.e., a column for simple features. The individual elements in the column have the class sfg, simple feature geometry. sfg are the individual geometric shapes (points, lines, polygons, …). Below, we will create sfg objects ourselves and compose an sf object from them. An object of class sf can be visualized with plot(). Fr this plot, I subset the data set to the first ten rows and three columns. plot(germany[1:10,7:9]) As you can see, by default each variable is plotted individually. Plots with single variables are created when we subset the dataset to one variable. plot(germany[,&quot;GID_3&quot;]) Exercise Load the Germany data in R Plot the variable NAME_2 1.2 Creating spatial data yourself Relationship of sf geometries (aus Lovelace et al 2021) In sf, we can create spatial objects ourselves. This is rarely necessary since we usually work with data that was created in other projects, but later procedures are easier to understand once you have gone through the process from the beginning. The functions to create geometric shapes follow a simple rule: st_ + name of the geometryType. So to create a point we use: point1 &lt;- st_point(x = c(1,1)) point1 is a point with coordinates 1 1 and has the classes , XY, POINT, and sfg. class(point1) ## [1] &quot;XY&quot; &quot;POINT&quot; &quot;sfg&quot; plot(point1) Exercise Create a point with the coordinates 3 5 Lines consist of several coordinates which are connected with each other. The single coordinates are vectors (c()) just like st_point(). We could use lists, matrices, or data.frames to st_linestring() with several point coordinates. The easiest way is to use matrices with two columns (for X and Y coordiantes) and as many rows as coordinate pairs. In the example we create a line with the coordinates 1 1, 1 2, 2 2, 2 3. line_coordinates &lt;- matrix(data = c(1,1,1,2,2,2,2,3), ncol = 2, byrow = T) line1 &lt;- st_linestring(line_coordinates) plot(line1) To create the object line_coordinates, we have transformed a vector with all coordinates into a matrix with two columns (ncol) and specified that the matrix is filled row by row (byrow = T), i.e., first row 1 column 1, then row 1 column 2 and so on. By default, matrices in R are filled column by column, not row by row. Since this notation is not very intuitive, I prefer the following notation: line_coordinates &lt;- rbind( c(1,1), c(1,2), c(2,2), c(2,3) ) line1 &lt;- st_linestring(line_coordinates) plot(line1) As you can see, the result is the same, but the single coordinates are not in one long vector. The function rbind() (short for row bind) takes single vectors and combines them as rows of a matrix. The equivalent function for columns is called cbind(). With line_coordinates we can also create a multipoint. multipoint1 &lt;- st_multipoint(line_coordinates) plot(multipoint1) Exercise Create a line with the coordinates 00 01 02 03 14 24 34 44 Polygons are created with lists. When we have a single one polygon it looks like a LineString with the difference that the first coordinate and the last are the same. polygon_coordinates &lt;- rbind( c(1,1), c(1,2), c(2,2), c(2,1), c(1,1) ) polygon1 &lt;- st_polygon(list(polygon_coordinates)) plot(polygon1) MultiLineStrings and Multipolygons are aslo created with lists. multilinestring_coordinates &lt;- list(rbind(c(1,1), c(1,2), c(1,3), c(1,4)), rbind(c(2,0), c(3,0), c(4,0), c(4,1))) multilinestring1 &lt;- st_multilinestring(multilinestring_coordinates) plot(multilinestring1) Exercise Extend the coordinates of the line you created in the last exercise to become a polygon. multipolygon_coordinates &lt;- list( list(rbind(c(0,0), c(0,1), c(1,1), c(1,0), c(0,0))), list(rbind(c(2,1), c(2,2), c(1,2), c(1,1), c(2,1))) ) multipolygon1 &lt;- st_multipolygon(multipolygon_coordinates) plot(multipolygon1) Geometry collections are single geometric objects that combine different GeometryTypes. geometrycollection1 &lt;- st_geometrycollection(x = list( st_multipolygon(multipolygon_coordinates), st_multilinestring(multilinestring_coordinates) )) plot(geometrycollection1) 1.3 Basic operations So far, our objects have geometric shapes, but they are not truly spatial. They are not conected to any specific locations on earth, because they don’t have a coordinate reference system (CRS). With the command st_crs() we assign a CRS to an object. Alternatively, we can do this in st_sfc() when we turn an sfg into an sfc or in st_as_sf when we turn an sfc into an sf. We can use different formats to describe the CRS, but in practice the EPSG code is the easiest. We assign geometrycollection1 the CRS World Geodetic Survey 1984 (WGS84) with the EPSG code 4326. geometrycollection_sfc &lt;- st_sfc(geometrycollection1, crs = &quot;EPSG:4326&quot;) geometrycollection_sf &lt;- st_as_sf(geometrycollection_sfc) Exercise Turn one of the objects into a spatial sf object. Once with the coordinate reference system EPSG:3035 and once with EPSG:4326. There are many superior alternatives to the plot() function to create maps in R. Here, we will use the tmap package (Tennekes 2018). Each tmap has at least two elements: 1. tm_shape() The spatial object you want to map. 2. the geometric shape you want to use: tm_dots() for points, tm_lines() for lines and tm_polygons() for polygons. These elements are combined with a +. There are no limits to how many objects or geometric shapes you can include in a single map. If you want to use different objects in one map you can call tm_shape() again with the next object after the +. ## Breaking News: tmap 3.x is retiring. Please test v4, e.g. with ## remotes::install_github(&#39;r-tmap/tmap&#39;) ## tmap mode set to plotting library(tmap) tm_shape(geometrycollection_sf) + tm_dots(size = 1) + tm_lines() + tm_polygons() With tmap you can create interactive and static maps. Interactive maps are great to explore your data, to check if you have specified the correct CRS, or for interactive documents in html format like this book. To create interactive maps you have to change the tm_mode from \"plot\" to \"view\". tmap_mode(&quot;view&quot;) ## tmap mode set to interactive viewing After that the same function as before will create interactive maps. tm_shape(geometrycollection_sf) + tm_dots(size = 1) + tm_lines() + tm_polygons() Exercise Create and interactive map in which you display the sf objects you created in the previous exercise. Why are the objects apart? Whats the main difference between the two CRS? Discuss with your neighbor. 1.4 Useful functions The names of the functions in sf are mostly self-explanatory. you can often just type sf::, scroll through the list of functions that appears, and with some background knowledge and imagination you’ll often find what you’re looking for. This works especially well if you combine this tactic with the help function (?function_name or highlight it and press F1). Nevertheless, to conclude, let’s look at some functions here as an example. 1.4.1 st_area With st_area() we can determine the area of polygons. germany_area &lt;- st_area(germany) class(germany_area) ## [1] &quot;units&quot; germany_area has the class units which introduced the eponymous package (Pebesma, Mailund, and Hiebert 2016). With units::drop_units() we can convert the object into simple numbers … germany_area &lt;- units::drop_units(germany_area) … and color the administrative districts according to their area on a map. To do this, we first create a variable called area in germany with the mutate() function from the dplyr package (Wickham et al. 2022). germany &lt;- dplyr::mutate(germany, area = germany_area) library(mapview) mapview(germany, zcol = &quot;area&quot;) To create this map we used the mapview package (Appelhans et al. 2021). mapview is often the faster and easier solution to create interactive maps. tmap offers more options and is the better solution to create maps for reports. Exercise Compute the size of German administrative districts. Select and plot only those districts that are larger the median district size. 1.4.2 st_distance With st_distance() we can determine the distance between two objects. The objects you want to use need to have an CRS otherwise R wont now what the distance unit is. As a first example, we just pick to random German districts and compute the distance between them. district1 &lt;- germany[10,] district2 &lt;- germany[100,] st_distance(x = district1, y = district2) ## Units: [m] ## [,1] ## [1,] 85111.62 We get a units matrix with the shortest distance between the two polygons. Note, that this is not the distance between the centroids but between the borders of the polygons. If we supply the function with vectors, the following happens: district1 &lt;- germany[8:10,] district2 &lt;- germany[98:100,] st_distance(x = district1, y = district2) ## Units: [m] ## [,1] [,2] [,3] ## [1,] 118532.39 116637.04 97183.43 ## [2,] 82949.84 81943.49 61832.11 ## [3,] 105208.29 105789.91 85111.62 Now the distance matrix has three times three elements. Each gives the distance between two of the elements from the original vectors. A question we might ask with this tool is are the ten largest districts closer to each other than the ten smallest. big10 &lt;- germany |&gt; # sort by area, decreasing so smallest value on top dplyr::arrange(area) |&gt; # take only the last 10 rows, i.e. the ten largest districts. dplyr::slice_tail(n = 10) big10 &lt;- st_distance(big10) This time, we only use one dataset, so the distance from each polygon to each polygon is calculated. In cell 3, row 2 is the distance from centroid of the third polygon to the second. Since the distance from the third to the second is equal to the distance from the second to the third, the matrix is symmetric. The diagonal contains the distances of objects to themselves, i.e. 0. big10 &lt;- big10 |&gt; units::drop_units() big10 &lt;- big10[upper.tri(big10)] small10 &lt;- germany |&gt; dplyr::arrange(area) |&gt; dplyr::slice_head(n = 10) |&gt; sf::st_distance() |&gt; units::drop_units() |&gt; {\\(x) x[upper.tri(x)]}() With upper.tri() I select the upper triangle of the matrix (see figure below) so I don’t have every value twice. This also removes the diagonal. For small10 we do the same, but this time with pipe operators. The only differences here are that slice_tail() has been replaced by slice_head() and the annonymous function at the end of the pipe. Anonymous functions are functions without a name. They are not stored as objects but executed directly. In R since version 4.1 we can create anonymous functions as follows: # The two functions do the same thing. plus5 &lt;- function(x) return(x+5) plus5(4) ## [1] 9 4 |&gt; {\\(x) x + 5}() ## [1] 9 Now we still need to package the results in a data set and display them. For the latter, we use the ggplot2 package (Wickham 2016) here. See here for an introduction. library(ggplot2) data &lt;- data.frame(size = rep(c(&quot;big&quot;, &quot;small&quot;), each = 45), distance = c(big10, small10)) ggplot(data, aes(y = distance, x = size)) + geom_boxplot() So we see that the median distance between the ten largest counties is slightly smaller than that between the ten smallest. If we have two data sets and we only want to compare the first element of data set 1 with the first of data set 2, the second with the second, and so on, we set the argument by_element to TRUE in the function st_distance(). Exercise Compute the distance between the district Landau in der Pfalz and Berlin (the variable is NAME_3) 1.4.3 st_nearest_feature With st_nearest_feature() we find the elements in a dataset that are closest to the selected element. # - Select the largest district big1 &lt;- germany |&gt; dplyr::arrange(area) |&gt; dplyr::slice_tail(n = 1) # - Which element from germany is closest to the largest district? nnid &lt;- st_nearest_feature(big1, germany) # - The result is the row number of this closest object. nnid ## [1] 2167 germany[nnid, ] ## Simple feature collection with 1 feature and 17 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: 13.15783 ymin: 52.39318 xmax: 13.24984 ymax: 52.4208 ## Geodetic CRS: WGS 84 ## GID_0 NAME_0 GID_1 NAME_1 NL_NAME_1 GID_2 NAME_2 NL_NAME_2 GID_3 ## 2167 DEU Germany DEU.4_1 Brandenburg &lt;NA&gt; DEU.4.13_1 Potsdam-Mittelmark &lt;NA&gt; DEU.4.13.6_1 ## NAME_3 VARNAME_3 NL_NAME_3 TYPE_3 ENGTYPE_3 CC_3 HASC_3 ## 2167 Kleinmachnow &lt;NA&gt; &lt;NA&gt; Amtsfreie Gemeinde Municipality 120690304 &lt;NA&gt; ## geom area ## 2167 MULTIPOLYGON (((13.23662 52... 11759803 mapview(rbind(big1,germany[nnid, ]), zcol = &quot;GID_3&quot;) Exercise What is the district closest to Landau in der Pfalz? 1.4.4 spatial subsetting There are many ways to subset tables in R, using [, select() and filter(). They use the position of the desired objects in the table (for [), their column names (for select()), or values of the various variables (for filter()). With spatial data, we can spatial relationships to subset data sets. The syntax follows the following scheme: Let X be the data set we want to select from and let Y be the data set we want to select with. For example: we have a set of bird observations all over Germany (X) and a data set with counties (Y) and we want to subset to the birds within one specific county. Now we subset X by Y by X[Y]. As an example, we use a set of bird observations (here for download). In the code below, you can also see how to turn a data frame into an sf object. We use the st_as_sf() and provide it with the data frame, the names of the columns that have the coordinates (coords, first x then y), and the CRS. # - load data birds &lt;- readRDS(&quot;data/birds.rds&quot;) # - drop observations without spatial coordinates birds &lt;- birds[which(!is.na(birds$decimalLongitude)), ] # - turn bird data into sf object. birds &lt;- st_as_sf(birds, coords = c(&quot;decimalLongitude&quot;, &quot;decimalLatitude&quot;), crs = &quot;EPSG:4326&quot;) # - subset data for plot birds_subset &lt;- birds[1:100, ] mapview(birds_subset) Now we want to select only the bird observations that are located in our county. The column NAME_2 holds the county name. We select the county Goslar. Then we subset birds with goslar. goslar &lt;- dplyr::filter(germany, NAME_2 == &quot;Goslar&quot;) goslar_birds &lt;- birds[goslar, ] mapview(goslar_birds) The subsetting checks for a topological relationship between the elements of birds and goslar. If, as in the command above, we do not explicitly choose a topological relationship, the default relationship, intersection (per st_intersects()), is applied. Alternatives are touching the objects (st_touch()), crossing the objects (st_cross()), and covering (st_covers()). See the following figure for more examples. Topological Relationships (Lovelace et al. 2021) We consider here another example to demonstrate a different topological relationship. In this example, we use the germany data set and the largest county from the data set (big1). We want to select all counties that are adjacent to the largest county. So we subset germany the data set containing all counties using big1, the data set containing only the largest county. Instead of the st_intersects() relation, which is set by default, we choose st_touches(). big1_neighbour &lt;- germany[big1, op = st_touches] mapview(big1_neighbour, zcol = &quot;GID_3&quot;, legend = FALSE) Exercise Select the birds within Berlin, Hannover, and Kiel 1.4.5 Spatial joins In a spatial join, we add the variables of a second data set to those of a first one. The spatial relationship between the elements of the objects is used to determine which elements are combined. An example would be to add the county to the bird data as a variable. # - Random subset of 500 titmice birds_subset &lt;- birds[sample(1:nrow(birds), 500), ] # - create a dataset based on germany but with only one variable: name_2 germany_name2 &lt;- dplyr::select(germany, NAME_2) # - spatial join birds_name2 &lt;- st_join(birds_subset, germany_name2) mapview(birds_name2, zcol = &quot;NAME_2&quot;, legend = FALSE) 1.4.6 Spatial Aggregation The last thing we want to look at is how we aggregate data. The question here might be: What is the mean abundance of birds in the different counties. So we want to: 1. group the bird data according to which district they fall into. 2. calculate for each group the mean value of the abundance (individualCount). 3. assign these mean values to the counties in a data set. We can do all this with a single function: aggregate(). However, we need to prepare the data a bit for this. aggregate() needs the following arguments: x which data should I aggregate? In our case it is the birds. by in which data are the groups in which I should aggregate? For us it is germany. FUN with which function should I aggregate the data? For us mean(), the mean value. If you want to give arguments to the function you use to aggregate you can do that afterward. # - Create a data set where the tits have only the variable individualCount. birds_count &lt;- dplyr::select(birds, individualCount) # - Reduce to rows that have information for the variable individualCount. birds_count &lt;- dplyr::filter(birds_count, !is.na(individualCount)) # - aggregate the data birds_count using germany with the function mean. birds_agg &lt;- aggregate(x = birds_count, by = germany, FUN = mean) # - If we didn&#39;t remove the NAs we could use the following function: birds_agg &lt;- aggregate(x = birds_count, by = germany, FUN = mean, na.rm = TRUE) # - The argument na.rm = TRUE is an argument of the function mean(). It removes (remove, rm) all NAs before calculating the mean. If you open the help page of aggregate() you will see &quot;...&quot; at the arguments. These so called ellipsis are placeholders for all arguments you can give to the function in FUN. mapview(birds_agg, zcol = &quot;individualCount&quot;) References Appelhans, Tim, Florian Detsch, Christoph Reudenbach, and Stefan Woellauer. 2021. “Mapview: Interactive Viewing of Spatial Data in r.” Lovelace, Robin, Jakub Nowosad, and Jannes Muenchow. 2019. Geocomputation with r. CRC Press. Pebesma, Edzer. 2018. “Simple Features for R: Standardized Support for Spatial Vector Data.” The R Journal 10 (1): 439. https://doi.org/10.32614/RJ-2018-009. Pebesma, Edzer, and Roger Bivand. 2022. Spatial Data Sciencewith Applications in r. https://r-spatial.org/book/. Pebesma, Edzer, Thomas Mailund, and James Hiebert. 2016. “Measurement Units in R.” R Journal 8 (2): 486–94. https://doi.org/10.32614/RJ-2016-061. Tennekes, Martijn. 2018. “tmap: Thematic Maps in R.” Journal of Statistical Software 84 (6): 1–39. https://doi.org/10.18637/jss.v084.i06. Wickham, Hadley. 2016. Ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org. Wickham, Hadley, Romain François, Lionel Henry, and Kirill Müller. 2022. Dplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr. "],["point-pattern-analysis.html", "Chapter 2 Point Pattern Analysis 2.1 the ppp class 2.2 First order processes 2.3 Second order processes", " Chapter 2 Point Pattern Analysis In this script, we will explore point pattern analysis (ppa) in R. The most common package for ppa in R is spatstat (Baddeley and Turner 2005). Other introductions to ppa and spatstat in specific are Chapter 4 in Fletcher and Fortin (2018), Chapter 11 in Pebesma and Bivand (2022), Baddeley and Turner (2005), and Baddeley, Rubak, and Turner (2015). There are some less common package that we will not delve into here but which I do want to mention as they might be helpful to you at some future point: ads (Pélissier and Goreaud 2015), ecespa (Cruz Rot 2008), splancs (Rowlingson and Diggle 2022), and stpp (Gabriel et al. 2022). 2.1 the ppp class spatstat introduced its own object class to R, the ppp class. Most functions we will work with in the following script only work with this object class. To illustrate the package we will use the a data set of bird occurrences in Germany we already used in the first chapter, the Database of Global Administrative Areas (GADM) which we also already used in the first lecture and a DEM of Rhineland Palatinate ( download here). library(pacman) p_load(sf, spatstat, mapview, dplyr, ggplot2, magrittr, terra, raster, maptools, data.table) birds &lt;- readRDS(&quot;data/birds.rds&quot;) gadm &lt;- st_read(&quot;data/gadm36_DEU_3_pk.gpkg&quot;) ## Reading layer `gadm36_DEU_3_pk&#39; from data source ## `C:\\Users\\jonat\\Documents\\001_Uni\\002_teaching\\online books\\book_spatial_data_science_in_R\\data\\gadm36_DEU_3_pk.gpkg&#39; ## using driver `GPKG&#39; ## Simple feature collection with 4680 features and 16 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: 5.866251 ymin: 47.27012 xmax: 15.04181 ymax: 55.05653 ## Geodetic CRS: WGS 84 DEM &lt;- rast(&quot;data/DTM Germany_Rheinland-Pfalz 20m.tif&quot;) We will need to prepare the bird data before we can start with the analyses. First, we remove all entries with missing coordinates. birds2 &lt;- birds[!is.na(birds$decimalLatitude), ] Then we remove duplicate observations, so that there is only one observation per location. This is easiest with the unique() function from the data.table package (Dowle and Srinivasan 2021). While base R also has a unique() function, the base R version only works with vectors. It returns all unique values of a vector. The data.table version works with data frames and returns only rows with unique values in the selected columns. The columns are selected with the by argument. We choose the columns that hold the coordinates decimalLatitude and decimalLongitude. birds2 &lt;- unique(birds2, by = c(&quot;decimalLatitude&quot;, &quot;decimalLongitude&quot;)) Now we can turn the data frame into an sf object. birds2 &lt;- st_as_sf(birds2, coords = c(&quot;decimalLongitude&quot;, &quot;decimalLatitude&quot;), crs = 4326) Lastly, we subset the data to only observations within Rhineland Palatinate. birds2 &lt;- st_filter(birds2, filter(gadm, NAME_1 == &quot;Rheinland-Pfalz&quot;)) Let’s have a look. mapview(birds2) These bird observations are our starting point for the ppa. We can use the as.ppp() function to convert the sf object into a ppp. There is one caveat however: the data need to be in a projected coordinate system. Otherwise as.ppp() will return an error. Currently, our data are in a geographic coordinate system (latitude and longitude) so we need to transform them first. I choose ETRS89 / UTM zone 32N (N-E) (EPSG 3044) because that is the CRS of the DEM we will use later. birds2 &lt;- st_transform(birds2, crs = &quot;EPSG:3044&quot;) We will not cover marked point patterns in this tutorial. Therefore, we need to remove all variables except for the geometry. birds2 &lt;- dplyr::select(birds2, geometry) Now we can create the ppp. birds_ppp &lt;- as.ppp(birds2) birds_ppp ## Planar point pattern: 9547 points ## window: rectangle = [300361, 463668.4] x [5426570, 5641700] units We see that we have a planar (i.e. 2 dimensional) point pattern with 9547 point which is equal to the number of rows in birds2. Besides the points, the ppp object also contains the window, the rectangle in that includes all points. We can plot the ppp using the base plot function. plot(birds_ppp) The window can be queried separately through the Window() function. The resulting object has the class owin (observation window). x &lt;- Window(birds_ppp) class(x) ## [1] &quot;owin&quot; 2.2 First order processes The most basic first order property is the density, the number of points in an area. We can use the quadratcount() function to separate our window in quadrants and count the points in each. The number of quadrant rows and columns is determined through the arguments nx and ny. First, we compute the global density so just a single quadrant, hence both nx and ny are one. Q0 &lt;- quadratcount(birds_ppp, nx= 1, ny=1) plot(birds_ppp, pch=20, cols=&quot;grey70&quot;, main=NULL) plot(Q0, add=TRUE) For the plot I have adjusted the symbol (pch), point color (cols) and plot title (main). We first plot the points (birds_ppp) and then add the quardrants (or just quadrant in this case) with Q0. Notice the add=TRUE this adds the elements from Q0 to the plot of birds_ppp instead of creating a new plot. By varying the number of quadrants we can alter the number of local densities we compute. Q1 &lt;- quadratcount(birds_ppp, nx= 2, ny=2) Q2 &lt;- quadratcount(birds_ppp, nx= 3, ny=6) Q3 &lt;- quadratcount(birds_ppp, nx= 10, ny=20) plot(birds_ppp, pch=20, cols=&quot;grey70&quot;, main = NULL) plot(Q1, add=TRUE) plot(birds_ppp, pch=20, cols=&quot;grey70&quot;, main = NULL) plot(Q2, add=TRUE) plot(birds_ppp, pch=20, cols=&quot;grey70&quot;, main = NULL) plot(Q3, add=TRUE) Exercise use data(“bei”) to load a second point pattern. use ?bei to learn more about the data compute the density of bei in a 5X6 grid The intensity is the density per area and can be computed by intensity(). Q0.d &lt;- intensity(Q0, image = T) Q1.d &lt;- intensity(Q1, image = T) Q2.d &lt;- intensity(Q2, image = T) Q3.d &lt;- intensity(Q3, image = T) # density raster plot(Q0.d, main = NULL) plot(Q1.d, main = NULL) plot(Q2.d, main = NULL) plot(Q3.d, main = NULL) The unit - points per square meter - is not very intuitive. We can change it to points per square kilometer with the rescale() function. birds_km &lt;- spatstat.geom::rescale(X = birds_ppp, s = 1000, unitname = &quot;km&quot;) Now we get the same results but in the new and much more intuitive units. Q &lt;- quadratcount(birds_km, nx= 10, ny=20) Q.d &lt;- intensity(Q, image = T) plot(Q.d, main = NULL) We can also use predefined areas to compute density and intensity. We will go through two different approaches here. The first is based on a categorized raster and the second on a polygon layer. The categorized raster is the DEM of Rhineland Palatinate we loaded in the beginning. As the raster is quite large we will aggregate cells to increase cell sizes and decrease the resolution. We loaded the raster with the rast() function from the terra package (Hijmans 2022b). terra is the leading package for working with raster data in R at the moment (see here for an introduction) . It supersedes the raster package (Hijmans 2022a) which was the standard package before. Each of the two packages has its own object class to store the raster internally. The raster package uses the class RasterLayer and the terra package uses the class SpatRaster. Some packages have not adjusted to the switch to terra yet and still require RasterLayer objects. This is also the case for spatstat, which in turn again uses its own raster object class im. Thus, we need to transform the SpatRaster to a RasterLayer which can be done with the function raster() and transform the RasterLayerto an im with as.im(). Please note that the maptools package needs to be loaded for the as.im() function to work with raster objects. DEM2 &lt;- terra::aggregate(DEM, fact = 4) ## |---------|---------|---------|---------|========================================= DEM3 &lt;- raster(DEM2) DEM4 &lt;- as.im(DEM3) DEM4 is still a continuous raster where each cell stores an elevation value. Now we will create a categorical raster which stores elevation classes instead of elevation values. First, we need to define the breaks, i.e., which elevation values correspond to which classes. As we have no a priori classification in mind here we will use the quartiles of the elevation values. summary(values(DEM2)) ## DTM Germany_Rheinland-Pfalz 20m ## Min. : 27.7 ## 1st Qu.:223.1 ## Median :318.2 ## Mean :318.1 ## 3rd Qu.:415.0 ## Max. :815.9 ## NA&#39;s :2787887 breaks &lt;- c( -Inf, 223, 318, 415 , Inf) With the cut() function we assign the cells of DEM4 to four classes labeld 1 to 4 according to the breaks defined in breaks. Afterwards we need to transform this to another object class one more time with tess(). elev_class &lt;- cut(DEM4, breaks = breaks, labels = 1:4) elev_class &lt;- tess(image = elev_class) plot(elev_class) Now we can use the classified elevation raster as quadrants for the quadratcount() function. Q_elev &lt;- quadratcount(birds_ppp, tess = elev_class) plot(Q_elev) Q_elev2 &lt;- intensity(Q_elev, image = T) plot(Q_elev2) Based on this we can already see, that the number of observations tends to decrease along the elevation classes. Exercise the elevation data for the point pattern is available under bei.extra$elev it already is an im object. create four elevation classes compute and plot the intensity in each class As a second example, we will use sf polygons to define quadrants. Here we will do so with the districts within Rhineland Palatinate. In the end, we still need to provide quadratcount()with an object of class tess. This can be created with # subset all of Germany to just RLP rlp &lt;- filter(gadm, NAME_1 == &quot;Rheinland-Pfalz&quot;) # assign new CRS that conforms the to CRS of birds rlp &lt;- st_transform(rlp, 3044) # Aggregate smaller district units to NAME_2 level rlp %&lt;&gt;% group_by(NAME_2) %&gt;% summarise(geom = st_union(geom)) # sf polygons can be transformed to owin objects with as.owin(). # We create a list where each object is one entry is one district as owin. rlp_list &lt;- list() for(i in 1:nrow(rlp)){ rlp_list[[i]] &lt;- as.owin(rlp[i, ]) } # convert owin to tess rlp_owin &lt;- as.tess(rlp_list) Q &lt;- quadratcount(birds_ppp, tess = rlp_owin) cl &lt;- interp.colours(c(&quot;lightyellow&quot;, &quot;orange&quot; ,&quot;red&quot;), length(rlp_list)) plot(Q, col = cl) Q.d &lt;- intensity(Q, image = T) plot(Q.d, col = cl, main = NULL) 2.2.1 Kernel density We can create continuous density surfaces with kernel density estimation. The image below is similar to the intensity plots above but the individuals cells are far smaller. Notice that we have he inconvenient unit of observations per square meter again, thus the small numbers. K1 &lt;- density(birds_ppp) plot(K1, main = NULL) contour(K1, add=TRUE) The bandwidth influences how far the influence of a single observation extends. If we choose a small bandwidth the kernel will only estimate higher densities in direct viccinity of the observations. If we choose a larger bandwidth the density estimations will increase for more removed parts. How exactly that works depends on the kernel function. For the Gaussian Kernel, which is the default setting in density() the function looks like this: \\[G(x, \\sigma) = \\frac{1}{\\sqrt{2\\pi\\sigma}} exp(-\\frac{x^2}{2\\sigma^2})\\] This is the same formula as for the normal probability distribution, where the bandwidth corresponds to the standard deviation. So you can imagine having a normal distribution centered on every observation. The weight of this observation, how much it adds to the estimated density it of a nearby point decreases with the distance to the observation. The speed with which it decreases corresponds to the standard deviation of my normal distribution and hence to the bandwidth. Hopefully Figure @ref{fig:} Figure 2.1: Effect of bandwidth in a gaussian Kernel In R, we can adjust the bandwidth with the sigma argument. To illustrate the effect of the bandwidth on the density maps, we create six different kernels with increasing bandwidths. par(mfrow = c(2,3)) for (i in seq(10000, 60000, by = 8433.33)) { x = density(birds_ppp, sigma = i) plot(x, main = paste0(&quot;bandwidth =&quot;, i)) contour(x, add = T) } We can also compare different kernel functions in the same way. For the density() function from the Raster package, there are four different kernel functions available: 1) Gaussian (the default option), 2) Quartic 3) Disc and 4) Epanechnikov. par(mfrow = c(2,2), mar = c(0,0,0,0)) for (i in c(&quot;gaussian&quot;, &quot;quartic&quot;, &quot;disc&quot;, &quot;epanechnikov&quot;)) { x = density(birds_ppp, sigma = 20000, kernel = i) plot(x, main = paste0(&quot;Kernel =&quot;, i)) contour(x, add = T) } Exercise try different Kernels with different bandwidths on the bei data set which combination do you think gives a good representation of the data? Next, we turn to some models that estimate the relationship between the intensity of a point pattern one some other variable. One such function is the rhohat() function. The name derived from the greek letter \\(\\rho\\) that is commonly used for intensity and the custom to mark mathematical estimates with a hat. So \\(\\hat{\\rho}\\) is an estimate of \\(\\rho\\). The rhohat() function fits a non-parametric model to the intensity that does not assume a specific functional form like linear or quadratic. As arguments it takes the ppp for which we want to model the intensity (object), the variable we want to predict the intensity with (covariate) rho &lt;- rhohat(birds_ppp, DEM4, method =&quot;ratio&quot;) plot(rho) We see four different lines: \\(\\hat{\\rho}\\) is the estimated mean intensity as function of the elevation, \\(\\bar{\\rho}\\) the overall mean instensity, \\(\\rho_{hi}\\) and \\(\\rho_{lo}\\) are the upper and lower bound of the 95% confidence interval respectively. We can use this model to predict the density for each cell of DEM4. pred &lt;- predict(rho) plot(raster(pred)) Again, we see that the intensity is highest in lowland areas and decreases with increasing elevation. Exercise compute rho hat and for the bei data and plot the predicted values Alternatively, we can use a Poisson point process model (PPM) for prediction. The poission point process follow as functional form. It assumes that the number of points within a given region follow the Poisson distribution. The model is written out as: \\[\\mathbf{P}(N(B)=n)=\\frac{\\lambda^{n}(\\nu(B))^{n}}{n !} \\exp (-\\lambda \\nu(B))\\] Where \\(\\mathbf{P}(N(B)=n)\\) is the probability in the area \\(B\\) the number of points (\\(N(B)\\)) equals \\(n\\). \\(\\lambda\\) is the intensity, the rate parameter of the Poission distribution, and therefore the parameter that is estimated when we fit the model. \\(\\nu(B)\\) is the area of \\(B\\). When running this command, there may be a warning message. This is not a problem. PPM1 &lt;- ppm(birds_ppp ~ DEM4) ## Warning: Values of the covariate &#39;DEM4&#39; were NA or undefined at 33% (16530 out of 49551) of the quadrature ## points. Occurred while executing: ppm.ppp(Q = birds_ppp, trend = ~DEM4, data = NULL, interaction = NULL) The effectfun() function now calculates the trend in bird intensity with increasing elevation. PPM1_effect &lt;- effectfun(PPM1, &quot;DEM4&quot;, se.fit=TRUE) plot(PPM1_effect, las=1) Again, we have a mean estimate as a function of the elevation (\\(\\hat{\\lambda}\\)), as well as higher and lower bounds of the 95% confidence interval (\\(\\lambda_{hi}\\) and \\(\\lambda_{lo}\\)). The general form is similar to the non-parametric model, but the decline in intensity is slower. Exercise compute the poisson point model for the bei data note that the raster needs to be assigned to a new object before Do results differ between rhohat and ppm? Both methods thus come to similar conclusions: par(mfrow = c(1,2)) plot(rho, main = &quot;non-parametric model&quot;) plot(PPM1_effect, main = &quot;PPM&quot; ) pred_ppm &lt;- predict(PPM1) plot(raster(pred), col = cl, main = &quot;non-parametric model&quot;) plot(pred_ppm, col = cl, main = &quot;PPM&quot;) 2.3 Second order processes Now we can turn to the second order processes, i.e., to metrics and functions that consider the location of point with respect to other points. 2.3.1 Average nearest neighbor For the average nearest neighbor (ANN) method we use the nndist() function. It calculates the distance from each point to the next closest. The argument k indicates in which neighbor we are interested. The nearest neighbor is found with k=1. With k = 2 we would calculate the distance to the second nearest neighbor. # - compute nearest neighbor NN &lt;- nndist(birds_ppp, k = 1) # - average distance to nearest neighbor over all points ANN &lt;- mean(NN) plot(sort(NN), type = &quot;l&quot;) abline(h = ANN, col = &quot;red&quot;, lwd = 2) We can see that most points are close to other ones. Only a few points are isolated with distances of up to approximately 10 kilometers to the closest observation. We have 9547 observations and thus there are 9546 (n-1) potential distance classes (i.e., values for k). This is more than we need here and we will content ourselves with 999 distance classes, to see how distance increases with increasing degrees of neighborhood. To do so, we call nndist() and provide a vector to the k argument. The function computes the distance for each observation and each value of k and returns them in a matrix. Each row is one observation and each column is one value of k. As we are interested in the mean distance per k value, we summarize this with the apply() function. apply() takes the matrix we compute with nndist() and applies the function in FUN to each column. It applies the function to columns because we set the MARGIN to 2. If we would have set the MARGIN to 1 apply() would have computed row-wise means. n &lt;- 999 NN &lt;- nndist(birds_ppp, k = 1:n) ANN &lt;- apply(X = NN, MARGIN = 2, FUN = mean) plot(ANN ~ eval(1:n), type = &quot;l&quot;, main=NULL, las=1, xlab = &quot;k&quot;) As would be expected, the distance to the next neighbor increases with k. There are no noticeable changes in the slop of the line. Hence there are no groups that are closely connected within and distanced between points. Exercise compute and visualize average nearest neighbor distances for k 1 to 300 for the bei data 2.3.2 The K-Funktion Next we will use the K,L and G functions. All three are implemented as functions in spatstat. K &lt;- Kest(birds_ppp, correction = &quot;isotropic&quot;) plot(K) K is always larger than that of a poisson distributed pattern. This indicates clustered observations. This is confirmed by the L function. The term . -r ~ r is necessary to set the line straight to zero. L &lt;- Lest(birds_ppp, correction = &quot;isotropic&quot;) plot(L, . -r ~ r) Lastly, the G-function G &lt;- Gest(birds_ppp) plot(G) For the G-Function, different estimators are displayed. The first three lines are different estimators of the G-Function and are generally in agreement in this case. The last line (\\(G_{pois}\\)) is the G-Function of a poisson point process. As with the K and the L function before, that fact that the estimates for our data are higher than that for the poisson processes point to the fact, that our data are clustered. :::: {.blackbox data-latex=““} ::: {.center data-latex=”“} Exercise ::: Use K, L, and G to determine whether the trees in bei are aggregated or dispersed. :::: 2.3.3 Morans I Lastly, we will have a look at Moran’s I a function for that looks for spatial autocorrelation. This will be different from the other function we covered in this tutorial so far, because it requires a different package spdep (Bivand, Pebesma, and Gómez-Rubio 2013) and we will need a mark. Thus we need to go back to the bird data and keep the individualCount variable. library(spdep) ## Lade nötiges Paket: spData ## To access larger datasets in this package, install the spDataLarge package with: ## `install.packages(&#39;spDataLarge&#39;, repos=&#39;https://nowosad.github.io/drat/&#39;, type=&#39;source&#39;)` # - We dont need to specify the geometry column in the call # - to select because it is sticky. birds3 &lt;- birds |&gt; filter(!is.na(decimalLatitude) &amp; !is.na(individualCount)) |&gt; st_as_sf(coords = c(&quot;decimalLongitude&quot;, &quot;decimalLatitude&quot;), crs = 4326) |&gt; st_filter(filter(gadm, NAME_1 == &quot;Rheinland-Pfalz&quot;)) |&gt; dplyr::select(count = individualCount) # - Add x and y coordinates as individual variables. birds3 %&lt;&gt;% mutate(x.coord = st_coordinates(birds3)[,1], y.coord = st_coordinates(birds3)[,2]) # - Turn birds3 into a data table so we can use the unique() function. setDT(birds3) birds3 %&lt;&gt;% unique(by = c(&quot;x.coord&quot;, &quot;y.coord&quot;)) %&gt;% st_as_sf() %&gt;% st_drop_geometry() After preparing the bird data, we need to create a neighborhood list for our data. This works in three steps: 1. identify the nearest (or k\\(^{th}\\)) neighbor with knearneigh(). 2. Turn knn object created by knearneigh() into a neighbors list of class nb with knn2nb(). 3. Add spatial weights nb with nb2listw(). For our case, spatial weights simply reinforce the neighborhood scheme. So only the nearest neighbor is weighted and all other observations have a weight of zero. Then finally, we can use moran.test() to compute Moran’s I. We provide the function with the variable of interest (the mark) and the weighted list of neighbor. knn &lt;- knearneigh(birds3, k = 1) nb &lt;- knn2nb(knn) listw &lt;- nb2listw(nb) moran.test(x = birds3$count, listw = listw) ## ## Moran I test under randomisation ## ## data: birds3$count ## weights: listw ## ## Moran I statistic standard deviate = 23.907, p-value &lt; 2.2e-16 ## alternative hypothesis: greater ## sample estimates: ## Moran I statistic Expectation Variance ## 0.978453524 -0.001047120 0.001678658 From these results we can see that close points are more similar than would be expected by chance (p-value &lt;0.05). Remember that a Moran’s I of 0 would indicate random dispersal, 1 perfect clustering, and -1 perfect dispersal. References Baddeley, Adrian, Ege Rubak, and Rolf Turner. 2015. Spatial Point Patterns: Methodology and Applications with r. CRC press. Baddeley, Adrian, and Rolf Turner. 2005. “Spatstat : An R Package for Analyzing Spatial Point Patterns.” Journal of Statistical Software 12 (6). https://doi.org/10.18637/jss.v012.i06. Bivand, Roger S., Edzer Pebesma, and Virgilio Gómez-Rubio. 2013. Applied Spatial Data Analysis with R. New York, NY: Springer New York. https://doi.org/10.1007/978-1-4614-7618-4. Cruz Rot, Marcelino de la. 2008. “Metodos Para Analizar Datos Puntuales.” In Introduccion Al Analisis Espacial de Datos En Ecologia y Ciencias Ambientales: Metodos y Aplicaciones., edited by Fernando T. Maestre, Adrian Escudero, and Andreu Bonet, 76–127. Asociacion Espanola de Ecologia Terrestre, Universidad Rey Juan Carlos; Caja de Ahorros del Mediterraneo. Dowle, Matt, and Arun Srinivasan. 2021. Data.table: Extension of ‘Data.frame‘. https://CRAN.R-project.org/package=data.table. Fletcher, Robert, and Marie-Josée Fortin. 2018. Spatial Ecology and Conservation Modeling: Applications with R. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-030-01989-1. Gabriel, Edith, Peter J Diggle, Barry Rowlingson, and Francisco J Rodriguez-Cortes. 2022. Stpp: Space-Time Point Pattern Simulation, Visualisation and Analysis. https://CRAN.R-project.org/package=stpp. Hijmans, Robert J. 2022a. Raster: Geographic Data Analysis and Modeling. https://CRAN.R-project.org/package=raster. ———. 2022b. Terra: Spatial Data Analysis. https://CRAN.R-project.org/package=terra. Pebesma, Edzer, and Roger Bivand. 2022. Spatial Data Sciencewith Applications in r. https://r-spatial.org/book/. Pélissier, Raphaël, and François Goreaud. 2015. “ads Package for R: A Fast Unbiased Implementation of the \\(K\\)-Function Family for Studying Spatial Point Patterns in Irregular-Shaped Sampling Windows.” Journal of Statistical Software 63 (6): 1–18. http://dx.doi.org/10.18637/jss.v063.i06. Rowlingson, Barry, and Peter Diggle. 2022. Splancs: Spatial and Space-Time Point Pattern Analysis. https://CRAN.R-project.org/package=splancs. "],["interpolation.html", "Chapter 3 Interpolation 3.1 The data 3.2 Proximity Polygons 3.3 Trend Surface Analysis 3.4 Splines 3.5 Weighted averaging 3.6 Kriging", " Chapter 3 Interpolation In this tutorial, you will learn how to use R to apply different interpolation methods to your data sets. We will use couple of R packages that are familiar to us, like sf and mapview but also some new ones. Most notably, we will use fields (Douglas Nychka et al. 2021) for splines and gstat (Gräler, Pebesma, and Heuvelink 2016) as well as automap (Hiemstra et al. 2008) for distance weighting. library(automap) library(ggplot2) library(sf) library(sp) library(dplyr) library(fields) library(gstat) library(tmap) library(magrittr) library(mapview) 3.1 The data We will use the LUCAS soil data. LUCAS is an acronym for Land Use/Land Cover Area Frame Survey. This data base contains approximately 20.000 soil samples from all EU27 countries with the exceptions of Romania and Bulgaria. You can download a subset of the LUCAS data base I created for this course here. Next, we load the LUCAS data. lucas &lt;- readRDS(&quot;data/lucas_saxony.rds&quot;) As always, we first inspect the new object. class(lucas) ## [1] &quot;sf&quot; &quot;data.frame&quot; The glimpse() function from dlyr is similar to the str() from base R. glimpse(lucas) ## Rows: 79 ## Columns: 23 ## $ Point_ID &lt;int&gt; 44743044, 44763132, 44763142, 44883126, 44923048, 44923066, 44923084, 44923152, 44963130, 44… ## $ Coarse &lt;int&gt; 34, NA, NA, NA, 23, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ Clay &lt;int&gt; 22, NA, NA, NA, 22, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ Sand &lt;int&gt; 26, NA, NA, NA, 18, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ Silt &lt;int&gt; 52, NA, NA, NA, 60, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ pH_CaCl2 &lt;dbl&gt; 5.4, 6.5, 6.6, 7.3, 5.3, 6.5, 6.0, 6.8, 4.5, 5.9, 5.8, 5.9, 6.0, 5.5, 4.4, 6.1, 5.2, 4.1, 3.… ## $ pH_H20 &lt;dbl&gt; 5.77, 6.55, 6.82, 7.70, 5.53, 6.83, 6.14, 7.00, 4.66, 5.94, 6.10, 5.96, 6.18, 5.80, 4.57, 6.… ## $ EC &lt;dbl&gt; 8.52, 22.00, 17.34, 24.90, 38.90, 9.32, 10.88, 20.50, 46.00, 50.50, 15.48, 122.00, 54.10, 59… ## $ OC &lt;dbl&gt; 36.3, 19.9, 15.7, 20.6, 34.1, 17.2, 18.1, 11.1, 97.8, 31.8, 10.2, 19.0, 60.1, 22.0, 40.3, 12… ## $ CaCO3 &lt;int&gt; 0, 2, 0, 16, 0, 1, 0, 2, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 2, 0, 0, 0, 3, 0, 2… ## $ P &lt;dbl&gt; 60.0, 48.2, 67.7, 4.6, 45.0, 44.1, 49.2, 33.7, 30.6, 18.6, 51.8, 96.1, 64.7, 57.3, 22.2, 25.… ## $ N &lt;dbl&gt; 4.1, 2.1, 1.8, 0.9, 4.3, 1.9, 1.9, 1.4, 5.7, 3.5, 1.2, 2.6, 6.3, 2.6, 2.1, 1.1, 2.7, 2.2, 10… ## $ K &lt;dbl&gt; 103.4, 192.1, 265.8, 90.1, 328.2, 123.8, 197.9, 149.4, 77.7, 91.1, 162.4, 724.9, 1752.9, 81.… ## $ LC &lt;chr&gt; &quot;E10&quot;, &quot;B11&quot;, &quot;B11&quot;, &quot;C10&quot;, &quot;E20&quot;, &quot;B13&quot;, &quot;B55&quot;, &quot;B11&quot;, &quot;C10&quot;, &quot;E20&quot;, &quot;B11&quot;, &quot;B11&quot;, &quot;E20&quot;, &quot;… ## $ LU &lt;chr&gt; &quot;U111&quot;, &quot;U111&quot;, &quot;U111&quot;, &quot;U120&quot;, &quot;U111&quot;, &quot;U111&quot;, &quot;U111&quot;, &quot;U111&quot;, &quot;U120&quot;, &quot;U111&quot;, &quot;U111&quot;, &quot;U11… ## $ NUTS_0 &lt;chr&gt; &quot;DE&quot;, &quot;DE&quot;, &quot;DE&quot;, &quot;DE&quot;, &quot;DE&quot;, &quot;DE&quot;, &quot;DE&quot;, &quot;DE&quot;, &quot;DE&quot;, &quot;DE&quot;, &quot;DE&quot;, &quot;DE&quot;, &quot;DE&quot;, &quot;DE&quot;, &quot;DE&quot;, &quot;D… ## $ NUTS_1 &lt;chr&gt; &quot;DED&quot;, &quot;DED&quot;, &quot;DED&quot;, &quot;DED&quot;, &quot;DED&quot;, &quot;DED&quot;, &quot;DED&quot;, &quot;DED&quot;, &quot;DED&quot;, &quot;DED&quot;, &quot;DED&quot;, &quot;DED&quot;, &quot;DED&quot;, &quot;… ## $ NUTS_2 &lt;chr&gt; &quot;DED4&quot;, &quot;DED5&quot;, &quot;DED5&quot;, &quot;DED5&quot;, &quot;DED4&quot;, &quot;DED4&quot;, &quot;DED4&quot;, &quot;DED5&quot;, &quot;DED5&quot;, &quot;DED4&quot;, &quot;DED5&quot;, &quot;DED… ## $ NUTS_3 &lt;chr&gt; &quot;DED44&quot;, &quot;DED52&quot;, &quot;DED53&quot;, &quot;DED52&quot;, &quot;DED44&quot;, &quot;DED45&quot;, &quot;DED45&quot;, &quot;DED53&quot;, &quot;DED52&quot;, &quot;DED45&quot;, &quot;D… ## $ LC0_Desc &lt;chr&gt; &quot;Grassland&quot;, &quot;Cropland&quot;, &quot;Cropland&quot;, &quot;Woodland&quot;, &quot;Grassland&quot;, &quot;Cropland&quot;, &quot;Cropland&quot;, &quot;Cropl… ## $ LC1_Desc &lt;chr&gt; &quot;Grassland with sparse tree/shrub cover&quot;, &quot;Common wheat&quot;, &quot;Common wheat&quot;, &quot;Broadleaved woodl… ## $ LU1_Desc &lt;chr&gt; &quot;Agriculture (excluding fallow land and kitchen gardens)&quot;, &quot;Agriculture (excluding fallow la… ## $ geometry &lt;POINT [°]&gt; POINT (12.15614 50.48765), POINT (12.22172 51.27776), POINT (12.22607 51.3676), POINT … We use the mapview package to display the data on an interactive map. mapview(lucas) We will need a layer of the the German federal state saxony to prepare maps later on. We get it from the Database of Global Administrative Areas (GADM). We have used the GADM data for Germany before (gadm36_DEU_3_pk.gpkg). You can download them here. You can always query other countries or resolutions by following this tutorial. saxony &lt;- st_read(&quot;data/gadm36_DEU_3_pk.gpkg&quot;) |&gt; st_as_sf() |&gt; filter(NAME_1 == &quot;Sachsen&quot;) ## Reading layer `gadm36_DEU_3_pk&#39; from data source ## `C:\\Users\\jonat\\Documents\\001_Uni\\002_teaching\\online books\\book_spatial_data_science_in_R\\data\\gadm36_DEU_3_pk.gpkg&#39; ## using driver `GPKG&#39; ## Simple feature collection with 4680 features and 16 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: 5.866251 ymin: 47.27012 xmax: 15.04181 ymax: 55.05653 ## Geodetic CRS: WGS 84 mapview(saxony) 3.2 Proximity Polygons We start out with proximity polygons. We can create Voronoi polygons around our points with st_voronoi(). #- st_voroni works better with projected coordinate reference systems voroni &lt;- st_voronoi(lucas) ## Warning in st_voronoi.sfc(st_geometry(x), st_sfc(envelope), dTolerance, : st_voronoi does not correctly ## triangulate longitude/latitude data The warning messages notifies us that it its advisable to use projected coordinates, thus we transform our data from WGS84 to Lamber Azimuthal Eqal Area. lucas %&lt;&gt;% st_transform(3035) saxony %&lt;&gt;% st_transform(3035) With the following code we create Voronoi polygons around the observations in lucas and then keep only those areas of the polygons that intersect with the saxony polygon. As you can see we need more functions than just st_voronoi() to do this. With st_union() we combine the single POINT objects in lucas to a single MULTIPOINT object. With st_voronoi() we can now create the Voronoi polygons which are returned in the GEOMERTRYCOLLECTION. To extract the polygons from this collection, we can use the st_collection_extract() function. Its is well worth your time to execute the steps one by one and to check out the intermediate products. voroni &lt;- lucas |&gt; st_union() |&gt; st_voronoi() |&gt; st_collection_extract() |&gt; st_intersection(y= saxony) |&gt; st_as_sf() Next, we add the electrical conductivity (EC, our focal variable for this example) to the respective polygons. We assign a point to each polygon with the st_nearest_feature() function. id &lt;- st_nearest_feature(voroni, lucas) voroni$EC &lt;- lucas$EC[id] Now we have the final product also shown in the lecture: mapview(voroni, zcol = &quot;EC&quot;) + mapview(lucas, zcol = &quot;EC&quot;, legend = F) Exercise Create proximity polygons for the variable Phosphorus 3.3 Trend Surface Analysis Trend surface analysis (TSA) is a multiple regression in which EC is explained by spatial coordinates. We can use the base R regression function lm() for TSA. To prepare the TSA, we need to extract the coordinates from the geometry column of lucas. For this we use the st_coordinates() function. coords &lt;- st_coordinates(lucas) x.coord &lt;- coords[,1] y.coord &lt;- coords[,2] lucas &lt;- mutate(lucas, x = x.coord, y = y.coord ) Then we conduct a simple linear regression and also a polynomial regression with polynomial of second degree. tsa1 &lt;- lm(EC ~ x+y, data = lucas) tsa2 &lt;- lm(EC ~ polym(x, y, degree=2), data = lucas) To predict the values of unobserved locations with the TSA we need the coordinates of these locations. Here, I show to different approaches: i) randomly distributed point in Saxony and ii) regularly spaced points. The randomly distributed points are created with st_sample(). The first argument to this function is the bounding box, i.e., the area in which the point can lie. The second argument is the number of points. Here, we create 300 new points in Saxony. random_points &lt;- st_sample(saxony, 300) mapview(random_points) Again, we have to extract the coordinates from the geometry column with st_coordinates(). Currently the random_points object is still of class sfc. This means it is only a sf column and not a table to which we can add columns. To create a sf table, we first need to use the st_as_sf() function. The geometry columns of sf objects are typically called geom or geometry. However, after calling st_as_sf() the column is called \"x\". We rename it with the rename() function available in the dplyr package. Lastly, we add the coordinates. random_points &lt;- random_points |&gt; st_as_sf() |&gt; rename(geom = x) |&gt; mutate(x = st_coordinates(random_points)[,1], y = st_coordinates(random_points)[,2]) We predict the EC for these coordinates with predict() random_points$EC_tsa1 &lt;- predict(tsa1, random_points) random_points$EC_tsa2 &lt;- predict(tsa2, random_points) Let’s have a look at the results. On the following map circles are true observaitons and diamonds are predicted values. breaks = c(0, 10, 15,20, 25, 30, 35, 40, 170) tmap_mode(&quot;plot&quot;) ## tmap mode set to plotting tm_shape(saxony) + tm_polygons() + tm_shape(random_points) + tm_dots(col = &quot;EC_tsa1&quot;, shape = 23, size = .5, breaks = breaks) + tm_shape(lucas) + tm_dots(col = &quot;EC&quot;, shape = 21, size = .5, breaks = breaks) + tm_layout(legend.outside = TRUE) tm_shape(saxony) + tm_polygons() + tm_shape(random_points) + tm_dots(col = &quot;EC_tsa2&quot;, shape = 23, size = .5, breaks = breaks, midpoint = NA) + tm_layout(legend.outside = TRUE) + tm_shape(lucas) + tm_dots(col = &quot;EC&quot;, shape = 21, size = .5, breaks = breaks) Exercise Predict phosphorus concentrations at 150 random locations in Saxony with a non-polynomial trend surface analysis. Now to the regularly spaced points. First we need to get the bounding box of saxony. We can get the bounding box with st_bbox(). saxony_bbox &lt;- st_bbox(saxony) saxony_bbox ## xmin ymin xmax ymax ## 4453658 3009238 4672526 3178781 With st_as_sfc() we can create a polygon from the bounding box. saxony_bbox_sfc &lt;- st_as_sfc(saxony_bbox) mapview(saxony_bbox_sfc) + mapview(saxony) With st_make_grid() we can create a grid within the bounding box. In addition to the bounding box, we provide the function with the envisioned cell size. Our data have a projected coordinate reference system (LAEA), therefore the cell size is given in meters. Here we use square cells with a side length of 10 km. saxony_grid &lt;- st_make_grid( x = saxony_bbox_sfc, cellsize = c(1e4,1e4), what = &quot;polygons&quot; ) |&gt; st_as_sf() mapview(saxony_grid) ## Warning in cbind(`Feature ID` = fid, mat): number of rows of result is not a multiple of vector length (arg 1) Now we need to crop the grid to saxony. saxony_grid %&lt;&gt;% st_intersection(st_union(saxony)) mapview(saxony_grid) ## Warning in cbind(`Feature ID` = fid, mat): number of rows of result is not a multiple of vector length (arg 1) Now we have several polygons but not points. Since predictions are made for a singe pair of x and y coordinates and the squares have four pairs we need to extract points from the polygons. There are two possibilities: the corners of the polygons or their centroids. Both can be created with the st_make_grid() function. saxony_grid_corner &lt;- st_make_grid( x = saxony_bbox_sfc, cellsize = c(1e4,1e4), what = &quot;corners&quot;) |&gt; st_as_sf() |&gt; st_intersection(saxony) ## Warning: attribute variables are assumed to be spatially constant throughout all geometries map1 &lt;- mapview(saxony_grid) + saxony_grid_corner ## Warning in cbind(`Feature ID` = fid, mat): number of rows of result is not a multiple of vector length (arg 1) saxony_grid_centroid &lt;- st_make_grid( x = saxony_bbox_sfc, cellsize = c(1e4,1e4), what = &quot;centers&quot;) |&gt; st_as_sf() |&gt; st_intersection(saxony) ## Warning: attribute variables are assumed to be spatially constant throughout all geometries map2 &lt;- mapview(saxony_grid) + saxony_grid_centroid ## Warning in cbind(`Feature ID` = fid, mat): number of rows of result is not a multiple of vector length (arg 1) map1 | map2 ## Lade nötigen Namensraum: leaflet.extras2 We could also have extracted the centroids from the polygons with the following code: saxony_grid_centroid &lt;- saxony_grid |&gt; st_centroid() |&gt; st_as_sf() |&gt; rename(geom = x) Now we can make predictions for the centroids and transfer them back to the grid polygons for visualization. saxony_grid_centroid %&lt;&gt;% mutate( x = st_coordinates(saxony_grid_centroid)[,1], y = st_coordinates(saxony_grid_centroid)[,2] ) saxony_grid_centroid$EC_tsa_1 &lt;- predict(tsa1, saxony_grid_centroid) saxony_grid_centroid$EC_tsa_2 &lt;- predict(tsa2, saxony_grid_centroid) saxony_grid$EC_tsa1 &lt;- saxony_grid_centroid$EC_tsa_1 saxony_grid$EC_tsa2 &lt;- saxony_grid_centroid$EC_tsa_2 mapview(saxony_grid, zcol = &quot;EC_tsa1&quot;) + mapview(lucas, zcol = &quot;EC&quot;, legend = F) mapview(saxony_grid, zcol = &quot;EC_tsa2&quot;) + mapview(lucas, zcol = &quot;EC&quot;, legend = F) 3.4 Splines Next, we have a quick look at interpolation with splines. For this we will need the fields package. We use a sort of spline that is called thin plate spline which can be easily called through the Tps() function. As arguments we provide the coordinates as a matrix and the independent variable (EC). tps_fit &lt;- Tps(x = matrix(c(lucas$x, lucas$y), ncol = 2), Y = lucas$EC) Again, we can use the predict() function to predict the electrical conductivity for unobserved locations. The result is a matrix which we transform to a numeric vector with as.numeric(). tps_prediction &lt;- predict(tps_fit, st_drop_geometry(saxony_grid_centroid[, c(&quot;x&quot;, &quot;y&quot;)])) saxony_grid$EC_tps &lt;- as.numeric(tps_prediction) mapview(saxony_grid, zcol = &quot;EC_tps&quot;) + mapview(lucas, zcol = &quot;EC&quot;, legend = F) Exercise Fit a thin plate spline to the phosphorus data 3.5 Weighted averaging In the rest of the script, we cover methods that weight the observed values based on their distance to the predicted location. One method for this, which we did not discuss in the lecture is k nearest neighbors. With this method we consider the k nearest points and build their mean value. These k points are all weighted equally. All other points are not considered. For all the weighted average procedures we will use the gstat package. With the epinomous function we create a model. As in lm() regression models, we start with the formula. ~1 indicates that we do not wish to use any explanatory variables but instead estimate the mean value of the dependent variable. With locations we choose the data set that gives the spatial coordinates, nmax gives the maximal number of points to use in any one prediction (k) and idp is the inverse distance parameter. It determines how harshly distanced points are down weighted. Here we choose the five nearest neighbors and no distance weighting, i.e., idp = 0. knn_mod &lt;- gstat(formula=EC~1, locations=lucas, nmax=5, set=list(idp = 0)) knn_pred &lt;- predict(knn_mod, saxony_grid) ## [inverse distance weighted interpolation] mapview(knn_pred, zcol = &quot;var1.pred&quot;) + mapview(lucas, zcol = &quot;EC&quot;, legend = F) For inverse distance weighting (IDW) the model construction and prediction are packaged into a single function. idw1 &lt;- idw(EC ~ 1, locations = lucas, newdata = saxony_grid_centroid, idp = 2) ## [inverse distance weighted interpolation] idw2 &lt;- idw(EC ~ 1, locations = lucas, newdata = saxony_grid_centroid, idp = 3) ## [inverse distance weighted interpolation] saxony_grid$EC_idw_1 &lt;- idw1$var1.pred saxony_grid$EC_idw_2 &lt;- idw2$var1.pred mapview(saxony_grid, zcol = &quot;EC_idw_1&quot;) + mapview(lucas, zcol = &quot;EC&quot;, legend = F) mapview(saxony_grid, zcol = &quot;EC_idw_2&quot;) + mapview(lucas, zcol = &quot;EC&quot;, legend = F) Exercise Try different values for the distance weighting parameter when predicting phosphorus concentrations. 3.6 Kriging First we compute the empirical variogram. gstat is becoming increasingly compatible with sf but it was originally designed for the predecessor sp. To be on the safe side we will transform all sf objects in to sp objects. Luckily, this can be done with a single function. lucas_sp &lt;- as(lucas, &quot;Spatial&quot;) Now we can plot a variogram cloud and fit the empirical variogram with the variogram() function. v_emp_cloud &lt;- variogram(EC~1, lucas_sp, cloud = T) v_emp &lt;- variogram(EC~1, lucas_sp) As with the idw() function we don’t assume that there is a spatial trend and construct a variogram for a constant mean value. plot(v_emp_cloud) plot(v_emp) To find the best variogram model we can either fit a selection of models manually … # spherical v_mod_sph &lt;- fit.variogram( object = v_emp, model = vgm(&quot;Sph&quot;) ) # exponential v_mod_exp &lt;- fit.variogram( object = v_emp, model = vgm(&quot;Exp&quot;) ) plot(v_emp, v_mod_sph) plot(v_emp, v_mod_exp) … or we use the automap package to automatically fit and compare multiple models. autovar &lt;- autofitVariogram(formula = EC ~ 1, input_data = lucas_sp) With this variogram we can use kriging to interpolate the EC values. kriging &lt;- krige( formula = EC~1, locations = lucas_sp, newdata = saxony_grid_centroid, model = autovar$var_model ) ## [using ordinary kriging] saxony_grid$EC_krige &lt;- kriging$var1.pred saxony_grid$EC_krige_variance &lt;- kriging$var1.var mapview(saxony_grid, zcol = &quot;EC_krige&quot;) + mapview(lucas, zcol = &quot;EC&quot;, legend = F) Lastly, we can look at the kriging variance wish shows our uncertainty for the predicted values. The further we are removed from measured points we larger the uncertainty. mapview(saxony_grid, zcol = &quot;EC_krige_variance&quot;) + mapview(lucas) Exercise Use the autofitVariogram() function to fit a Kirging model to the phosphorus data. References Douglas Nychka, Reinhard Furrer, John Paige, and Stephan Sain. 2021. “Fields: Tools for Spatial Data.” Boulder, CO, USA: University Corporation for Atmospheric Research. https://github.com/dnychka/fieldsRPackage. Gräler, Benedikt, Edzer Pebesma, and Gerard Heuvelink. 2016. “Spatio-Temporal Interpolation Using Gstat.” The R Journal 8: 204–18. https://journal.r-project.org/archive/2016/RJ-2016-014/index.html. Hiemstra, P. H., E. J. Pebesma, C. J. W. Twenh\"ofel, and G. B. M. Heuvelink. 2008. “Real-Time Automatic Interpolation of Ambient Gamma Dose Rates from the Dutch Radioactivity Monitoring Network.” Computers &amp; Geosciences. "],["machine-learning.html", "Chapter 4 Machine Learning 4.1 Terrain analysis with terra 4.2 Ploting the data 4.3 Logistic regression 4.4 mlr3", " Chapter 4 Machine Learning In this tutorial you will learn how to use R to - fit and predict from logistic generalized linear models - fit and predict from different machine learning algorithms with the mlr3 package (Lang et al. 2019) - conduct spatial and non-spatial cross validation to evaluate the genralizability of your model. Specifically, we will predict the probability of landslides in a small part of the Ecuadorian Andes. As always we start out by loading all the necessary packages. library(data.table) library(dplyr) library(ggplot2) library(magrittr) library(mapview) library(mlr3spatiotempcv) library(mlr3tuning) library(mlr3verse) library(sf) library(spdep) library(statmod) library(terra) library(tmap) The data you need for this exercise consists of a digital elevation model (DEM) for the area and a data set of landslides locations (download here). dem &lt;- rast(&quot;data/ml_raster.tiff&quot;) lsl &lt;- read.csv(&quot;data/landslides.csv&quot;) Let have a short look at the data. head(lsl) ## x y lslpts slope cplan cprof elev log10_carea ## 1 713887.7 9558537 FALSE 33.75185 0.023180449 0.003193061 2422.810 2.784319 ## 2 712787.7 9558917 FALSE 39.40821 -0.038638908 -0.017187813 2051.771 4.146013 ## 3 713407.7 9560307 FALSE 37.45409 -0.013329108 0.009671087 1957.832 3.643556 ## 4 714887.7 9560237 FALSE 31.49607 0.040931452 0.005888638 1968.621 2.268703 ## 5 715247.7 9557117 FALSE 44.07456 0.009686948 0.005149810 3007.774 3.003426 ## 6 714927.7 9560777 FALSE 29.85981 -0.009047707 -0.005738329 1736.887 3.174073 Here we have 350 observations from the Andes in Ecuador. The data include the initiation points of 175 landslides (lslpts = TRUE). In addition, 175 reference points which are randomly distributed in the sampling area are included. We will try to predict landslides, or determine the probability of a landslide occurring for the whole study area. For these predictions we will use the slope, the plan curvature (cplan), the profile curvature (cprof), the elevation (elev), and the log\\(_{10}\\) of the catchment area (log10_carea). These predictors are already part of the data you loaded. Nonetheless, we will go through the steps with which you can derive them from you elevation raster and add them to your sf data set. The specific variables that are already provided in the data are a little trickier to derive than what we will do. You would need to use a so called bridge from R to other GIS software (in this case SAGA R) to do so, and this goes beyond the scope of this course. If you are interested in this topic and want to give it a try I recommend Chapter 10 of Lovelace, Nowosad, and Muenchow (2019). 4.1 Terrain analysis with terra Conducting a terrain analysis with terra is reasonably easy. There is a single function (terrain()) with which you can compute several variables. The function has four arguments you will want to consider and some more for writing the results to file. x is the DEM in SpatRaster format, the format used by terra. v is the variable or variables you want to compute. You can choose slope, aspect, TPI, TRI, roughness, and flow direction. TRI (Terrain Ruggedness Index) is the mean of the absolute differences between the value of a cell and the value of its 8 surrounding cells. TPI (Topographic Position Index) is the difference between the value of a cell and the mean value of its 8 surrounding cells. Roughness is the difference between the maximum and the minimum value of a cell and its 8 surrounding cells. You can provide multiple values to v by combining them in a vector. neighbors is the number of neighboring cells you want to consider to compute your slope and aspect. You can choose between 4 and 8 also known as rook and queen case after the chess pieces (see Figure 4.1 taken from Lloyd (2010)). Figure 4.1: Queens and Rook case ta1 &lt;- terrain(dem$elev, v = &quot;slope&quot;) ta2 &lt;- terrain(dem$elev, v = c(&quot;slope&quot;, &quot;aspect&quot;, &quot;TPI&quot;)) We can see that the results are again SpatRaster objects. ta2 ## class : SpatRaster ## dimensions : 415, 383, 3 (nrow, ncol, nlyr) ## resolution : 10, 10 (x, y) ## extent : 711962.7, 715792.7, 9556862, 9561012 (xmin, xmax, ymin, ymax) ## coord. ref. : WGS 84 / UTM zone 17S (EPSG:32717) ## source(s) : memory ## names : slope, aspect, TPI ## min values : 0.00000, 4.752037e-04, -21.59433 ## max values : 74.14679, 3.599997e+02, 13.26920 To fit models, we now need to add the new variables to the landslides data. To add the raster values to the sample points we have to convert the points, which at this stage are not yet spatially explicit, into sf format and then to terra's vector format SpatVector with vect(). We create lsl_terrain a copy of lsl which we use for this demonstration only. Afterward, we will fit the models with the original and unaltered lsl. lsl_terrain &lt;- lsl lsl_terrain %&lt;&gt;% st_as_sf(coords = c(&quot;x&quot;, &quot;y&quot;), crs=&quot;EPSG:32717&quot;) %&gt;% vect() We extract the values of the raster cells at the location of the landslide observations with the extract() function. lsl2 &lt;- extract(y = lsl_terrain, x = ta2) Now we can bring the landslide data back to the sf format and combine it with the extracted terrain analysis data. In all of this the variable lslpts is reclassified as character at some point. With the last line of the code below, we turn it back into a bolean. lsl_terrain %&lt;&gt;% st_as_sf() %&gt;% bind_cols(lsl2) %&gt;% mutate(lslpts = as.logical(lslpts)) ## New names: ## • `slope` -&gt; `slope...2` ## • `slope` -&gt; `slope...9` lsl_terrain ## Simple feature collection with 350 features and 10 fields ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: 712197.7 ymin: 9556947 xmax: 715737.7 ymax: 9560807 ## Projected CRS: WGS 84 / UTM zone 17S ## First 10 features: ## lslpts slope...2 cplan cprof elev log10_carea ID slope...9 aspect TPI ## 1 FALSE 33.75185 0.023180449 0.003193061 2422.810 2.784319 1 34.29864 238.95861 0.2219849 ## 2 FALSE 39.40821 -0.038638908 -0.017187813 2051.771 4.146013 2 38.21037 33.21008 -2.3615265 ## 3 FALSE 37.45409 -0.013329108 0.009671087 1957.832 3.643556 3 37.70793 312.93348 0.1600342 ## 4 FALSE 31.49607 0.040931452 0.005888638 1968.621 2.268703 4 30.82579 12.59296 1.4586945 ## 5 FALSE 44.07456 0.009686948 0.005149810 3007.774 3.003426 5 43.54292 295.99650 0.2507935 ## 6 FALSE 29.85981 -0.009047707 -0.005738329 1736.887 3.174073 6 31.43264 39.48969 -0.9816742 ## 7 FALSE 31.57465 0.055624146 0.021838507 2583.551 2.251919 7 32.18023 231.95105 2.2877808 ## 8 FALSE 53.42223 0.005728012 0.001018965 2522.235 2.583303 8 53.35639 285.90275 0.6058044 ## 9 FALSE 32.60400 0.024040293 -0.016939975 1929.097 2.836454 9 33.26295 355.05564 -0.5811005 ## 10 FALSE 37.45409 -0.013329108 0.009671087 1957.832 3.643556 10 37.70793 312.93348 0.1600342 ## geometry ## 1 POINT (713887.7 9558537) ## 2 POINT (712787.7 9558917) ## 3 POINT (713407.7 9560307) ## 4 POINT (714887.7 9560237) ## 5 POINT (715247.7 9557117) ## 6 POINT (714927.7 9560777) ## 7 POINT (714287.7 9558367) ## 8 POINT (714147.7 9558467) ## 9 POINT (713717.7 9560657) ## 10 POINT (713407.7 9560307) We can now remove the data as we will continue our work with the origina ones. 4.2 Ploting the data Next we will look at DEM and landslides in parallel. First, we will mask the raster to just the study area. That means we remove all cells of the raster that are not within the study area. Partly, because it looks nicer and partly because it removes the NaN in the raster that would cause problems later on. First, we turn the lsl data into sf format. lsl %&lt;&gt;% st_as_sf(coords = c(&quot;x&quot;, &quot;y&quot;), crs=&quot;EPSG:32717&quot;) Then we derive its convex hull. We need to call st_union() on the points before, because otherwise st_convex_hull() would try to create a separate convex hull for each point. mask &lt;- lsl |&gt; st_union() |&gt; st_convex_hull() When we call the mask function we need to use vect() to transform the mask into the SpatVector class. dem2 &lt;- mask(dem, vect(mask)) Now we can create a nice plot. # create hill shade hs &lt;- shade(slope = dem2$slope * pi / 180, terrain(dem2$elev, v = &quot;aspect&quot;, unit = &quot;radians&quot;)) # tmaptools does not support terra yet. bbx = tmaptools::bb(raster::raster(hs), xlim = c(-0.0001, 1), ylim = c(-0.0001, 1), relative = TRUE) map = tm_shape(hs, bbox = bbx) + tm_grid(col = &quot;black&quot;, n.x = 1, n.y = 1, labels.inside.frame = FALSE, labels.rot = c(0, 90), lines = FALSE) + tm_raster(palette = gray(0:100 / 100), n = 100, legend.show = FALSE) + tm_shape(dem2$elev) + tm_raster(alpha = 0.5, palette = terrain.colors(10), legend.show = FALSE) + tm_shape(lsl) + tm_bubbles(&quot;lslpts&quot;, size = 0.2, palette = &quot;-RdYlBu&quot;, title.col = &quot;Landslide: &quot;) + tm_layout(inner.margins = 0, legend.outside = TRUE) + tm_legend(bg.color = &quot;white&quot;) map 4.3 Logistic regression With the glm() function we can fit a logistic regression model that tries to predict the probability of a landslide with planar curvature, profile curvature, elevation, the decadal logarithm of the cathment area. The call looks similar to the linear regression models you already know except for the family = binomial() argument, which specifies the distribution we assume for the residuals. glm1 = glm(lslpts ~ slope + cplan + cprof + elev + log10_carea, family = binomial(), data = lsl) Just like linear regressions, GLMs have a number of assumptions about the data that need to be met in order for the model to be meaningful. These assumptions are: 1. correct distribution function, 2. correct link function, 3. linearity, 4. lack of outliers, and 5. independence of observations. Your first step after fitting a model should always be to see if it fits the data well and if assumptions are met. Some of them can be checked in tandem: link function, linearity and distribution would, if wrongly specified, all lead to patterns in the residuals that we can investigate with residual plots. For linear models we use the response residuals, i.e. the difference between the observed point and the predicted point \\(y_i - \\hat{\\mu}\\). For GLMs this is not possible, because the variance of the residuals changes with the mean. That means we assume that a pattern in the residuals and visually testing whether our data deviate from the expected pattern is a lot harder than looking for any pattern at all. Thus we use other types of residuals which should indeed show not pattern but are a little bit more difficult to compute. The details of this was discussed in the lecutre and plenty of material is provided in the presentation notes. Here we will use Quantile residuals which we can compute with the statmodpackage (Dunn and Smyth 1996). quantile_residuals &lt;- statmod::qresiduals(glm1) plot(quantile_residuals ~ glm1$fitted.values) abline(h = mean(quantile_residuals)) investigating the QQ Plot (points should be on diagonal line), or qqnorm(quantile_residuals); qqline(quantile_residuals, col = 2) residuals versus row number (again optimally no pattern). scatter.smooth(quantile_residuals) Outliers or influential observations can be identified with Cook’s distance. This metric gives the degree to which the model is altered by removing any one observation. Larger values imply larger changes, i.e. a larger influence on the model. There are some rule of thumbs what values are considered problematic: \\(4/(n-p-1)\\) (Fox 2002) or 1 (Braun 2018). Here, we simply can see that all values are below these thresholds. Nonetheless, we have a look at the largest value. How do regression parameters change if we drop this observation. lsl.cd &lt;- cooks.distance(glm1) plot(lsl.cd) infl &lt;- which.max(lsl.cd) glm2 &lt;- update(glm1, subset = (-infl)) coef(glm1) ## (Intercept) slope cplan cprof elev log10_carea ## 2.511364e+00 7.901064e-02 -2.894196e+01 -1.756360e+01 1.789238e-04 -2.274877e+00 coef(glm2) ## (Intercept) slope cplan cprof elev log10_carea ## 2.642992e+00 8.082282e-02 -3.003641e+01 -8.765858e+00 1.641055e-04 -2.342504e+00 Parameters barely change. Our data are spatial and spatial data often contain spatial autocorrelation. If our data would be spatially auto correlated the assumption of independent observations would be violated. We can check the Moran’s I of the residuals. knn &lt;- knearneigh(lsl, k = 1) nb &lt;- knn2nb(knn) listw &lt;- nb2listw(nb) quantile_residuals &lt;- qres.binom(glm1) moran.test(x = quantile_residuals, listw = listw) ## ## Moran I test under randomisation ## ## data: quantile_residuals ## weights: listw ## ## Moran I statistic standard deviate = 0.29571, p-value = 0.3837 ## alternative hypothesis: greater ## sample estimates: ## Moran I statistic Expectation Variance ## 0.017438628 -0.002865330 0.004714543 Indeed, there seems to be a weak but statistically significant autocorrelation in our residuals. At this point we will not address possible fixes for this problem as they extend beyond the scope of this lecture. With summary() we get a quick overview of the results. summary(glm1) ## ## Call: ## glm(formula = lslpts ~ slope + cplan + cprof + elev + log10_carea, ## family = binomial(), data = lsl) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 2.511e+00 2.035e+00 1.234 0.217 ## slope 7.901e-02 1.506e-02 5.248 1.54e-07 *** ## cplan -2.894e+01 4.746e+00 -6.098 1.07e-09 *** ## cprof -1.756e+01 1.083e+01 -1.622 0.105 ## elev 1.789e-04 5.492e-04 0.326 0.745 ## log10_carea -2.275e+00 4.848e-01 -4.692 2.70e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 485.20 on 349 degrees of freedom ## Residual deviance: 372.83 on 344 degrees of freedom ## AIC: 384.83 ## ## Number of Fisher Scoring iterations: 4 Using the model we can predict the probability of a landslide for each cell of the raster terrain_analysis. pred_glm &lt;- terra::predict(object = dem, model = glm1, type = &quot;response&quot;) map = tm_shape(hs, bbox = bbx) + tm_grid(col = &quot;black&quot;, n.x = 1, n.y = 1, labels.inside.frame = FALSE, labels.rot = c(0, 90), lines = FALSE) + tm_raster(palette = gray(0:100 / 100), n = 100, legend.show = FALSE) + tm_shape(mask(pred_glm, vect(mask))) + tm_raster(alpha = 0.5, palette = &quot;Reds&quot;, n = 6, legend.show = TRUE, title = &quot;Probability of a Landslide: &quot;) + tm_layout(inner.margins = 0, legend.outside = TRUE) + tm_legend(bg.color = &quot;white&quot;) map A common metric to evaluate the predictive capacity of a model is the area under the receiver operating characteristic curve (AUROC or ROC). This is a value between 0.5 and 1.0, with 0.5 indicating a model that is no better than random and 1.0 indicating perfect prediction of the two classes. Thus, the higher the AUROC, the better the model’s predictive power. The following code chunk computes the AUROC value of the model with roc(), which takes the response and the predicted values as inputs. auc() returns the area under the curve. pROC::auc(pROC::roc(lsl$lslpts, fitted(glm1))) ## Area under the curve: 0.8216 4.4 mlr3 Now we will turn to the machine learning technique random forest. There are different frameworks for machine learning in R. We will focus on mlr3 (Lang et al. 2019) which is a versatile and popular framework. mlr3 follows a logic which is shown in Figure 4.2. If your looking for an in depth introduction to the package, you can find a book length introduction to mlr3 here. Figure 4.2: flow diagram of mlr3 4.4.1 Creating a task First we need to create a task. A task contains the data as well as some information on how we want to model the data, like the column name of the dependent variable. There are different types of tasks which differ in the kinds of dependent variables they support. For example, classification tasks are for cases where our dependent variable consists of binary or nominal data. A regression task is designed for continuous numeric quantities. The mlr3spatiotempcv(Schratz and Becker 2022) package introduced a special spatial task type we will look at later. Here we will create a classification task for our landslides data. This is the optimal task type because the dependent variable is binary (landslide or no landslide). First we need to turn the lslpts column into a factor column. For this first example we will drop the geometry column and add the coordinates as individual columns. lsl2 &lt;- st_drop_geometry(lsl) lsl2 %&lt;&gt;% mutate(x = st_coordinates(lsl)[,1], y = st_coordinates(lsl)[,2], lslpts = factor(lslpts)) The spatial classification task is defined by TaskClassifST$new(). The function takes the argument backend, the data set, target, the name of the dependent variable, and id, the name of a column the can be used to identify each observation. Additionally we provide the names of the coordinate columns (coordinate_names), tell the model not to use the coordinates as features (coords_as_features = FALSE) and provide the coordinate reference system (crs). task = mlr3spatiotempcv::TaskClassifST$new( id = &quot;ecuador_lsl&quot;, backend = mlr3::as_data_backend(lsl2), target = &quot;lslpts&quot;, positive = &quot;TRUE&quot;, coordinate_names = c(&quot;x&quot;, &quot;y&quot;), coords_as_features = FALSE, crs = &quot;EPSG:32717&quot; ) The new object has the class TaskClassifST and we can get a short summary of the tasks if we print it to the console. class(task) ## [1] &quot;TaskClassifST&quot; &quot;TaskClassif&quot; &quot;TaskSupervised&quot; &quot;Task&quot; &quot;R6&quot; print(task) ## &lt;TaskClassifST:ecuador_lsl&gt; (350 x 6) ## * Target: lslpts ## * Properties: twoclass ## * Features (5): ## - dbl (5): cplan, cprof, elev, log10_carea, slope ## * Coordinates: ## x y ## &lt;num&gt; &lt;num&gt; ## 1: 713887.7 9558537 ## 2: 712787.7 9558917 ## 3: 713407.7 9560307 ## 4: 714887.7 9560237 ## 5: 715247.7 9557117 ## --- ## 346: 714877.2 9558362 ## 347: 714909.5 9558581 ## 348: 713713.6 9558849 ## 349: 715253.2 9558797 ## 350: 713825.6 9559078 The task registers all variables that are not the target as predictors or, as they are commonly called in the machine learning literature, features. We want to use all six features here but in case we would just want to a subset this would be done with the following code task$select(c(&quot;slope&quot;, &quot;cplan&quot;)) We can use the autoplot() function to get a visual summary of the data. autoplot(task, type = &quot;pairs&quot;) 4.4.2 The learner The learner includes the machine learning algorithm we want to use as well as some information on hyperparameters. Hyperparameters are parameters that you have to determine before running the model. They are not estimated in the fitting procedure. Think of them as different settings for the methods. We will encounter some hyperparameters and discuss how you should choose them later. The learner works in a two-stage procedure: First, a randomly selected subset of the data (the training set, see Fig. 4.2) is used to train the specified algorithm and the trained model is stored in the learner. Second, the trained model is used to predict the target in the test set (i.e. all observations that are not in the training set). The predictions can be compared to the actual values to determine how well the model fares on data it has not seen before. This is also known as cross-validation. If a model performs well in cross-validation we can have a higher believe that it might generalize to unobserved data from the same context (e.g. landslide probability in the area). However, cross validation can not inform us on transferability, i.e., whether the model is adequate to estimate landslide probabilities in other regions of the world. Cross validation usually splits in more than just two groups. The number of groups in a CV is called folds. With five folds we have five equally sized groups. The model is fit on four of the groups and predicts the fifth group. This is repeated for each group. Then the distribution of observations into groups is repeated. In the example below, we split the data into five groups 30 times. Please note that 30 is a rather low number of repetitions, I choose it to keep computational load light for this demonstration. In real world applications you should consider using more repetitions. # non spatial resampling approach ns_resampling &lt;- rsmp(&quot;repeated_cv&quot;, folds = 5, repeats = 30) CV assumes that the observations are independent which, as we have seen, is not the case for our data. A way to met this assumption is to use spatial or blocked CV. Instead of randomly selection point for the folds, all the points in a fold will be close to each other (see Figure 4.3 as an example). Figure 4.3: Spatial vs non-spatial cross validation Thanks to mlr3spatiotempcv (Schratz and Becker 2022) we can select such blocked cross validation approaches as resampling scheme. # spatial resampling approach sp_resampling = rsmp(&quot;repeated_spcv_coords&quot;, folds = 5, repeats = 30) mlr3 contains many algorithms to choose for you learner. A complete overview can be found here. We want to choose three different learners here: A logistic GLM A Random Forest A support vector machine The random forest algorithm is based on the implementation in the ranger package (Wright and Ziegler 2017) and the support vector machine on the implementation in the e1071 package (Meyer et al. 2022). In the codeblock below, we also define fallback learners that are used if the original learner (random forest or support vector machine) return an error. The featureless classifier does not use any features and simply always predicts the more common result. lrnr_glm &lt;- lrn(&quot;classif.log_reg&quot;, predict_type = &quot;prob&quot;) lrnr_rf &lt;- lrn(&quot;classif.ranger&quot;, predict_type = &quot;prob&quot;) lrnr_svm &lt;- lrn(&quot;classif.svm&quot;, predict_type = &quot;prob&quot;) lrnr_rf$fallback &lt;- lrn(&quot;classif.featureless&quot;, predict_type = &quot;prob&quot;) lrnr_svm$fallback &lt;- lrn(&quot;classif.featureless&quot;, predict_type = &quot;prob&quot;) Now we can run the resampling. We will run spatial and non spatial resampling for all three learners and see how their results differ. ## non spatial CV rr_nscv_glm &lt;- mlr3::resample(task = task, learner = lrnr_glm, resampling = ns_resampling) rr_nscv_rf &lt;- mlr3::resample(task = task, learner = lrnr_rf, resampling = ns_resampling) rr_nscv_svm &lt;- mlr3::resample(task = task, learner = lrnr_svm, resampling = ns_resampling) ## with spatial CV rr_spcv_glm &lt;- mlr3::resample(task = task, learner = lrnr_glm, resampling = sp_resampling) rr_spcv_rf &lt;- mlr3::resample(task = task, learner = lrnr_rf, resampling = sp_resampling) rr_spcv_svm &lt;- mlr3::resample(task = task, learner = lrnr_svm, resampling = sp_resampling) We can extract the AUROC and the Brier score from the score element of the resampling object. For a full list of available performance measures see here. auroc_nsp_glm &lt;- rr_nscv_glm$score(measure = mlr3::msr(&quot;classif.auc&quot;)) auroc_nsp_rf &lt;- rr_nscv_rf$score(measure = mlr3::msr(&quot;classif.auc&quot;)) auroc_nsp_svm &lt;- rr_nscv_svm$score(measure = mlr3::msr(&quot;classif.auc&quot;)) auroc_sp_glm &lt;- rr_spcv_glm$score(measure = mlr3::msr(&quot;classif.auc&quot;)) auroc_sp_rf &lt;- rr_spcv_rf$score(measure = mlr3::msr(&quot;classif.auc&quot;)) auroc_sp_svm &lt;- rr_spcv_svm$score(measure = mlr3::msr(&quot;classif.auc&quot;)) brier_nsp_glm &lt;- rr_nscv_glm$score(measure = mlr3::msr(&quot;classif.bbrier&quot;)) brier_nsp_rf &lt;- rr_nscv_rf$score(measure = mlr3::msr(&quot;classif.bbrier&quot;)) brier_nsp_svm &lt;- rr_nscv_svm$score(measure = mlr3::msr(&quot;classif.bbrier&quot;)) brier_sp_glm &lt;- rr_spcv_glm$score(measure = mlr3::msr(&quot;classif.bbrier&quot;)) brier_sp_rf &lt;- rr_spcv_rf$score(measure = mlr3::msr(&quot;classif.bbrier&quot;)) brier_sp_svm &lt;- rr_spcv_svm$score(measure = mlr3::msr(&quot;classif.bbrier&quot;)) These tables contain more columns than we need. We drop the unnecessary columns to get clearer table. We also prepare the table for joining them later. auroc_nsp_glm %&lt;&gt;% dplyr::select(resampling_id, value = classif.auc) %&gt;% dplyr::mutate (measure = &quot;auroc&quot;, model = &quot;glm&quot;) auroc_nsp_rf %&lt;&gt;% dplyr::select(resampling_id, value = classif.auc) %&gt;% dplyr::mutate (measure = &quot;auroc&quot;, model = &quot;rf&quot;) auroc_nsp_svm %&lt;&gt;% dplyr::select(resampling_id, value = classif.auc) %&gt;% dplyr::mutate (measure = &quot;auroc&quot;, model = &quot;svm&quot;) auroc_sp_glm %&lt;&gt;% dplyr::select(resampling_id, value = classif.auc) %&gt;% dplyr::mutate (measure = &quot;auroc&quot;, model = &quot;glm&quot;) auroc_sp_rf %&lt;&gt;% dplyr::select(resampling_id, value = classif.auc) %&gt;% dplyr::mutate (measure = &quot;auroc&quot;, model = &quot;rf&quot;) auroc_sp_svm %&lt;&gt;% dplyr::select(resampling_id, value = classif.auc) %&gt;% dplyr::mutate (measure = &quot;auroc&quot;, model = &quot;svm&quot;) brier_nsp_glm %&lt;&gt;% dplyr::select(resampling_id, value = classif.bbrier) %&gt;% dplyr::mutate (measure = &quot;brier&quot;, model = &quot;glm&quot;) brier_nsp_rf %&lt;&gt;% dplyr::select(resampling_id, value = classif.bbrier) %&gt;% dplyr::mutate (measure = &quot;brier&quot;, model = &quot;rf&quot;) brier_nsp_svm %&lt;&gt;% dplyr::select(resampling_id, value = classif.bbrier) %&gt;% dplyr::mutate (measure = &quot;brier&quot;, model = &quot;svm&quot;) brier_sp_glm %&lt;&gt;% dplyr::select(resampling_id, value = classif.bbrier) %&gt;% dplyr::mutate (measure = &quot;brier&quot;, model = &quot;glm&quot;) brier_sp_rf %&lt;&gt;% dplyr::select(resampling_id, value = classif.bbrier) %&gt;% dplyr::mutate (measure = &quot;brier&quot;, model = &quot;rf&quot;) brier_sp_svm %&lt;&gt;% dplyr::select(resampling_id, value = classif.bbrier) %&gt;% dplyr::mutate (measure = &quot;brier&quot;, model = &quot;svm&quot;) Now we combine the two auroc data sets and the two brier score data sets. results_cross_validation &lt;- bind_rows( auroc_nsp_glm, auroc_nsp_rf, auroc_nsp_svm, auroc_sp_glm, auroc_sp_rf, auroc_sp_svm, brier_nsp_glm, brier_nsp_rf, brier_nsp_svm, brier_sp_glm, brier_sp_rf, brier_sp_svm ) We display the results using violin plots. They are similar to boxplots, the line in the middle indicates the median. However instead of an uninformative box we get the distribution of values as a shape. Remember the higher the AUC the better and the lower the Brier score to better the model. ggplot(results_cross_validation, aes(y = value, x = resampling_id)) + geom_violin(draw_quantiles = .5) + geom_jitter(alpha = 0.1, height = 0, width = 0.03) + facet_wrap(model~ measure, scales = &quot;free&quot;) 4.4.3 Hyperparameter Tuning We will use a spatial nested cross validation scheme to tune the hyperparameters of the random forest. As in the cross validation example above, we will use a relatively small numbers of folds and repetitions just to keep the computational demand reasonable. Please note, that for real analyses you should use more folds and repetitions. Try to find norms in your respective filed of study from published papers and see if performance estimates stabilize at a certain number of folds and repetition. For the hyperparameter tuning we first establish the search space, that is the space in which we look for possible parameter values. This is done with ps() function from the paradox package (Lang et al. 2022). Here, we tune the parameter max.depth, that is the maximal depth, the number of splits, in a single tree. Low depth leads to underfitting, large depth to overfitting. We tell the ps() function that the value should be an integer (p_int(), enter paradox::p_ into the console to see alternatives) between 1 and 10. search_space = ps(max.depth = p_int(lower = 1, upper = 10)) We also specify the resampling scheme, the performance measure, and when the cross validation should end. resampling = rsmp(&quot;spcv_coords&quot;) measure = msr(&quot;classif.auc&quot;) terminator = trm(&quot;evals&quot;, n_evals = 30) With all of those, we specify a tuning algorithm for one criterion with TuningInstanceSingleCrit$new(). instance = TuningInstanceSingleCrit$new( task = task, learner = lrnr_rf, resampling = resampling, measure = measure, search_space = search_space, terminator = terminator ) We specify the number of hyperparemeter vaules to evaluate in the tnr() function with the resolution argument. tuner = tnr(&quot;grid_search&quot;, resolution = 10) tuner$optimize(instance) parameter_tuning_results &lt;- as.data.table(instance$archive) Lets have a look at the results. ggplot(parameter_tuning_results, aes(y = classif.auc, x = max.depth)) + geom_line() + geom_point() + geom_point(data =filter(parameter_tuning_results, classif.auc == min(classif.auc)), col = &quot;red&quot;, size = 4) + geom_label(data =filter(parameter_tuning_results, classif.auc == min(classif.auc)), aes(label = max.depth), nudge_x = .5) Our hyperparameter tuning determined that four is the optimal maximal depth for the trees. We can also tune multiple parameters at the same time. Here we additionally tune the minimum nodes size, i.e., the minimal number of observations that a single node should have. search_space = ps(max.depth = p_int(lower = 1, upper = 100), min.node.size = p_int(lower = 1, upper = 100)) instance = TuningInstanceSingleCrit$new( task = task, learner = lrnr_rf, resampling = resampling, measure = measure, search_space = search_space, terminator = terminator ) tuner = tnr(&quot;grid_search&quot;, resolution = 30) tuner$optimize(instance) parameter_tuning_results &lt;- as.data.table(instance$archive) This is a little bit more difficult to visualize. In the following plot each axis is one hyperparameter. Larger circles and brighter color indicate a higher AUC. The combination with the highest AUC is marked with a red dot. ggplot(parameter_tuning_results, aes(x = max.depth, y = min.node.size)) + geom_point(aes(fill = classif.auc, size = classif.auc), shape = 21) + geom_point( data = filter(parameter_tuning_results, classif.auc == max(classif.auc)), col = &quot;red&quot;, size = 3 ) + theme(legend.position = &quot;none&quot;) We can extract the optimal solution with instance$result_learner_param_vals ## $num.threads ## [1] 1 ## ## $max.depth ## [1] 32 ## ## $min.node.size ## [1] 83 and set the parameters accordingly. lrnr_rf$param_set$values$max.depth &lt;- 1 lrnr_rf$param_set$values$min.node.size &lt;- 32 Now we turn to nested cross validation. We start out by defining the inner resampling. Most of the functions here we have seen before. The only new one is the AutoTuner$new() function. The AutoTuner is a learner which wraps another learner (in our case lrnr_rf). Wrapping here means that it covers it and calls it when it is called. A wrapping function is a function that calls another function. So the AutoTuner calls our learner and tunes its hyperparameters with the specified resampling procedure, search space, terminator, tuner, and measure. The best hyperparameters are set as parameters for a final model which is fit to the full data. resampling = rsmp(&quot;spcv_coords&quot;, folds = 4) measure = msr(&quot;classif.auc&quot;) terminator = trm(&quot;evals&quot;, n_evals = 5) tuner = tnr(&quot;grid_search&quot;, resolution = 10) at = AutoTuner$new(learner = lrnr_rf, resampling = resampling, measure = measure, terminator = terminator, tuner = tuner, search_space = search_space) Now we can pass this inner to a resampling scheme for the outer resampling. outer_resampling = rsmp(&quot;spcv_coords&quot;, folds = 3) rr = mlr3::resample(task, at, outer_resampling, store_models = TRUE) The aggregated performance over all nested instances can be determined with: mean(rr$score(measure = msr(&quot;classif.auc&quot;))$classif.auc) ## [1] 0.7961248 4.4.4 Predictions We can also use the AutoTuner to fit the final model we want to use for prediction. at$train(task) If you do not want to use the AutoTuner but instead train the model without tuning the hyperparameters with hyperparamters values you determined before you can use: lrnr_rf$train(task) at$model ## $learner ## &lt;LearnerClassifRanger:classif.ranger&gt;: Random Forest ## * Model: ranger ## * Parameters: num.threads=1, max.depth=78, min.node.size=100 ## * Packages: mlr3, mlr3learners, ranger ## * Predict Types: response, [prob] ## * Feature Types: logical, integer, numeric, character, factor, ordered ## * Properties: hotstart_backward, importance, multiclass, oob_error, twoclass, weights ## ## $tuning_instance ## &lt;TuningInstanceSingleCrit&gt; ## * State: Optimized ## * Objective: &lt;ObjectiveTuning:classif.ranger_on_ecuador_lsl&gt; ## * Search Space: ## id class lower upper nlevels ## &lt;char&gt; &lt;char&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; ## 1: max.depth ParamInt 1 100 100 ## 2: min.node.size ParamInt 1 100 100 ## * Terminator: &lt;TerminatorEvals&gt; ## * Result: ## max.depth min.node.size classif.auc ## &lt;int&gt; &lt;int&gt; &lt;num&gt; ## 1: 78 100 0.7665477 ## * Archive: ## max.depth min.node.size classif.auc ## &lt;int&gt; &lt;int&gt; &lt;num&gt; ## 1: 78 23 0.7570487 ## 2: 78 100 0.7665477 ## 3: 67 34 0.7562337 ## 4: 45 67 0.7539634 ## 5: 89 89 0.7627918 lrnr_rf$model ## Ranger result ## ## Call: ## ranger::ranger(dependent.variable.name = task$target_names, data = task$data(), probability = self$predict_type == &quot;prob&quot;, case.weights = task$weights$weight, num.threads = 1L, max.depth = 1L, min.node.size = 32L) ## ## Type: Probability estimation ## Number of trees: 500 ## Sample size: 350 ## Number of independent variables: 5 ## Mtry: 2 ## Target node size: 32 ## Variable importance mode: none ## Splitrule: gini ## OOB prediction error (Brier s.): 0.2114375 We can use the predict landslide probability for the whole region. ta2 &lt;- data.table(slope = values(dem$slope)[,1], cplan = values(dem$cplan)[,1], cprof = values(dem$cprof)[,1], log10_carea = values(dem$log10_carea)[,1], elev = values(dem$elev)[,1]) ta2[is.na(slope), c(&quot;slope&quot;, &quot;cplan&quot;, &quot;cprof&quot;, &quot;log10_carea&quot;, &quot;elev&quot;) := -10] # predict new values y &lt;- at$predict_newdata(ta2) x &lt;- lrnr_rf$predict_newdata(ta2) # replace predictions for -10 placeholders y &lt;- as.data.frame(y$data$prob) y[which(ta2$slope == -10), &quot;TRUE&quot;] &lt;- NA x &lt;- as.data.frame(x$data$prob) x[which(ta2$slope == -10), &quot;TRUE&quot;] &lt;- NA dem$prediction_rf &lt;- x$&#39;TRUE&#39; dem3 &lt;- terra::mask(dem, vect(mask)) Create map with predictions. map = tm_shape(hs, bbox = bbx) + tm_grid(col = &quot;black&quot;, n.x = 1, n.y = 1, labels.inside.frame = FALSE, labels.rot = c(0, 90), lines = FALSE) + tm_raster(palette = gray(0:100 / 100), n = 100, legend.show = FALSE) + tm_shape(dem3$prediction_rf) + tm_raster(alpha = 0.5, palette = &quot;Reds&quot;, n = 6, legend.show = TRUE, title = &quot;Probability of a Landslide: &quot;) + tm_layout(inner.margins = 0, legend.outside = TRUE) + tm_legend(bg.color = &quot;white&quot;) map References Braun, John Maindonald AND W. John. 2018. Data Analysis and Graphics Using r: An Example-Based Approach. Cambridge University Press. Dunn, Peter K, and Gordon K Smyth. 1996. “Randomized Quantile Residuals” 5 (3): 236–44. Fox, John. 2002. An r and s-Plus Companion to Applied Regression. Sage Publications. Lang, Michel, Martin Binder, Jakob Richter, Patrick Schratz, Florian Pfisterer, Stefan Coors, Quay Au, Giuseppe Casalicchio, Lars Kotthoff, and Bernd Bischl. 2019. “mlr3: A Modern Object-Oriented Machine Learning Framework in R.” Journal of Open Source Software, December. https://doi.org/10.21105/joss.01903. Lang, Michel, Bernd Bischl, Jakob Richter, Xudong Sun, and Martin Binder. 2022. Paradox: Define and Work with Parameter Spaces for Complex Algorithms. https://CRAN.R-project.org/package=paradox. Lloyd, Christopher D. 2010. Spatial Data Analysis. Oxford University Press. Lovelace, Robin, Jakub Nowosad, and Jannes Muenchow. 2019. Geocomputation with r. CRC Press. Meyer, David, Evgenia Dimitriadou, Kurt Hornik, Andreas Weingessel, and Friedrich Leisch. 2022. E1071: Misc Functions of the Department of Statistics, Probability Theory Group (Formerly: E1071), TU Wien. https://CRAN.R-project.org/package=e1071. Schratz, Patrick, and Marc Becker. 2022. Mlr3spatiotempcv: Spatiotemporal Resampling Methods for ’Mlr3’. https://CRAN.R-project.org/package=mlr3spatiotempcv. Wright, Marvin N., and Andreas Ziegler. 2017. “ranger: A Fast Implementation of Random Forests for High Dimensional Data in C++ and R.” Journal of Statistical Software 77 (1): 1–17. https://doi.org/10.18637/jss.v077.i01. "],["graph-analysis.html", "Chapter 5 Graph Analysis 5.1 The igraph package 5.2 From data to graph 5.3 Centrality measures 5.4 Visualization 5.5 Network Models 5.6 Optimal Channel Networks 5.7 sfnetworks", " Chapter 5 Graph Analysis In this script, we will work with network data in R. To this end will, we will use different packages: igraph, tidygraph, and network for representing networks in R through specific object classes. OCN for the creation of optimal channel networks, and sfnetworks for spatial networks. 5.1 The igraph package We will start with the igraphpackage (Csardi and Nepusz 2006). It is a very popular implementation of networks in R, which is also available for Python and C++. library(dplyr) library(purrr) library(magrittr) library(igraph) First, we create a simple graph with three nodes (n). Thee first node is connected to the second node, the second to the third, and the third to the first. This specified in the edges argument, which takes a vector. Each pair of entries in the vector gives the number or name of the originating and the terminating node. For the network I described above this works out to c(1,2,2,3,3,1). g &lt;- graph(edges = c(1,2,2,3,3,1), n = 3) plot(g) The base plot function returns an image where nodes are represented by orange circles with their names inscribed. The edges are arrows, so we know that the default option of the graph() function is to create directed graphs. Exercise Create a directed graph We can create a non-directed network by setting directed=FALSE. g &lt;- graph(edges = c(1,2,2,3,3,1), n = 3, directed = FALSE) plot(g) If we have more nodes in the network than are indicated in the edge list, the additional nodes are unconnected. g &lt;- graph(edges = c(1,2,2,3,3,1), n = 6, directed = FALSE) plot(g) We can also create complete, undirected graphs that form predefined geometries like tetrahedrons. See the help file of make_graph for more geometries. g &lt;- make_graph(&quot;Tetrahedron&quot;) plot(g) A further alternative is to use a literal description of the graph, where named nodes are connected via -. g &lt;- graph_from_literal(Fred-Daphne, Velma-Shaggy, Shaggy-Fred) plot(g) We can see that - is an undirected edge. You can use as many - as you like, which might improve legibility in some cases. g &lt;- graph_from_literal(Fred--Daphne, Velma----Shaggy, Shaggy----Fred) plot(g) The : colon operator links vertex sets, so that all vertices from both sets are connected. g &lt;- graph_from_literal(Fred-Daphne:Velma-Shaggy, Fred-Shaggy-Scooby) plot(g) In directed graphs, you can indicate the direction with -+ where the + marks the head of the arrow. g &lt;- graph_from_literal(Fred-+Daphne:Velma+-Shaggy, Fred+-Shaggy+-+Scooby) plot(g) Exercise Create a directed graph with the graph_from_literal() function Now lets have a look at how the graph is represented non-graphically to us: g ## IGRAPH bc164bb DN-- 5 7 -- ## + attr: name (v/c) ## + edges from bc164bb (vertex names): ## [1] Fred -&gt;Daphne Fred -&gt;Velma Shaggy-&gt;Fred Shaggy-&gt;Daphne Shaggy-&gt;Velma Shaggy-&gt;Scooby Scooby-&gt;Shaggy The output always starts with IGRAPH, telling us the we have an igraph object. Next, is a seven character string. For me, while I write this, it is c98a3fb. For you it will be different. It will even be different when you read this, because the number changes every time I compile this document. This character string is the ID of the graph. After the ID follow four letters: 1. D or U for directed or undirected 2. N or - for named or unnamed vertices 3. W or - for weighted or unweighted edges 4. B or - for bipartite and unipartite networks.D— After the four letter code, we get the number of nodes (5) and the number of edges (7, the edge between Shaggy and Scooby is counted double). If the whole graph has a name that is printed after the the numbers. All of these properties can also be queried independently. # number of nodes vcount(g) # number of edges ecount(g) # is the graph directed? igraph::is.directed(g) # is the graph bipartite igraph::is.bipartite(g) # is the graph weighted is.weighted(g) # is the graph named is.named(g) Below that the attributes are given under +attr :. For our graph, we get one attribute: name(v/c). The code after the attribute name (in our case the name is name), tells us whether the attribute is of a node (v), edge (e), or whole graph (g) and whether the attribute is a character (c), numeric (n), logical (l), or other (x). So our attribute names concerns nodes and is a character. Lastly, the edges are listed. We can change the attributes of nodes by accessing the nodes of the graph with V(). Then we can change and add attributes in the following way: # name the nodes Bill, Joe, Josy, Laura, and Tyler V(g)$name &lt;- c(&quot;Bill&quot;, &quot;Joe&quot;, &quot;Josy&quot;, &quot;Laura&quot;, &quot;Tyler&quot;) Exercise Rename the nodes in your network What would the code for the first graph we created be? Solution We can see the change in the summary and the plot. g ## IGRAPH bc164bb DN-- 5 7 -- ## + attr: name (v/c) ## + edges from bc164bb (vertex names): ## [1] Bill -&gt;Joe Bill -&gt;Josy Laura-&gt;Bill Laura-&gt;Joe Laura-&gt;Josy Laura-&gt;Tyler Tyler-&gt;Laura plot(g) We can also add a new attribute called math grade. V(g)$math_grade &lt;- c(4,2,3,1,1) g ## IGRAPH bc164bb DN-- 5 7 -- ## + attr: name (v/c), math_grade (v/n) ## + edges from bc164bb (vertex names): ## [1] Bill -&gt;Joe Bill -&gt;Josy Laura-&gt;Bill Laura-&gt;Joe Laura-&gt;Josy Laura-&gt;Tyler Tyler-&gt;Laura cols &lt;- c(&quot;blue&quot;,&quot;red&quot;,&quot;black&quot;,&quot;magenta&quot;) plot(g, vertex.color = cols[V(g)$math_grade]) Exercise Assign values to your nodes and plot them. Color the nodes by the assigned value. We can access the edges in the same way. E(g) ## + 7/7 edges from bc164bb (vertex names): ## [1] Bill -&gt;Joe Bill -&gt;Josy Laura-&gt;Bill Laura-&gt;Joe Laura-&gt;Josy Laura-&gt;Tyler Tyler-&gt;Laura E(g)$likes &lt;- sample(1:4, ecount(g), replace = TRUE) plot(g, edge.color = cols[E(g)$likes]) 5.2 From data to graph You often start with an adjacency matrix or an edge list which you want to turn into a graph. To illustrate how this works, I will simulate an example for each data type. First, we create an adjacency matrix. Remember, an adjacency matrix is a matrix filled with zeros and ones. A one in the j\\(^{th}\\) column of the i\\(^{th}\\) row implies that the j\\(^{th}\\) node is connected to the i\\(^{th}\\) node, while a zero would imply that they are not connected. Here, we simulate a 20x20 matrix that is randomly populated with zeros and ones. For this, we use random draws from the binomial distribution rbinom() . The binomial distribution is what you might use to simulate a coin toss or any other event that can lead to two different outcomes in each trial. In this case, we draw 400 numbers (20^2), and each draw only consists of one trial and a likelihood of success (i.e. a 1) of 0.1. adjacency &lt;- matrix(rbinom(20^2, 1, .1), ncol = 20) # turn all values on the diagonal to zero as a node is not connected to itself. diag(adjacency) &lt;- 0 ga &lt;- graph_from_adjacency_matrix(adjacency, mode = &quot;undirected&quot;) ## Warning: The `adjmatrix` argument of `graph_from_adjacency_matrix()` must be symmetric with mode = &quot;undirected&quot; as of ## igraph 1.6.0. ## ℹ Use mode = &quot;max&quot; to achieve the original behavior. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated. plot(ga) We can also go the other way around and turn a graph into an adjacency matrix. as_adjacency_matrix(ga) ## 20 x 20 sparse Matrix of class &quot;dgCMatrix&quot; ## ## [1,] . . 1 . 1 . . . . . . 1 1 . . . . 1 . . ## [2,] . . 1 . 1 . . . . . . . . . . 1 . . . 1 ## [3,] 1 1 . 1 1 . 1 1 . . . . . . . . . . . . ## [4,] . . 1 . . 1 . . . . . . 1 . . . . . . . ## [5,] 1 1 1 . . . . . . . . . . . . . 1 . 1 1 ## [6,] . . . 1 . . . . . . 1 1 . . . . 1 . . . ## [7,] . . 1 . . . . 1 . . . . . . . . 1 . . 1 ## [8,] . . 1 . . . 1 . . . . . . . . . . . . . ## [9,] . . . . . . . . . . . . . . . . 1 . . . ## [10,] . . . . . . . . . . . . . . . 1 . . . . ## [11,] . . . . . 1 . . . . . 1 . . . . . . 1 . ## [12,] 1 . . . . 1 . . . . 1 . . . . . 1 . . 1 ## [13,] 1 . . 1 . . . . . . . . . 1 1 1 . 1 1 . ## [14,] . . . . . . . . . . . . 1 . 1 . . . . . ## [15,] . . . . . . . . . . . . 1 1 . 1 . . . . ## [16,] . 1 . . . . . . . 1 . . 1 . 1 . . . . . ## [17,] . . . . 1 1 1 . 1 . . 1 . . . . . . . . ## [18,] 1 . . . . . . . . . . . 1 . . . . . 1 . ## [19,] . . . . 1 . . . . . 1 . 1 . . . . 1 . . ## [20,] . 1 . . 1 . 1 . . . . 1 . . . . . . . . You see that the format is unusual for matrices in R. As it says above the output, the output is a sparse matrix. A sparse matrix is a matrix where most of the entries are zeros. Instead of storing all the zeros, which would waste a lot of space, sparse matrix formats only store the non-zero values and their row and column positions. This saves memory and allows computations to skip over the zeros, making operations much faster. Exercise Create an adjacency matrix with a link probability of 0.25 Create a graph from the adjacency matrix and plot it Next, we will create an edge list and derive a graph from it. letters is a vector that contains all the letters of the alphabet. We take a random sample of 20 letters with sample(). The replace = TRUE argument enables us to sample the same letter multiple time. After we have two vectors with randomly drawn letters (letters1 and letters2) we use them as columns in a matrix with the cbind() function. This matrix is the edge list we can use to create a graph with the graph_from_edgelist() function. letters1 &lt;- sample(letters, 20, replace = TRUE) letters2 &lt;- sample(letters, 20, replace = TRUE) el &lt;- cbind(letters1, letters2) el ## letters1 letters2 ## [1,] &quot;g&quot; &quot;a&quot; ## [2,] &quot;g&quot; &quot;u&quot; ## [3,] &quot;k&quot; &quot;x&quot; ## [4,] &quot;i&quot; &quot;a&quot; ## [5,] &quot;g&quot; &quot;t&quot; ## [6,] &quot;o&quot; &quot;i&quot; ## [7,] &quot;k&quot; &quot;t&quot; ## [8,] &quot;r&quot; &quot;b&quot; ## [9,] &quot;l&quot; &quot;m&quot; ## [10,] &quot;y&quot; &quot;c&quot; ## [11,] &quot;r&quot; &quot;j&quot; ## [12,] &quot;f&quot; &quot;a&quot; ## [13,] &quot;s&quot; &quot;h&quot; ## [14,] &quot;l&quot; &quot;e&quot; ## [15,] &quot;s&quot; &quot;q&quot; ## [16,] &quot;y&quot; &quot;s&quot; ## [17,] &quot;r&quot; &quot;q&quot; ## [18,] &quot;e&quot; &quot;k&quot; ## [19,] &quot;q&quot; &quot;h&quot; ## [20,] &quot;k&quot; &quot;f&quot; ge &lt;- graph_from_edgelist(el, directed = FALSE) plot(ge) Until now, all networks we have created have been unipartite. There has been one set of nodes and each node could be connected to every other node. In bipartite networks, there are two distinct sets of nodes. Each node is only connected to nodes from the other set. Below, I create an example of a bipartite network. Six students (S1-S6) can register for four courses (C1-C4). The affiliation matrix has one column per class and one row per student. If a student is part of a class, the respective cell in the adjacency matrix holds a one. If a student does not visit a class, it is a zero. Both classes and students are nodes but neither students nor classes are connected among each other. When we print the graph to the console, we can see that the graph is bipartite by looking at the four-letter code: UN-B C1 &lt;- c(1,1,1,0,0,0) C2 &lt;- c(0,1,1,1,0,0) C3 &lt;- c(0,0,1,1,1,0) C4 &lt;- c(0,0,0,0,1,1) aff.df &lt;- data.frame(C1,C2,C3,C4) row.names(aff.df) &lt;- c(&quot;S1&quot;,&quot;S2&quot;,&quot;S3&quot;,&quot;S4&quot;,&quot;S5&quot;,&quot;S6&quot;) bn &lt;- graph.incidence(aff.df) ## Warning: `graph.incidence()` was deprecated in igraph 2.0.0. ## ℹ Please use `graph_from_biadjacency_matrix()` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated. bn ## IGRAPH bcc6e98 UN-B 10 11 -- ## + attr: type (v/l), name (v/c) ## + edges from bcc6e98 (vertex names): ## [1] S1--C1 S2--C1 S2--C2 S3--C1 S3--C2 S3--C3 S4--C2 S4--C3 S5--C3 S5--C4 S6--C4 plt.x &lt;- c(rep(2, 6), rep(4, 4)) plt.y &lt;- c(7:2, 6:3) lay &lt;- as.matrix(cbind(plt.x, plt.y)) shapes &lt;- c(&quot;circle&quot;, &quot;square&quot;) colors &lt;- c(&quot;blue&quot;, &quot;red&quot;) plot( bn, vertex.color = colors[V(bn)$type + 1], vertex.shape = shapes[V(bn)$type + 1], vertex.size = 10, vertex.label.degree = -pi / 2, vertex.label.dist = 1.2, vertex.label.cex = 0.9, layout = lay ) 5.3 Centrality measures Next, we will turn to the centrality measures we discussed in the lecture. The most basic measure is the degree centrality which is just the degree of a node. We can get the degree with the function degree(). igraph::degree(ga) ## [1] 5 4 6 3 6 4 4 2 1 1 3 5 7 2 3 4 5 3 4 4 We can assign each node its degree as an attribute and plot the network with accordingly colored nodes. For this we need two new packages: ggraph (Pedersen 2022a) and tidygraph (Pedersen 2022b). We will look at both packages a little more in depth later in this tutorial. library(tidygraph) ## ## Attache Paket: &#39;tidygraph&#39; ## Das folgende Objekt ist maskiert &#39;package:igraph&#39;: ## ## groups ## Das folgende Objekt ist maskiert &#39;package:raster&#39;: ## ## select ## Das folgende Objekt ist maskiert &#39;package:stats&#39;: ## ## filter library(ggraph) ## ## Attache Paket: &#39;ggraph&#39; ## Das folgende Objekt ist maskiert &#39;package:sp&#39;: ## ## geometry ## Das folgende Objekt ist maskiert &#39;package:spatstat.geom&#39;: ## ## square V(ga)$degree &lt;- igraph::degree(ga) ga ## IGRAPH bc8471e U--- 20 38 -- ## + attr: degree (v/n) ## + edges from bc8471e: ## [1] 1-- 3 1-- 5 1--12 1--13 1--18 2-- 3 2-- 5 2--16 2--20 3-- 4 3-- 5 3-- 7 3-- 8 4-- 6 4--13 ## [16] 5--17 5--19 5--20 6--11 6--12 6--17 7-- 8 7--17 7--20 9--17 10--16 11--12 11--19 12--17 12--20 ## [31] 13--14 13--15 13--16 13--18 13--19 14--15 15--16 18--19 ga2 &lt;- as_tbl_graph(ga) ggraph(ga2) + geom_edge_link(lwd = 0.4) + geom_node_point(aes(col = degree), size = 3) ## Using &quot;stress&quot; as default layout ## Warning in geom_edge_link(lwd = 0.4): Ignoring unknown parameters: `linewidth` Other centrality metrics can also be computed with the epinomous functions from the igraph package. V(ga)$closeness &lt;- igraph::closeness(ga) V(ga)$betweenness &lt;- igraph::betweenness(ga) Another nice way to display these data is through radial plots. However, for this we again need two additional packages: sna (Butts 2022) and network (Butts 2008) . sna makes the plots and network provides the object class that sna uses. library(network) library(sna) In the following plots, nodes that are closer to the center have higher centrality. A &lt;- as_adjacency_matrix(ga, sparse=FALSE) ga3 &lt;- as.network.matrix(A) gplot.target( ga3, degree(ga3, gmode = &quot;graph&quot;), main = &quot;Degree&quot;, circ.lab = FALSE, circ.col = &quot;skyblue&quot;, usearrows = FALSE, edge.col = &quot;darkgray&quot; ) gplot.target( ga3, closeness(ga3, gmode = &quot;graph&quot;), main = &quot;Closeness&quot;, circ.lab = FALSE, circ.col = &quot;skyblue&quot;, usearrows = FALSE, edge.col = &quot;darkgray&quot; ) gplot.target( ga3, betweenness(ga3, gmode = &quot;graph&quot;), main = &quot;Betweenness&quot;, circ.lab = FALSE, circ.col = &quot;skyblue&quot;, usearrows = FALSE, edge.col = &quot;darkgray&quot; ) 5.4 Visualization For non-spatial networks, there is not a single correct position for any one node. They can be freely arranged in space. However, certain arrangements will make it easier to understand the network while others will simply be confusing. Visualization is as much art as science but there are some common sense rules that usually result in a less confusing figure: Minimize edge crossings Maximize the symmetry of the layout of nodes Minimize the variability of the edge lengths Maximize the angle between edges when they cross or join nodes Minimize the total space used for the network display These were taken from Luke (2015) We will quickly go through some options that igraph gives you and then cover the basics of the ggraph package. As a baseline, we start with the default plot from an igraph object. plot(ga) We can change the color of the nodes with vertex.color and the color of edges with edge.color. par(mfrow = c(1,2)) plot(ga, vertex.color=&quot;cyan&quot;) plot(ga, edge.color=&quot;cyan&quot;) We can add a title with plot(ga, main=&quot;Random layout&quot;) Next, we can change the graph layout, which means the rules, and the algorithm, which are used to optimize the position of the nodes. par(mfrow = c(1,3)) plot(ga, layout=layout.circle, main = &quot;circle&quot;) plot(ga, layout=layout.fruchterman.reingold, main = &quot;fruchterman&quot;) plot(ga, layout=layout.random, main = &quot;random&quot;) We also have the option to create an interactive 2D and 3D graph with tkplot() and rglplot(). It is not possible to embed the result in the .html file but you are currently reading. You can use the code here to try it out yourself. tkplot(ga) rglplot(ga) With that we come to ggraph, an effort to place network visualization inside the ggplot framework. As seen above, we need a tidygraph object to use ggraph. Here, we can use the same object we used above (ga2). Each ggraph plot starts with the same function: ggraph(). The only argument in this function is the object we wish to display. As in ggplot2, this object is not a working plot yet. ga2 %&gt;% ggraph() ## Using &quot;stress&quot; as default layout All we see is a gray background. To visualize the data, we need to add geoms. First, we will add the edges. There are multiple options here and I will show some of them. ga2 %&gt;% ggraph() + geom_edge_link() ## Using &quot;stress&quot; as default layout ga2 %&gt;% ggraph() + geom_edge_bend() ## Using &quot;stress&quot; as default layout ga2 %&gt;% ggraph() + geom_edge_arc() ## Using &quot;stress&quot; as default layout Next, we have a look at the nodes. ga2 %&gt;% ggraph() + geom_node_point() ## Using &quot;stress&quot; as default layout ga2 %&gt;% ggraph() + geom_node_label(aes(label = degree)) ## Using &quot;stress&quot; as default layout ga2 %&gt;% ggraph() + geom_node_tile(width = 1, height = 1) ## Using &quot;stress&quot; as default layout Of course, we can also show both in one plot. ga2 |&gt; ggraph() + geom_edge_link() + geom_node_point() ## Using &quot;stress&quot; as default layout ga2 |&gt; ggraph() + geom_edge_arc() + geom_node_tile( width = .1, height = .1, aes(fill = degree) ) ## Using &quot;stress&quot; as default layout 5.5 Network Models We can simulate network models with igraph. A Poisson random graph is simulated with erdos.renyi.game() where n is the number of nodes, m is the number of edges, p is the probability of two nodes sharing an edge, and the type is either gnm or gnp, depending on whether you supply the number of edges or the probability of adjacency. prg1 &lt;- erdos.renyi.game(n=12,10,type=&#39;gnm&#39;) prg2 &lt;- erdos.renyi.game(n=12,0.1,type=&#39;gnp&#39;) par(mfrow = c(1,2)) plot(prg1) plot(prg2) Exercise Create a Poisson random graph model with 30 nodes and a link probability of 0.15 Compute the betweenness centrality of the network A small world network is created with watts.strogatz.game(), where dim is the dimension of the starting lattice, size is the size of the lattice along each dimension, nei is the neighborhood of the lattice in which nodes are connected, and p the rewiring probability. watts.strogatz.game(dim=1,size=100,nei=2,p=0.5) |&gt; as_tbl_graph() |&gt; ggraph() + geom_edge_link() + geom_node_point() ## Warning: `watts.strogatz.game()` was deprecated in igraph 2.0.0. ## ℹ Please use `sample_smallworld()` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated. ## Using &quot;stress&quot; as default layout Preferential attachment models are created with barabasi.game(), where n is the number of nodes, m is the number of nodes added in each round, and zero.appeal is a factor that raises the likelihood of binding to a node that has no edges so far above zero. g &lt;- barabasi.game(n = 100, m = 1, directed = FALSE, zero.appeal = .5) ## Warning: `barabasi.game()` was deprecated in igraph 2.0.0. ## ℹ Please use `sample_pa()` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated. g &lt;- as_tbl_graph(g) ggraph(g, layout = &quot;igraph&quot;, algorithm = &quot;kk&quot;) + geom_edge_link() + geom_node_point() 5.6 Optimal Channel Networks Next, we will look at optimal channel networks and their R implementation in the OCNet Package (Carraro et al. 2020). As always, we first load the package. library(OCNet) ## Registered S3 methods overwritten by &#39;adegraphics&#39;: ## method from ## biplot.dudi ade4 ## kplot.foucart ade4 ## kplot.mcoa ade4 ## kplot.mfa ade4 ## kplot.pta ade4 ## kplot.sepan ade4 ## kplot.statis ade4 ## scatter.coa ade4 ## scatter.dudi ade4 ## scatter.nipals ade4 ## scatter.pco ade4 ## score.acm ade4 ## score.mix ade4 ## score.pca ade4 ## screeplot.dudi ade4 ## Registered S3 method overwritten by &#39;ape&#39;: ## method from ## plot.mst spdep ## Registered S3 methods overwritten by &#39;adespatial&#39;: ## method from ## plot.multispati adegraphics ## print.multispati ade4 ## summary.multispati ade4 Creating a OCN starts with a call to create_OCN(). This function establishes a lattice with dimX points in the x direction and dimY points in the y direction. In this grid it also establishes the flow direction of each point. The function has many more arguments but usually we do not have to worry about them and can keep the default settings. Here, we create an OCN in a 20x20 grid. OCN &lt;- create_OCN(dimX = 20, dimY = 20) The function produces a list with A a vector of drainage area values (i.e., how many cells does this cell drain) OCN$FD$A ## [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 5 2 1 1 5 ## [27] 1 3 1 1 4 1 1 5 1 1 4 2 3 1 1 1 10 1 1 1 10 4 1 1 8 1 ## [53] 7 1 1 1 19 10 1 1 1 14 1 2 1 1 18 1 1 1 1 18 1 4 1 23 1 1 ## [79] 5 1 1 1 18 1 1 22 1 1 1 1 3 1 72 1 41 1 1 1 1 1 1 21 1 1 ## [105] 25 1 1 2 1 1 1 79 1 47 1 16 13 1 1 1 400 372 369 29 2 1 216 2 1 1 ## [131] 83 1 1 1 3 1 1 10 2 1 1 4 1 338 1 221 1 210 206 203 1 1 1 1 1 1 ## [157] 1 1 4 1 2 1 2 1 334 1 2 1 1 1 118 106 2 1 1 1 1 1 1 1 1 1 ## [183] 1 108 1 3 1 1 1 8 1 1 102 1 3 1 1 5 1 1 1 1 11 93 1 1 1 1 ## [209] 1 4 1 1 1 97 1 23 19 1 3 1 1 9 1 1 90 1 3 1 1 1 1 1 7 1 ## [235] 84 1 1 12 1 1 4 1 1 3 1 83 1 1 1 1 1 4 1 1 57 1 1 1 6 1 ## [261] 2 1 2 1 57 1 20 16 2 1 1 1 1 7 1 48 1 1 1 1 1 1 1 2 52 1 ## [287] 1 1 12 3 1 1 4 1 1 43 1 1 1 1 1 1 1 1 49 1 1 1 1 5 1 1 ## [313] 1 1 16 1 25 1 5 1 1 3 2 19 1 28 1 1 1 2 2 1 1 3 11 1 1 20 ## [339] 1 1 1 1 14 1 1 2 22 14 1 1 1 1 1 3 6 1 1 11 1 1 1 6 1 2 ## [365] 1 4 1 1 11 1 4 1 2 1 3 1 3 1 6 1 1 1 1 1 1 1 1 1 1 6 ## [391] 1 1 1 1 1 1 1 1 1 1 and W a sparse adjacency matrix. OCN$FD$W ## [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## [54] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## [107] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## [160] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## [213] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## [266] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## [319] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## [372] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## Class &#39;spam&#39; (32-bit) The following functions all take the OCN object as their argument. Instead of creating a new object, they attach new levels to the list. create_OCN() created the list at the flow direction level (FD). landscape_OCN() creates the DEM of the river network and adds the catchment level (CM) to the list. The DEM is stored in OCN$FD$Z. library(terra) OCN &lt;- landscape_OCN(OCN) OCN$FD$Z |&gt; matrix(ncol = 20) |&gt; rast() |&gt; plot() Now we need to aggregate the flow network. We set a threshold of how many cells need to flow into one cell for this cell to become a river. This is done with aggregate_OCN(). This function builds the network at the river network (RN), aggregated (AG), subcatchment (SC), and catchment levels. We use the default threshold here, so the only argument we need is OCN. OCN &lt;- aggregate_OCN(OCN) draw_contour_OCN(OCN) draw_elev2D_OCN(OCN) draw_elev3D_OCN(OCN) We can also take the OCN and transform it into an igraph object. river_graph &lt;- OCN_to_igraph(OCN, level = &quot;AG&quot;) river_graph |&gt; as_tbl_graph() |&gt; ggraph(layout = &quot;igraph&quot;, algorithm = &quot;kk&quot;) + geom_edge_link(color = &quot;blue&quot;) + geom_node_point(size = 0) Exercise Simulate a 15x20 optimal channel network Turn the OCN into an igraph object Compute the degree centrality of the nodes 5.7 sfnetworks Lastly, we will turn towards the sfnetworks package (van der Meer et al. 2024). This R package enables us to turn spatial sf objects into networks. sfnetworks uses the tidygraph package instead of igraph. However, tidygraph is intimately linked to igraph. It takes most of the functions from igraph and changes them so they adhere to the tidy principles. There is an implementation of spatial networks in R which works directly with igraph, spNetworks(Gelb 2021) but we will not cover this package here. Since tidygraph adheres to the tidy principles, we can use dplyr verbs and piping. So, before we work with sfnetworks, let’s have a look at tidygraph. To represent networks while following tidy principles (i.e., one row = one observation), tidygraph uses two tables for each graph. One table represents the nodes and the other table represents the edges. As an example, we will use the OCN we just created. river_tidy &lt;- as_tbl_graph(river_graph) river_tidy ## # A tbl_graph: 377 nodes and 376 edges ## # ## # A rooted tree ## # ## # Node Data: 377 × 0 (active) ## # ## # Edge Data: 376 × 2 ## from to ## &lt;int&gt; &lt;int&gt; ## 1 1 22 ## 2 2 22 ## 3 3 22 ## # ℹ 373 more rows river_tidy has the class tbl_graph. We also see the number of nodes and edges and two tibbles: Node Data without columns, because the nodes do not have attributes yet and Edge Data which is an edge list. plot(river_tidy) Plotting returns the same default output as for an igraph object and a look at class of river_tidy reveals that it has inherited the igraph class from river_graph. class(river_tidy) ## [1] &quot;tbl_graph&quot; &quot;igraph&quot; If we want to work with either one of the tables, we use the activate() function. In the following example, I add an attribute, letter, to the nodes. river_tidy %&gt;% activate(nodes) %&gt;% mutate(letter = sample(x = letters, size = vcount(river_graph), replace = TRUE)) ## # A tbl_graph: 377 nodes and 376 edges ## # ## # A rooted tree ## # ## # Node Data: 377 × 1 (active) ## letter ## &lt;chr&gt; ## 1 e ## 2 z ## 3 b ## 4 o ## 5 u ## 6 h ## 7 c ## 8 z ## 9 j ## 10 k ## # ℹ 367 more rows ## # ## # Edge Data: 376 × 2 ## from to ## &lt;int&gt; &lt;int&gt; ## 1 1 22 ## 2 2 22 ## 3 3 22 ## # ℹ 373 more rows We can activate the edges in the same way and we can activate edges and nodes in a single pipe. river_tidy %&lt;&gt;% activate(nodes) %&gt;% mutate(degree = centrality_degree()) %&gt;% activate(edges) %&gt;% mutate(centrality = centrality_edge_betweenness()) %&gt;% arrange(centrality) This will be enough tidygraph for now. The package has almost 300 functions, so we will not be able to cover the package extensively. Another short introduction by the package’s author is available here. Now we turn back to sfnetworks. library(sf) library(sfnetworks) library(mapview) To demonstrate the functionality, we use a demo data set that is included in the Package. The roxel data contain the road network (including bike lanes and footpaths) of the Roxel neighborhood in the German city of Münster. data(&quot;roxel&quot;) mapview(roxel) There are three options of creating a network from sf data with sfnetwork. 1. You provide a point data set as nodes and a line data set as edges 2. You provide a line data set and the nodes are automatically created at the ends of the lines. 3. You provide a point data set the the edges are drawn as straight lines between the nodes. We will try out the just linestrings and just points versions here. We will start with the just linestrings option. roxel_nw1 &lt;- as_sfnetwork(roxel, directed = FALSE) The object looks similar to the tidygraph object we saw before. New are the coordinate reference system and the coordinates of the nodes and edges. roxel_nw1 ## # A sfnetwork with 701 nodes and 851 edges ## # ## # CRS: EPSG:4326 ## # ## # An undirected multigraph with 14 components with spatially explicit edges ## # ## # A tibble: 701 × 1 ## geometry ## &lt;POINT [°]&gt; ## 1 (7.533722 51.95556) ## 2 (7.533461 51.95576) ## 3 (7.532442 51.95422) ## 4 (7.53209 51.95328) ## 5 (7.532709 51.95209) ## 6 (7.532869 51.95257) ## # ℹ 695 more rows ## # ## # A tibble: 851 × 5 ## from to name type geometry ## &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;fct&gt; &lt;LINESTRING [°]&gt; ## 1 1 2 Havixbecker Strasse residential (7.533722 51.95556, 7.533461 51.95576) ## 2 3 4 Pienersallee secondary (7.532442 51.95422, 7.53236 51.95377, 7.53209 51.95328) ## 3 5 6 Schulte-Bernd-Strasse residential (7.532709 51.95209, 7.532823 51.95239, 7.532869 51.95257) ## # ℹ 848 more rows plot(roxel_nw1) Now we can extract our new nodes as sf point object. roxel_point &lt;- st_as_sf(activate(roxel_nw1, nodes), crs = 4326) roxel_nw2 &lt;- roxel_point |&gt; as_sfnetwork() plot(roxel_nw2) Working with the spatial network works the same as the non-spatial tidygraph networks. roxel_nw1 %&gt;% activate(&quot;edges&quot;) %&gt;% mutate(weight = edge_length()) %&gt;% activate(&quot;nodes&quot;) %&gt;% mutate(bc = centrality_betweenness(weights = weight, directed = FALSE)) ## # A sfnetwork with 701 nodes and 851 edges ## # ## # CRS: EPSG:4326 ## # ## # An undirected multigraph with 14 components with spatially explicit edges ## # ## # A tibble: 701 × 2 ## geometry bc ## &lt;POINT [°]&gt; &lt;dbl&gt; ## 1 (7.533722 51.95556) 13808 ## 2 (7.533461 51.95576) 9777 ## 3 (7.532442 51.95422) 35240 ## 4 (7.53209 51.95328) 31745 ## 5 (7.532709 51.95209) 7174 ## 6 (7.532869 51.95257) 9081 ## # ℹ 695 more rows ## # ## # A tibble: 851 × 6 ## from to name type geometry weight ## &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;fct&gt; &lt;LINESTRING [°]&gt; [m] ## 1 1 2 Havixbecker Strasse residential (7.533722 51.95556, 7.533461 51.95576) 28.8 ## 2 3 4 Pienersallee secondary (7.532442 51.95422, 7.53236 51.95377, 7.53209 51.95328) 108. ## 3 5 6 Schulte-Bernd-Strasse residential (7.532709 51.95209, 7.532823 51.95239, 7.532869 51.9525… 54.3 ## # ℹ 848 more rows We can also use spatial operations like spatial filtering on the network. bbox &lt;- st_bbox(roxel) bbox |&gt; st_as_sfc() |&gt; st_as_sf(crs=4326) |&gt; st_transform(3035) -&gt; bbox bbox &lt;- st_buffer(x = bbox, dist = -700, joinStyle = &quot;MITRE&quot;, mitreLimit = 2) bbox %&lt;&gt;% st_transform(4326) plot(roxel_nw1, col = &quot;grey&quot;) plot(bbox, border = &quot;red&quot;, lty = 4, lwd = 4, add = TRUE) filtered = roxel_nw1 %&gt;% activate(&quot;edges&quot;) %&gt;% st_filter(bbox) %&gt;% activate(&quot;nodes&quot;) %&gt;% st_filter(bbox) plot(roxel_nw1, col = &quot;grey&quot;) plot(filtered, add = TRUE) We can also filter to observations around a specified point. point = st_centroid(st_combine(roxel_nw1)) filtered = roxel_nw1 %&gt;% activate(&quot;nodes&quot;) %&gt;% st_filter(point, .predicate = st_is_within_distance, dist = 500) plot(roxel_nw1, col = &quot;grey&quot;) plot(filtered, add = TRUE) plot(point, col = &quot;red&quot;, add = TRUE) 5.7.1 Finding paths in the network Within our network we can find the shortest way to get from point A to point B with the function st_network_paths(). The function uses the most common routing algorithm (i.e., algorithm to find the shortest path): the Dijkstra algorithm. Before we call it, we prepare the data: We transform it into a projected coordinate system. Now the unit of edge lengths are meters and not fractions of degrees. Add the edge length as an attribute to the edges. In a spatial context, the length of edges has a natural interpretation as the spatial distance between points. net &lt;- roxel_nw1 %&gt;% # transform so that distances are in meters st_transform(3035) %&gt;% activate(&quot;edges&quot;) %&gt;% dplyr::mutate(weight = edge_length()) We compute the shortest paths from the 38\\(^{th}\\) to the 200\\(^{th}\\) and the 517\\(^{th}\\) node. The length of the paths is weighted by the weight variable which we computed in the code block above. paths = st_network_paths(net, from = 38, to = c(200, 517), weights = &quot;weight&quot;) Now what is the output of this function? paths ## # A tibble: 2 × 2 ## node_paths edge_paths ## &lt;list&gt; &lt;list&gt; ## 1 &lt;int [25]&gt; &lt;int [24]&gt; ## 2 &lt;int [22]&gt; &lt;int [21]&gt; A tibble with two list columns, two rows long. Each row is one path, i.e., the first row is the path from 38 to 200 and the second row is the path from 38 to 517. In the first column we get the node ids on the respective path and in the second column we get the edge ids. To visualize this we will write a small function that plots each node in the node paths (i.e., column 1). plot_path = function(node_path) { net %&gt;% activate(&quot;nodes&quot;) %&gt;% slice(node_path) %&gt;% plot(cex = 1.5, lwd = 1.5, add = TRUE) } Now we apply this function to all paths and also add symbols to the start and ending points of our paths. colors = sf.colors(3, categorical = TRUE) plot(net, col = &quot;grey&quot;) paths %&gt;% pull(node_paths) %&gt;% walk(plot_path) net %&gt;% activate(&quot;nodes&quot;) %&gt;% st_as_sf() %&gt;% slice(c(38, 200, 517)) %&gt;% plot(col = colors, pch = 8, cex = 2, lwd = 2, add = TRUE) If you need an overview of the distances between all nodes use st_network_cost(). cost &lt;- st_network_cost(net) hist(cost) References Butts, Carter T. 2008. “Network: A Package for Managing Relational Data in r.” Journal of Statistical Software 24 (2). https://www.jstatsoft.org/htaccess.php?volume=24&amp;type=i&amp;issue=02&amp;filename=paper. ———. 2022. Sna: Tools for Social Network Analysis. https://CRAN.R-project.org/package=sna. Carraro, Luca, Enrico Bertuzzo, Emanuel A. Fronhofer, Reinhard Furrer, Isabelle Gounand, Andrea Rinaldo, and Florian Altermatt. 2020. “Generation and Application of River Network Analogues for Use in Ecology and Evolution.” Preprint. Ecology. https://doi.org/10.1101/2020.02.17.948851. Csardi, Gabor, and Tamas Nepusz. 2006. “The Igraph Software Package for Complex Network Research.” InterJournal Complex Systems: 1695. https://igraph.org. Gelb, Jérémy. 2021. spNetwork: Spatial Analysis on Network. https://jeremygelb.github.io/spNetwork/. Luke, Douglas. 2015. A User’s Guide to Network Analysis in r. Use r! Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-23883-8. Pedersen, Thomas Lin. 2022a. Ggraph: An Implementation of Grammar of Graphics for Graphs and Networks. https://CRAN.R-project.org/package=ggraph. ———. 2022b. Tidygraph: A Tidy API for Graph Manipulation. https://CRAN.R-project.org/package=tidygraph. van der Meer, Lucas, Lorena Abad, Andrea Gilardi, and Robin Lovelace. 2024. Sfnetworks: Tidy Geospatial Networks. https://CRAN.R-project.org/package=sfnetworks. "],["rasters-in-r-the-terra-package.html", "Chapter 6 Rasters in R: The terra package 6.1 Basic idea 6.2 Creating a new Raster 6.3 Going beyond the hull 6.4 Multiple layers in one SpatRaster 6.5 Raster algebra 6.6 High-level functions 6.7 Cell-level functions", " Chapter 6 Rasters in R: The terra package For the longest time, when you wanted to manipulate raster files in R, the raster package was your tool of choice. And it still is a well-proven and tested alternative to the newer packages, one of which we will discuss today: terra. 6.1 Basic idea terra implements two new object classes: SpatRaster and SpatVector. It is written and maintained by Robert Hijmans who also did so for the raster package. In that sense you can think of terra as the sequel to raster. Why did Robert Hijmans decide we need a new package? Well the problem of creating a widely used R package is that other packages start to use it and depend on it working they way it did when that package was written. So even if you don’t like certain aspects of you package anymore, you can’t just go ahead and rewrite them. Robert Hijmans thinks that raster has grown to be unnecessarily complex with over 200 functions. Its also slower than it could be and doesn’t support HDF5 files, which is a popular format for satellite data complex. So now terra is faster, simpler (functions have been streamlined) and more capable. However, when you know raster, you also know how to do most things in terra. 6.2 Creating a new Raster The easiest way to specify a new raster is by calling rast() without any arguments. # loading the required packages library(terra) library(dplyr) library(sf) library(tmap) (x &lt;- rast()) ## class : SpatRaster ## dimensions : 180, 360, 1 (nrow, ncol, nlyr) ## resolution : 1, 1 (x, y) ## extent : -180, 180, -90, 90 (xmin, xmax, ymin, ymax) ## coord. ref. : lon/lat WGS 84 (CRS84) (OGC:CRS84) As you can see, the new object covers the whole earth and is projected using longitude and latitude on a WGS84 globe. The resolution is 1 by 1 which means each cell covers 1 degree of latitude and one degree of longitude. Of course, we can also specify the extend and resolution of the rasters we create. The following, raster for example, only covers the southern hemisphere. (x = rast(ymax = 0)) ## class : SpatRaster ## dimensions : 180, 360, 1 (nrow, ncol, nlyr) ## resolution : 1, 0.5 (x, y) ## extent : -180, 180, -90, 0 (xmin, xmax, ymin, ymax) ## coord. ref. : lon/lat WGS 84 (CRS84) (OGC:CRS84) As you can see, this also automatically altered the resolution to 0.5. We cut the area in half so the resolution that corresponds to that direction is also cut in half. By explicitly setting the resolution, we can prevent this from happening. x = rast(ymax = 0, res = c(1,1)) Alternatively we can also specify the number of rows and cells we want our raster to have. x = rast(nrow = 180, ncol = 360) You can check the resolution of a raster with the res() function. The function can also be used to compare the resolution of two (or more) rasters. y = rast() all(res(x) == res(y)) ## [1] TRUE We can also res() to change the resolution of a raster after creating or loading it. res(y) = 10 y ## class : SpatRaster ## dimensions : 18, 36, 1 (nrow, ncol, nlyr) ## resolution : 10, 10 (x, y) ## extent : -180, 180, -90, 90 (xmin, xmax, ymin, ymax) ## coord. ref. : lon/lat WGS 84 (CRS84) (OGC:CRS84) res(y) = c(10,100) y ## class : SpatRaster ## dimensions : 2, 36, 1 (nrow, ncol, nlyr) ## resolution : 10, 100 (x, y) ## extent : -180, 180, -90, 110 (xmin, xmax, ymin, ymax) ## coord. ref. : lon/lat WGS 84 (CRS84) (OGC:CRS84) The last thing we have not altered about this raster skeleton, is its coordinate reference system. If you only have an EPSG code for you desired CRS check out this website. It provides you with the corresponding PROJ.4 string. x = rast() crs(x) ## [1] &quot;GEOGCRS[\\&quot;WGS 84 (CRS84)\\&quot;,\\n DATUM[\\&quot;World Geodetic System 1984\\&quot;,\\n ELLIPSOID[\\&quot;WGS 84\\&quot;,6378137,298.257223563,\\n LENGTHUNIT[\\&quot;metre\\&quot;,1]]],\\n PRIMEM[\\&quot;Greenwich\\&quot;,0,\\n ANGLEUNIT[\\&quot;degree\\&quot;,0.0174532925199433]],\\n CS[ellipsoidal,2],\\n AXIS[\\&quot;geodetic longitude (Lon)\\&quot;,east,\\n ORDER[1],\\n ANGLEUNIT[\\&quot;degree\\&quot;,0.0174532925199433]],\\n AXIS[\\&quot;geodetic latitude (Lat)\\&quot;,north,\\n ORDER[2],\\n ANGLEUNIT[\\&quot;degree\\&quot;,0.0174532925199433]],\\n USAGE[\\n SCOPE[\\&quot;unknown\\&quot;],\\n AREA[\\&quot;World\\&quot;],\\n BBOX[-90,-180,90,180]],\\n ID[\\&quot;OGC\\&quot;,\\&quot;CRS84\\&quot;]]&quot; # set to WGS 84 crs(x) &lt;- &quot;+proj=longlat +datum=WGS84 +no_defs &quot; # set to UTM 48 crs(x) &lt;- &quot;+proj=utm +zone=48 +datum=WGS84&quot; x ## class : SpatRaster ## dimensions : 180, 360, 1 (nrow, ncol, nlyr) ## resolution : 1, 1 (x, y) ## extent : -180, 180, -90, 90 (xmin, xmax, ymin, ymax) ## coord. ref. : +proj=utm +zone=48 +datum=WGS84 +units=m +no_defs 6.3 Going beyond the hull We can now create an empty raster anywhere on the world. However, as long as no values are in the cells this doesn’t get us anywhere. About time we introduce some values. r &lt;- rast(ncol=10, nrow=10) # number of cells (ncol * nrow) ncell(r) ## [1] 100 # do we have any values yet? hasValues(r) ## [1] FALSE # now we fill the raster with increasing numbers, staring with 1 values(r) &lt;- 1:ncell(r) plot(r, main=&#39;Raster with 100 cells&#39;) Let’s have a look at two actual raster file from my hard drive. You can download them here. dem1 &lt;- rast(&quot;data/DGM25_2905530.xyz&quot;) dem2 &lt;- rast(&quot;data/DGM25_2905540.xyz&quot;) par(mfrow = c(1,2)) plot(dem1) plot(dem2) crs(dem1) = &quot;+proj=utm +zone=32 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs &quot; crs(dem2) = &quot;+proj=utm +zone=32 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs &quot; As you can see they are two digital elevation models. dem1 ## class : SpatRaster ## dimensions : 161, 161, 1 (nrow, ncol, nlyr) ## resolution : 25, 25 (x, y) ## extent : 295987.5, 300012.5, 5535988, 5540013 (xmin, xmax, ymin, ymax) ## coord. ref. : +proj=utm +zone=32 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs ## source : DGM25_2905530.xyz ## name : DGM25_2905530 ## min value : 207.683 ## max value : 511.909 6.4 Multiple layers in one SpatRaster One SpatRaster can have multiple raster layers. Think of the temperature in the same area in several years where each layer is one year. Let’s simulate such an example. # First we create a raster with temperatures between 10 and 15 degrees r1 = rast(nrow = 25, ncol = 25) values(r1) = runif(n = 25^2, min = 10, max = 15) plot(r1) # Now we create two rasters that are based on r1 but slightly warmer. r2 = r1 + 1 r3 = r2 + 1 We can combine the three rasters into one multi-layer object. With terra this is as easy as writing a simple vector s &lt;- c(r1, r2, r3) s ## class : SpatRaster ## dimensions : 25, 25, 3 (nrow, ncol, nlyr) ## resolution : 14.4, 7.2 (x, y) ## extent : -180, 180, -90, 90 (xmin, xmax, ymin, ymax) ## coord. ref. : lon/lat WGS 84 (CRS84) (OGC:CRS84) ## source(s) : memory ## names : lyr.1, lyr.1, lyr.1 ## min values : 10.00479, 11.00479, 12.00479 ## max values : 14.99076, 15.99076, 16.99076 Note that the class of a multi-layer object is the same as a single layer raster. class(s) ## [1] &quot;SpatRaster&quot; ## attr(,&quot;package&quot;) ## [1] &quot;terra&quot; Calling the generic plot function on such a multi-layer raster plots all layers, so be care full with this if you have many layers! plot(s) We can subset the object like a list. s[[1]] ## class : SpatRaster ## dimensions : 25, 25, 1 (nrow, ncol, nlyr) ## resolution : 14.4, 7.2 (x, y) ## extent : -180, 180, -90, 90 (xmin, xmax, ymin, ymax) ## coord. ref. : lon/lat WGS 84 (CRS84) (OGC:CRS84) ## source(s) : memory ## name : lyr.1 ## min value : 10.00479 ## max value : 14.99076 6.5 Raster algebra SpatRaster objects can be passed to most algebraic operations in R such as + and - or sum() and abs(). You have just seen this above, when I added a degree of temperature to our simulated temperature rasters. Below are some more examples p = r1 + 4 o = sqrt(r1) x = sum(values(r)) You can also add, substract, multiply and divide rasters with each other. xo = x + o ox = x + o px = p / x 6.6 High-level functions In most situations you will have rasters that are larger than what you need. You might be interested in the landcover of your study area but the data covers all of Europe. Conversely, you might also need to combine different rasters if the scale of your analysis is larger than the data you are using. Often this is the case when larger rasters are split in smaller areas to reduce their size. In the first case you would use crop(). Provided with a raster and an extend, crop() will return only the part of the raster that lies within the extend. An extend usually is a named vector with four elements: xmin, xmax, ymin and ymax. It might for example be the bounding box you derived from the vector data you were analyzing in sf. You can create this raster yourself if you know, or can derive, the relevant numbers. An alternative is the drawExtent() function from the raster package. This lets you click on an already plotted raster an returns the extend of the thus created rectangle. Try the code below in your own R instance to see what I mean. plot(dem1) extent = raster::drawExtent() dem1crop = terra::crop(x =dem1, y=extent) Nice! Now have reduced the raster to the area we are actually interested in. We can compute the mean hight or the sum of all hights with global(). global(dem1crop, &quot;sum&quot;) ## sum ## DGM25_2905530 100724.5 global(dem1crop, &quot;mean&quot;) ## mean ## DGM25_2905530 331.3307 We can also expand a raster with the extend() function. That means we add rows and/ or columns with NA values. The other way around we can also trim them, removing all cells with NAs in the margins. dem1.exp = extend(dem1, y = c(100,100)) plot(dem1.exp) Since all the NAs are just white, it’s hard to see what we actually did here. Let’s give them some random value so we can see the cells we added. na_cells = which(is.na(values(dem1.exp))) values(dem1.exp)[na_cells] = 300 plot(dem1.exp) We can now clearly see the added cells in brown. With trim() we can remove them again. na_cells = which(values(dem1.exp) == 300) values(dem1.exp)[na_cells] = NA dem1.trm = trim(dem1.exp) plot(dem1.trm) As mentioned before, sometimes you want to combine rasters. For example our two DEMs are both part of a larger DEM that covers all of the German federal state Rhineland-Palatinate. We can combine the two rasters with merge() dem_merge = merge(dem1, dem2) plot(dem_merge) Whenever you want to use multiple raster in an operation, it is important that they have the same CRS and cell sizes. You can increase cell sizes with aggregate() and reduce cell sizes with disaggregate(). In the first case, several cells are combined into one. The number of cells is set with the fact argument. When you supply one number it is used for both directions. So if you call aggregate(x, fact = 2) and raster has the cell size 10m by 10m, the new raster will be 20m by 20m. You can also provide fact with a vector that specifies the aggregation factor in each direction. The third argument is fun which specifies how to calculate the new cell value. If our new cell consists of four old cells (i.e., we used an aggregation factor of 2) we somehow need to combine those four numbers into one. Common options here are the mean, the median, the modal, the maximum, the minimum, or the sum. dem1_agg_2_mean = aggregate(dem1, fact=2, fun = &quot;mean&quot;) dem1_agg_3_mean = aggregate(dem1, fact=4, fun = &quot;mean&quot;) dem1_agg_4_mean = aggregate(dem1, fact=6, fun = &quot;mean&quot;) dem1_agg_5_mean = aggregate(dem1, fact=8, fun = &quot;mean&quot;) par(mfrow = c(2,2)) plot(dem1_agg_2_mean) plot(dem1_agg_3_mean) plot(dem1_agg_4_mean) plot(dem1_agg_5_mean) par(mfrow = c(2,2)) dem1_agg_mean = aggregate(dem1, fact = c(1,6), fun = &quot;mean&quot;) dem1_agg_min = aggregate(dem1, fact = 4, fun = &quot;min&quot;) dem1_agg_max = aggregate(dem1, fact = 4, fun = &quot;max&quot;) dem1_agg_sum = aggregate(dem1, fact = 4, fun = &quot;sum&quot;) plot(dem1_agg_mean, main = &quot;directed aggregation&quot;) plot(dem1_agg_min, main = &quot;minimum&quot;) plot(dem1_agg_max, main = &quot;maximum&quot;) plot(dem1_agg_sum , main = &quot;sum&quot;) When we want to go in the other direction, we disaggregate raster cells with the disagg() function. Sadly, we cannot magically determine what values are the field truth for the new, smaller cells. Instead each new cell gets the same value as it’s parent. dem1_disagg = disagg(dem1, fact = 2) plot(dem1_disagg) 6.6.1 Extracting raster values with vectors We often encounter situations where we want to extract the values of a raster that coincide with vector data we have. For example, we might have taken soil samples along a mountain and now we want to now the altitude of the samples or we are interested in the land cover of a river catchment. To demonstrate this, we will need some vector data sets. For the point data, we can simulate points within the bounding box of the DEMs we used before. # - extract extend of DEM dem.ext &lt;- ext(dem1) # - convert extend to bounding box usable by sf dem.bb &lt;- st_bbox(dem.ext) # - create 10 random points within bounding box random_points &lt;- st_sample(x = st_as_sfc(dem.bb), 10) random_points &lt;- st_as_sf(random_points, crs = 25832) #- interactive map tmap_mode(&quot;view&quot;) ## tmap mode set to interactive viewing tm_shape(dem1) + tm_raster() + tm_shape(random_points) + tm_dots(size = 1) Now we have ten points that are randomly distributed across the map. If your repeat this with you own computer your ten points will be at other positions. Now that we have, we can extract the raster values, i.e., the altitude, at the locations of our points. This extraction is done with extract(). The first argument to extract() is the raster raster from which we want to extract values. The second argument is the location of points, at which we want to extract the values. ex.points &lt;- extract(dem1, random_points) ## Warning: [extract] transforming vector data to the CRS of the raster We get a non-spatial data.frame back that holds the row number of points and the elevation at these points. We can add the altitude values back to the points like this: random_points$altitude &lt;- ex.points$DGM25_2905530 tm_shape(random_points) + tm_dots(col = &quot;altitude&quot;) Now we want to look at polygons instead of points. First, we need some polygons. Here, we use river catchments from Rhineland-Palatinate. catchments &lt;- st_read(&quot;data/catchments.gpkg&quot;) ## Reading layer `catchments&#39; from data source ## `C:\\Users\\jonat\\Documents\\001_Uni\\002_teaching\\online books\\book_spatial_data_science_in_R\\data\\catchments.gpkg&#39; ## using driver `GPKG&#39; ## Simple feature collection with 2754 features and 55 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: 4042743 ymin: 2874149 xmax: 4212789 ymax: 3094667 ## Projected CRS: ETRS89-extended / LAEA Europe Let’s have a look at the data. tm_shape(catchments) + tm_polygons(alpha = .5) + tm_shape(dem_merge) + tm_raster() As we can see, we are missing elevation data for most of the catchments. Thus, we will first remove all catchments for which we are lacking data. dem.ext &lt;- ext(dem_merge) # - convert extend to bounding box usable by sf dem.bb &lt;- st_bbox(dem.ext) |&gt; st_as_sfc() |&gt; st_as_sf(crs = 25832) # - transform catchments to same bb as the DEMs catchments &lt;- st_transform(catchments, crs = 25832) # - subset catchments to only those that are completely within the DEM catchments2 &lt;- catchments[dem.bb, , op = st_within] Let’s have a look at the result. tm_shape(catchments2) + tm_polygons(alpha = .5) + tm_shape(dem_merge) + tm_raster() Next, we extract the raster values from the cells within the different catchment polygons with the extract() function. There is a function with the same name in the raster package. To make sure that we are using the function from terra, we should therefore call the package explicitly by using terra:: in front of the extract() function. dem_ext &lt;- terra::extract(dem_merge, catchments2) ## Warning: [extract] transforming vector data to the CRS of the raster head(dem_ext) ## ID DGM25_2905530 ## 1 1 458.403 ## 2 1 458.774 ## 3 1 459.436 ## 4 1 460.564 ## 5 1 461.968 ## 6 1 463.475 The result is a data.frame with two columns: ID and DGM25_2905530. The second cell is named after the raster from which we extracted the values. The ID column assigns the raster values to the polygons we used for the extraction. unique(dem_ext$ID) ## [1] 1 2 3 4 5 6 7 As we can see, the ID values go from one to seven and we have nrow(catchments2) ## [1] 7 seven catchment polygons. So all rows in dem_ext in which the ID variable is 1 contain raster cell values from cells that are within the polygon in the first row on catchments2. We could now compute the mean elevation within each catchment and assign those values as a variable to catchments2. Here, I will show two different ways to summarize one variable by another. In our case, the elevation (DGM25_2905530) by the catchment (ID). First, we can use group_by() and summarize() from the dplyr package. The first function defines the variable that groups the rows and the second computes some function of a variable. dem_ext |&gt; group_by(ID) |&gt; summarize(mean_elev = mean(DGM25_2905530)) ## # A tibble: 7 × 2 ## ID mean_elev ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1 392. ## 2 2 283. ## 3 3 377. ## 4 4 327. ## 5 5 278. ## 6 6 385. ## 7 7 364. Alternatively, we can use the data.table package. Here, we need to turn dem_ext into a data.table object with setDT(). setDT() is a mutating function. That means we do not need to assign the result to an object but the object within the function is changed (mutated) directly. Data tables can be subset or modified following the basic scheme dt[i,j,by], we i is a logical expression subsetting the rows, j is an expression subsetting or modifying columns and by provides a grouping structure for j. library(data.table) setDT(dem_ext) (dem_ext2 &lt;- dem_ext[, mean(DGM25_2905530), by = &quot;ID&quot;]) ## ID V1 ## &lt;num&gt; &lt;num&gt; ## 1: 1 391.8343 ## 2: 2 282.9041 ## 3: 3 377.3319 ## 4: 4 326.7337 ## 5: 5 277.8304 ## 6: 6 385.4874 ## 7: 7 363.8802 Either way we get the same results, which we can now add back to the catchments. catchments2$elev_new &lt;- dem_ext2$V1 tm_shape(catchments2) + tm_polygons(col = &quot;elev_new&quot;) 6.6.2 Masking Sometimes we might want to fill holes in rasters, i.e., areas that only contain NAs or some other non-informative value, with the values from a second raster. This operation is called masking. Here we will work through a short example, where there are randomly scatted NAs throughout the raster. This might happen if small but thick clouds prevented our satellite to accurately measure conditions on the ground. # Example1 dem1_na = dem1 values(dem1_na)[sample(1:ncell(dem1), size = 100)] = NA #to increase the visibility of the masked cells we will create a separate mask layer with extreme values. dem1_mask = dem1 values(dem1_mask) = 1000 dem1_nax = cover(x = dem1_na, y = dem1_mask) plot(dem1_nax) The lapp() function is used for functional programming. It is similar to apply()-family functions or map-family functions from the purrr package. It applies a function to each layer of a raster Here for example we increase the height of a third of the cells by 10m. dem1_2 = dem1_3 = dem1 values(dem1_2) = rbinom(ncell(dem1),1,0.33) values(dem1_3) = 10 dem1_all = c(dem1, dem1_2, dem1_3) dem_lapp = lapp(dem1_all, function(x,y,z) x + y * z) plot(dem_lapp) We can classify the cells with the classify() function. m &lt;- c(0, 100, 1, 100, 250, 2, 250, 400, 3, 400, 600, 4) dem1_cut = classify(dem1, rcl = m) plot(dem1_cut) focal() can be used to replace the value of a focal cell by some function of its neighbours. Which neighbors and what function to use can be chosen as arguments. mw = focal(dem1, w=3) plot(mw) 6.7 Cell-level functions In terra the cells within a raster a numbered from the upper left cell to the upper right and then continuing in the second row. See the first plot in Going beyond the hull. There is bunch of functions to help you figure out the values of specific cells: # how many columns ncol(r) # how many rows nrow(r) # how many cells ncell(r) # in which row is cell 3? rowFromCell(dem1, 3) # in which column is cell 3? colFromCell(dem1, 3) # which cell is in row 5 and column 5? cellFromRowCol(dem1,5,5) # what are the coordinates of cell 100? xyFromCell(dem1, 100) # which cell lies at the coordinates 0,0 cellFromXY(r, cbind(0,0)) # which column has a Y coordinate of 0 colFromX(r, 0) # which row has a Y coordinate of 0 rowFromY(r, 0) We have seen before that we can use values() to alter or extract the cell values of a raster to a vector. As alternatives we can use valuesBlock() to read a rectangle of blocks or extract() to get the cell values in a specific area. cells &lt;- cellFromRowCol(dem1, 50, 35:39) xy &lt;- xyFromCell(dem1, cells) extract(dem1, xy) ## DGM25_2905530 ## 1 227.560 ## 2 228.122 ## 3 225.683 ## 4 228.754 ## 5 231.383 This wraps up our quick first peek at the terra package. However there are many more things one can do with the package which we might cover in a later post. If you want to know more about it before, check out this talk of Robert Hijmans and Anirddha Ghosh "],["creating-maps-in-r.html", "Chapter 7 Creating Maps in R 7.1 tmap 7.2 rasterVis 7.3 Cartographer 7.4 Rayshader", " Chapter 7 Creating Maps in R This chapter gives you an introduction into basic map creation with R. We will use a range of packages (currently only tmap implemented) to create both interactive (not yet) and static maps. Interactive maps are great to explore your data or for interactive documents like html documents and websites. Static maps require more effort but are necessary for static documents, i.e., .pdf, docx., or pptx. First, we load all the sf package which we need to represent spatial data in R. We will also load some data to plot. # loading the required packages library(sf) library(terra) library(dplyr) library(ncdf4) lucas &lt;- readRDS(&quot;data/lucas_saxony.rds&quot;) germany &lt;- st_read(&quot;data/gadm36_DEU_3_pk.gpkg&quot;) rivers &lt;- st_read(&quot;data/rivers_sachsen.gpkg&quot;) We will also download a DEM for Saxony with the geodata package (Hijmans, Ghosh, and Mandel 2022). The function elevation_30s() downloads a DEM for Germany with a resolution of 30 arc seconds (i.e. one degree divided by 3600). For more details on the geodata package see here. The data are downloaded to a folder specified in the path argument. Afterward, we load the data with the terra package. library(geodata) elevation_30s(country = &quot;DEU&quot;, path = &quot;data/elevation&quot;) dem &lt;- rast(&quot;data/elevation/DEU_elv_msk.tif&quot;) All the maps I create as examples here are for Saxony. So we will extract Saxony from the germany data set and the raster. saxony &lt;- filter(germany, NAME_1 == &quot;Sachsen&quot;) saxony_dem &lt;- mask(dem, saxony) saxony_dem &lt;- crop(saxony_dem, saxony) For most object classes, we can use the base plot() function to create a very basic static map. For sf objects, we get one plot per variable. plot(lucas) ## Warning: plotting the first 9 out of 22 attributes; use max.plot = 22 to plot all These maps are not very informative and should not be used for reports or presentations. The defaul plot for rasters is already a lot better. plot(dem) There are several good options to create maps with both rasters and vectors in R. Some package can create both static and interactive maps and others are specifically designed for one of the two. We will go through several packages in order of decreasing importance. From my experience you will rarely need other packages than tmap (Tennekes 2018) to create good maps. This is why we start with this package. However, some packages provide functions that are not available in tmap and so will also cover several other further down. 7.1 tmap tmap is a large package with lots of functions. We will only scratch the surface here. For a more in-depth introduction see here (at the time of writing the book is still under construction). library(tmap) Besides the regular maps, tmap can create quickmaps with the qtm() function that can be handy for data exploration. qtm(lucas) These maps are similar to those created by the base plot() function. The main difference is that we only get one map. While the beauty of the qtm() function is that we do not need to add any information besides the data set, if we want, we can add more variables. qtm(lucas, symbols.shape = 24, symbols.size = &quot;EC&quot;, title = &quot;Electrical conductivity&quot;) 7.1.1 Basic tmaps Usually however, you will be using the regular map approach. tm_shape(lucas) + tm_symbols(shape = 24, size = 3) Here, we first provide the data set we want to plot to the tm_shape() function and then specify how we want to display the data. Here, we used tm_symbols() but there are many different visual elements we can add to a map. Some are just shape-explicit versions of tm_symbols. For example, tm_dots(), tm_squares(), tm_markers(), or tm_bubbles(). tm_shape(lucas) + tm_dots() tm_shape(lucas) + tm_bubbles() tm_shape(lucas) + tm_squares() All of these visual elements are applicable to point data. For polygons and lines we need other elements. For lines, we use tm_lines(). tm_shape(rivers) + tm_lines() For polygons, we can use tm_polygons(), tm_borders(), or tm_fill(). tm_shape(saxony) + tm_polygons() tm_shape(saxony) + tm_borders() tm_shape(saxony) + tm_fill() For raster, we use the tm_raster() function. tm_shape(saxony_dem) + tm_raster() So far, we have only used the most basic functionality of tmap. We have specified a single data set and used one visual element to populate the map. Most visual elements have properties that we can adjust. We have already seen this with tm_symbols(), where we specified the size and shape of symbols. Those specifications are made within the brackets following the call to the visual element. For example, if we want to set the size of our bubbles we use: 7.1.2 Adjusting visual elements and multiple groups tm_shape(lucas) + tm_bubbles(size=2) These attributes can either be fixed or variable. So far we have only used fixed attributes (e.g., all points have the size 2). Variable attributes follow variables of the data set. We could for example control the color of the points by the electrical conductivity. tm_shape(lucas) + tm_bubbles(col = &quot;EC&quot;) As you can see, we the name of the variable, we want to use, needs to be provided in quotation marks. Since the electrical conductivity is a numerical variable, tmap automatically chooses a continuous color scale. If we transform EC into a factor variable or a character, tmap would use a categorical color scale. lucas &lt;- mutate(lucas, EC_char = as.character(EC)) tm_shape(lucas) + tm_bubbles(col = &quot;EC_char&quot;) ## Warning: Number of levels of the variable &quot;EC_char&quot; is 75, which is larger than max.categories (which is 30), ## so levels are combined. Set tmap_options(max.categories = 75) in the layer function to show all levels. For point data we generally have the arguments: size, color, transparency (alpha), border color, line width, and transparency. tm_shape(lucas) + tm_bubbles(col = &quot;EC&quot;, size = &quot;OC&quot;, alpha = 0.5, border.col = &quot;red&quot;, border.lwd = .5, border.alpha = 1) For line data, we can determine the line width, color, type, and transparency. tm_shape(rivers) + tm_lines(col = &quot;blue&quot;, lty = &quot;dashed&quot;, lwd = .3) For polygons, we have similar options. We can alter color and border type. tm_shape(saxony) + tm_fill(col = &quot;NAME_2&quot;) + tm_borders(lty = &quot;dashed&quot;, lwd = 2, col = &quot;red&quot;) As you can see, we can add multiple visual elements based on the same shape. We can also have mutliple shapes in the same map. tm_shape(saxony_dem) + tm_raster() + tm_shape(st_union(saxony)) + tm_borders(lwd = 2, lty = &quot;dashed&quot;) + tm_shape(rivers) + tm_lines(col = &quot;blue&quot;) + tm_shape(lucas) + tm_bubbles(col = &quot;EC&quot;, palette = &quot;viridis&quot;, size = .2) The combination of shape and visual element is called a group. As you have just seen each map can have multiple groups and each group can have multiple visual elements. 7.1.3 Map extend The map uses the extend and projection of the first group, i.e. the data set that is specified in the first call to tm_shape(). This can be changed by explicitly specifying the master shape with is.master. tm_shape(saxony_dem) + tm_raster() + tm_shape(st_union(saxony)) + tm_borders(lwd = 2, lty = &quot;dashed&quot;) + tm_shape(rivers) + tm_lines(col = &quot;blue&quot;) + tm_shape(lucas, is.master= T) + tm_bubbles(col = &quot;EC&quot;, palette = &quot;viridis&quot;, size = .2) Alternatively, the projection and extend can also be set manually. tm_shape(saxony_dem, projection = 8857) + tm_raster() + tm_shape(rivers) + tm_lines(col = &quot;blue&quot;) tm_shape(saxony_dem, bbox = &quot;Europe&quot;) + tm_raster() + tm_shape(rivers) + tm_lines(col = &quot;blue&quot;) tm_shape(saxony_dem, bbox = &quot;Bautzen&quot;) + tm_raster() + tm_shape(rivers) + tm_lines(col = &quot;blue&quot;) tm_shape(saxony_dem, bbox = c(13, 14, 50.8, 51.3)) + tm_raster() + tm_shape(rivers) + tm_lines(col = &quot;blue&quot;) tm_shape(saxony_dem, bbox = lucas) + tm_raster() + tm_shape(rivers) + tm_lines(col = &quot;blue&quot;) We can use place names, coordinates, or other data sets to set the map limits. 7.1.4 Further map elements There are additional elements we can add to our map. Good maps should always have a north arrow and a scale bar. tm_shape(saxony_dem, bbox = &quot;Saxony&quot;) + tm_raster() + tm_shape(rivers) + tm_lines(col = &quot;blue&quot;) + tm_compass(position = c(&quot;right&quot;, &quot;top&quot;)) + tm_scale_bar(position = c(&quot;right&quot;, &quot;bottom&quot;)) Further elements are graticules and credits. tm_graticules() + tm_shape(saxony_dem, bbox = &quot;Saxony&quot;) + tm_raster() + tm_shape(rivers) + tm_lines(col = &quot;blue&quot;) + tm_compass(position = c(&quot;right&quot;, &quot;top&quot;)) + tm_scale_bar(position = c(&quot;right&quot;, &quot;bottom&quot;)) + tm_credits(&quot;Jonathan Jupke, 2023&quot;) 7.1.5 Making the map prettier So far we have only been concerned with putting data on our map. Like all plots however, maps should convey their message and be nice to look at. Your readers will have to spend more effort trying to understand an ugly map. Generally, nicer figures are not only better in getting across your message. They also convey a general sense of care - nice figures show that you put thought and effort into your document. The maps we have created thus far do not convey this impression. To fix this, we need to address the background. There are two options: using a base map or a background color. The latter is easier, so I will show it first. tm_shape(saxony_dem) + tm_raster() + tm_layout(bg.color = &quot;lightblue&quot;) Background colors can be an easy solution. They can however, also invoke false impressions. The above map, for example, makes it seem like Saxony is an island. Base maps are maps on which your data is drawn. They are usually rasters and can be downloaded from multiple sources. Here, we will use the maptiles package (Giraud 2022). This package basically has just one function: get_tiles(). That function downloads a base map for you. We provide a spatial object (sf, sfc, bbox, SpatRaster, SpatVerctor, or SpatExtent object) to its x argument, the base map style we would like to the provider arguments (see here for examples), and the level of detail to the zoom argument. library(maptiles) bm &lt;- get_tiles(x = saxony_dem, provider = &quot;Esri.WorldImagery&quot;, zoom = 8, crop = TRUE) tm_shape(bm) + tm_rgb() + tm_shape(saxony_dem) + tm_raster() Second, the legend is often covering parts of the map. We can avoid this by moving it outside. tm_shape(bm) + tm_rgb() + tm_shape(saxony_dem) + tm_raster() + tm_layout(legend.outside = TRUE) Lastly, the color scale is rather unusual for a DEM. Here we use nicer colors and also add all the other elements to the map. You can have a look at all the palettes available to tmap if you call tmaptools::palette_explorer(). tm_shape(bm) + tm_rgb() + tm_shape(saxony_dem) + tm_raster(palette = terrain.colors(n = 20), alpha = 1, title = &quot;Elevation [m]&quot;) + tm_shape(rivers) + tm_lines(col = &quot;lightblue&quot;) + tm_shape(lucas) + tm_dots(size = .3, col = &quot;EC&quot;, palette = &quot;inferno&quot;, title = &quot;Electrical Conductivity [μS]&quot;) + tm_compass() + tm_scale_bar() + tm_layout(legend.outside = TRUE, main.title = &quot;Electrical Conductivity in Saxony&quot; ) It can also be nice to create maps with an inset, i.e. maps that include a smaller map that shows the larger context of the map. This is possible with tmap but extends beyond the scope of this chapter. See here for a tutorial. 7.1.6 Save to maps to file You can save tmap objects to file with the tmap_save() function. 7.2 rasterVis Here, we will explore the basic functionality of the rasterVis package. As the name already suggests, the purpose of this package is to display raster data. While the common R packages to work with rasters, like raster or terra (see chapter 6)), already provide basic plotting functionality, rasterVis extends this substantially. You can find a more extensive documentation for the package here. library(rasterVis) library(terra) First we need to load some rasters. We will use the geodata package (see next chapter) to download a digital elevation model of Austria. dem_austria &lt;- geodata::elevation_30s(country = &quot;Austria&quot;, path = &quot;data/&quot;) In addition, I want to show you how changes over time can be displayed. So we will also need spatio-temporal data. Below, we download and load solar irradiance data from the Satellite Application Facility on Climate Monitoring (CMSAF). There are a series of packages that you could also use specifically to access these data, for example the cmsaf package. download.file(&#39;https://raw.github.com/oscarperpinan/spacetime-vis/master/data/SISmm2008_CMSAF.zip&#39;, &#39;SISmm2008_CMSAF.zip&#39;) unzip(&#39;SISmm2008_CMSAF.zip&#39;) listFich &lt;- dir(pattern=&#39;\\\\.nc&#39;) stackSIS &lt;- stack(listFich) stackSIS &lt;- stackSIS * 24 ##from irradiance (W/m2) to irradiation Wh/m2 idx &lt;- seq(as.Date(&#39;2008-01-15&#39;), as.Date(&#39;2008-12-15&#39;), &#39;month&#39;) SISmm &lt;- setZ(stackSIS, idx) names(SISmm) &lt;- month.abb ## Lade nötigen Namensraum: ncdf4 Ok, with that we can start plotting the data. The basic plotting function in rasterVis is called levelplot(). If we use that with our Austrian DEM and the irradiance data we get these plots: levelplot(dem_austria) levelplot(SISmm) As you can see, the second plot does not have plots in the margins like the first one. levelplot() only creates marginal plots for rasters with a single layer. If we select one layer, the solar irradiance plots also include these marginal plots. levelplot(SISmm, layers = 1) By default these plots show the means of their respective column or row but we can change this to any function we like. In the plot below, for example, we see the minimal altitude not the mean altitude in the margins. levelplot(dem_austria,margin = list(FUN = &#39;min&#39;)) We can also alter the raster values directly in the call to levelplot(). For example, we can display the logarithm (with base e) by adding the argument: zscaleLog=\"e\". In this plot, we also add contour lines. levelplot(dem_austria, zscaleLog=&quot;e&quot;, contour=TRUE, margin=list(FUN = &quot;min&quot;)) You might have noticed that sofar all plots have used the same colors (magma palette from the viridisLite package). We can easily change this to other palettes: levelplot(dem_austria, par.settings = RdBuTheme) Alternative palettes are; viridisTheme, infernoTheme, plasmaTheme, YlOrRdTheme, BuRdTheme, GrTheme and BTCTheme. 7.2.1 Beyond the usual All of this is very nice but merely a small extension of what raster or terra can already do. However, rasterVis has some more tricks up its sleeve. Like the xyplot() you can see below. xyplot(Jan+Feb~Jul|cut(x, 2), data = SISmm, auto.key = list(space=&#39;right&#39;)) In the function call there are three building blocks: 1. Jan+Feb~Jul|cut(x, 2); 2. data = SISmm; 3. auto.key = list(space='right'). The latter two require less explanation so we will start with them. The data argument receives the raster you want to plot. The auto.key, at least here, is only there to receive the position of the legend. The first argument is written in R’s formula notation. You probably know it from fitting models in R, e.g. lm(x ~ y + z). The response (Jan+Feb) is displayed on the Y-axis and the predictor (Jul) on the X-axis. Each dot in the scatter plot is one cell value of SISmm. |cut(x,2) indicates that we want to split the X-axis into two distinct plots. The values on top of the two plots indicate the X-range for each of them. One drawback of this plot is that many points are on top of each other. A density plot would be helpful for that and indeed rasterVis provides hexbinplots just for that. Note that we have to drop the second response variable in the hexbinplot. hexbinplot(Jan~Jul|cut(x, 2), data = SISmm) Lastly, rasterVis makes it easy to make four common exploratory plots for the raster cell values: 1. scatter plot matrices (splom), 2. histograms, 3. densityplots and box and whiskers plots. splom(SISmm) histogram(SISmm) densityplot(SISmm) bwplot(SISmm) 7.3 Cartographer This section will be added soon 7.4 Rayshader In this entry we will go through the basics of rayshader. Rayshader is one of those packages that you see again and again when you follow people from the R-spatial community on twitter. Especially the package’s creator Tyler Morgan-Wall (see here for his website and here for his Twitter) posts videos and images of things he did with it on an almost daily basis. And he has all the reasons to do so. I don’t know any other package that enables you to create such stunning 3D graphics with R. If you have never seen this package before I am quite sure you are going to be surprised by what is possible. 7.4.1 Setup First things first, we need to install and load the rayshader package, as well as some other packages we will need along the way. install.packages(&quot;rayshader&quot;) library(rayshader) library(raster) library(dplyr) We will use an example raster file from Tyler Morgan-Walls website. We can download the files with the following code: loadzip = tempfile() download.file(&quot;https://tylermw.com/data/dem_01.tif.zip&quot;, loadzip) localtif = raster(unzip(loadzip, &quot;dem_01.tif&quot;)) unlink(loadzip) Before we do anything fancy let’s have look at our new raster. It is a DEM with 505 rows and 550 columns and each cell has size of 33.3m * 33.3m. There are strong differences in height here, from 0 to 971m. localtif ## class : RasterLayer ## dimensions : 505, 550, 277750 (nrow, ncol, ncell) ## resolution : 33.36518, 33.36518 (x, y) ## extent : 505010.3, 523361.2, 5258284, 5275133 (xmin, xmax, ymin, ymax) ## crs : +proj=utm +zone=55 +south +ellps=GRS80 +units=m +no_defs ## source : dem_01.tif ## names : dem_01 ## values : 0, 971 (min, max) plot(localtif) To work with *rayshader** we will need to transform this to a matrix with the raster_to_matrix() function. Then we can start with add a specific texture as well as shadows. elmat = raster_to_matrix(localtif) ## [1] &quot;Dimensions of matrix are: 505x550&quot; elmat %&gt;% sphere_shade(texture = &quot;desert&quot;) %&gt;% plot_map() The shadows are computed for a default sun angle, but we can change that angle if we like. elmat %&gt;% sphere_shade(sunangle = 45, texture = &quot;desert&quot;) %&gt;% plot_map() Now you might have noticed that the areas in the valley look suspiciously flat. That’s because they are water surfaces. With the two functions detect_water() and add_water() we can add water to our map. detect_water() returns a binary matrix with the same dimensions as elmat. A one indicates that the corresponding cell in elmat contains water and a zero that the cell does not. With the cutoff argument you can specify how high the water should be, i.e. which cells are classified as carrying water. High values mean lower water level. elmat %&gt;% sphere_shade(texture = &quot;desert&quot;) %&gt;% add_water(detect_water(elmat, cutoff = 0.9), color = &quot;desert&quot;) %&gt;% plot_map() Now we don’t have any shadows on the water yet. But that can be done with add_shadow(). elmat %&gt;% sphere_shade(texture = &quot;desert&quot;) %&gt;% add_water(detect_water(elmat), color = &quot;desert&quot;) %&gt;% add_shadow(ray_shade(elmat), 0.5) %&gt;% plot_map() In addition to rayshade we can also ad ambient shade, to make it look even better. elmat %&gt;% sphere_shade(texture = &quot;desert&quot;) %&gt;% add_water(detect_water(elmat), color = &quot;desert&quot;) %&gt;% add_shadow(ray_shade(elmat), 0.5) %&gt;% add_shadow(ambient_shade(elmat), 0) %&gt;% plot_map() All this was only the preparation though for the 3D capabilities of rayshader. elmat %&gt;% sphere_shade(texture = &quot;desert&quot;) %&gt;% add_water(detect_water(elmat), color = &quot;desert&quot;) %&gt;% add_shadow(ray_shade(elmat, zscale = 3), 0.5) %&gt;% add_shadow(ambient_shade(elmat), 0) %&gt;% plot_3d(elmat, zscale = 10, fov = 0, theta = 135, zoom = 0.75, phi = 45, windowsize = c(1000, 800)) Sys.sleep(0.2) render_snapshot() This package can do much more than we discussed here or than is really useful for scientific purposes. See here for more extensive coverage. References Giraud, Timothée. 2022. Maptiles: Download and Display Map Tiles. https://CRAN.R-project.org/package=maptiles. Hijmans, Robert J., Aniruddha Ghosh, and Alex Mandel. 2022. Geodata: Download Geographic Data. https://CRAN.R-project.org/package=geodata. Tennekes, Martijn. 2018. “tmap: Thematic Maps in R.” Journal of Statistical Software 84 (6): 1–39. https://doi.org/10.18637/jss.v084.i06. "],["where-do-i-get-data.html", "Chapter 8 Where do I get data❓ 8.1 Websites 8.2 Packages", " Chapter 8 Where do I get data❓ 8.1 Websites Here, I list and very briefly describe several excellent sources for free and relevant geo data. gbif The Global Biodiversity Information Facility hosts information on the distribution of different species. There is also an R package for directly quering GBIF from R. awsome geodata A curated list of geo data providers. Corine Land Cover European land use and land cover data set. EarthEnv Global raster data sets for many relevant variables, including freshwater variables. Ecology &amp; Fish Data Explorer Data on river biota from the UK. EEA Datahub Data provided by the European Environmental Agency. EU Crop Map High-resolution raster map of European crops. Eu Hydro-DEM European river network and catchment data base. GADM Global data on administrative areas. INSPIRE Infrastructure for spatial information in Europe, lots of official data sets from EU countries. JRC Data Catalogue Data sets provided by the European Unions Joint Reserach Center. Naiades Data on surface water quality in France. USGS Earth Explorer Source for LandSat satellite data. Find a tutorial on how to use it here. worldclim Global raster dataset with many important variables. This list is by no means exhaustive. If you find some source you think would be valuable here, please let me know and I will consider adding it. 8.2 Packages There is a selection of R packages specifically designed to provide data. Among them are geodata, , tigris, and prism. 8.2.1 geodata The geodata package is written and maintained by Rob Hijmans who also wrote the raster and terra packages. The purpose of the package is to provide an easy-to-use interface to download different handy spatial data sets directly from R. Here, we will go through most of the functions that this package provides, and see we how we can use them. library(geodata) 8.2.1.1 cmip6 With this function, you can download climate projections from the Coupled Model Intercomparison Project 6 (CIMP6, see here for more information). Shortly, CMIP is an multi-group project that compares the projection of different climate models and also combined them in so called ensemble models to project future climates. The function has six arguments cmip6_world(model, ssp, time, var, res, path). model determines which of the CIMP6 models you want to get the results from. You can select between: “BCC-CSM2-MR”, “CNRMCM6-1”, “CNRM-ESM2-1”, “CanESM5”, “GFDL-ESM4”, “IPSL-CM6A-LR”, “MIROC-ES2L”, “MIROC6” and “MRI-ESM2-0”. Each model represents the efforts of one working group namely: the Bejing Climate Center (BCC-CSM), the Centre National de Recherches Météorologiques (CNRMCM and CNRM-ESM); the Canadian Centre for Climate Modelling (CanESM); the Geophysical Fluid Dynamics Laboratory (GFDL-ESM4), the Institute Pierre Simon Laplace (IPSL), the Japan Agency for Marine-Earth Science and Technology (MIROC-ES2L and MIROC6) and lastly, the Meterological Research Insitute (MRI-ESM2-0). ssp is the code for a “Shared Socio-economic Pathway” (see here and here). Possible codes are “126”, “245”, “370” and “585”. The first number here gives the general narrative which start at very sustainable (SSP1) and grow more and more fossil fuel dependend (SSP5). time refers to the time period that you would like to have the results for? Possible intervals are: “2021-2040”, “2041-2060”, or “2061-2080”. var determines which variable is returned? The climate models above return: minimum temperature (“tmin”), maximum temperature (“tmax”), average temperature (“tavg”), precipitation (“prec”) and biolcimatic variables (“bioc”, see here for an explanation). res is the resolution of the raster that is returned. Valid values are 2.5, 5 and 10 (minutes of a degree). Lastly, path refers to the folder you want to save the data in? Enter the path to the designated folder here. To demonstrate the functions capabilities, we will download the global monthly minum temperatures from the Chinese model, for the foreseeable future on a sustainable socioeconomic pathway, in a resolution of 10 minutes of a degree (i.e. 1/6th of a degree). bcc.10.tmin &lt;- geodata::cmip6_world(model = &quot;BCC-CSM2-MR&quot;, ssp = &quot;126&quot;, time = &quot;2021-2040&quot;, var = &quot;tmin&quot;, res = 10, path = &quot;geodata/&quot;) Lets have a look at the object. bcc.10.tmin ## class : SpatRaster ## dimensions : 1080, 2160, 12 (nrow, ncol, nlyr) ## resolution : 0.1666667, 0.1666667 (x, y) ## extent : -180, 180, -90, 90 (xmin, xmax, ymin, ymax) ## coord. ref. : lon/lat WGS 84 (EPSG:4326) ## source : wc2.1_10m_tmin_BCC-CSM2-MR_ssp126_2021-2040.tif ## names : wc2.1~040_1, wc2.1~040_2, wc2.1~040_3, wc2.1~040_4, wc2.1~040_5, wc2.1~040_6, ... ## min values : -46.84375, -50.7, -62.3125, -68.0250, -68.70000, -67.000, ... ## max values : 28.35000, 28.1, 28.8000, 29.2625, 30.46875, 31.925, ... From a quick look we can see that it is a SpatRaster with 12 layers (months of the year). It is in longitude and latitude. The code below uses the tmap package to save the 12 rasters as a animation in gif format, which is displayed underneath. anim &lt;- tm_shape(bcc.10.tmin) + tm_raster() + tm_facets(nrow = 1, ncol = 1) tmap_animation(anim, &quot;anim_file.gif&quot;) 8.2.1.2 country_codes The country_codes() function is different from most other functions included in this package. It does not download spatial data but instead provide a table with different codes and IDs for countries. Below you can see the first few lines. country_codes() %&gt;% dplyr::select(1:4) %&gt;% head ## NAME ISO3 ISO2 NAME_ISO ## 1 Afghanistan AFG AF AFGHANISTAN ## 2 Akrotiri and Dhekelia XAD &lt;NA&gt; &lt;NA&gt; ## 3 Åland ALA AX ÅLAND ISLANDS ## 4 Albania ALB AL ALBANIA ## 5 Algeria DZA DZ ALGERIA ## 6 American Samoa ASM AS AMERICAN SAMOA 8.2.1.3 elevation The are three different elevation functions. Each loads a digital elevation model (DEM, in this case from the Shuttle Radar Topography Mission, see here). The three functions are elevation_3s(), elevation_30s() and elevation_global(). elevation_30s() has a country argument which you can supply the name or ISO3 code (see function country_code) and it will download the respective DEM. As I am writing this, the elevation_3s() function does not seem to work, there is a problem with the server. The elevation_global() function downloads a DEM for the entire globe in a specified resolution. dem_andorra &lt;- elevation_30s(country = &quot;Andorra&quot;, path = &quot;geodata/&quot;) dem_global &lt;- elevation_global(res = 10, path = &quot;geodata/&quot;) Just as example, and because its relatively small, I downloaded the DEM for Andorroa. You can explore it in the interactive map below. ## tmap mode set to interactive viewing tm_shape(dem_andorra) + tm_raster() The DEM for the whole world is to large to display interactively, so we will just plot it staticly. plot(dem_global) 8.2.1.4 gadm The function gadm() returns a SpatVector (the terra vector class) of administrative boundaries. It has 3 arguments. country is the country name or ISO3 code (see function country_code()). level is the the administrative subdivision, the higher the level the more detail the map has. For higher levels this is quite extensive. To find Landau as an object in the data set, for example, you only need to go to level 3 Here we only look at level 0 (all of Germany) and level 1 (federal states). ger0 &lt;- gadm(country = &quot;DEU&quot;, level = 0, path = &quot;geodata&quot;) ger1 &lt;- gadm(country = &quot;DEU&quot;, level = 1, path = &quot;geodata&quot;) ger0 %&lt;&gt;% st_as_sf() ger1 %&lt;&gt;% st_as_sf() ger0 %&gt;% tm_shape() + tm_polygons() ger1 %&gt;% tm_shape() + tm_polygons() Alternatively, we can query the whole world with world() world &lt;- world(resolution = 5, level = 0, path = &quot;data&quot;) world %&gt;% st_as_sf() %&gt;% st_make_valid() %&gt;% tm_shape() + tm_polygons() On this map we see that the polygons were created on a plane. Geometries that cross the antimeridian (180° longitude) are stretched across the whole globe. This is also the reason that we need to use st_make_valid() which makes to display the data. This problem occurs because the sf package assumes spherical geometries. We can explicitly tell it to not do so and avoid the ugly smears on the map. sf::sf_use_s2(FALSE) ## Spherical geometry (s2) switched off world %&gt;% st_as_sf() %&gt;% st_make_valid() %&gt;% tm_shape() + tm_polygons() 8.2.1.5 sp_occurrence The sp_occurrence() function can be used to download data from the global biodiversity information facility (GBIF). We provide the function with genus and species names we want to download observations for. Here Bubo bubo, the Eagle Owl. occ.bb &lt;-sp_occurrence(genus = &quot;Bubo&quot;, species = &quot;bubo&quot;) This data set has may columns and lots of observations. To simplify things, we remove all columns except longitude and latitude (dplyr::select()) and only keep observations that have data in these columns (dplyr::filter()) and transform it into a simple feature geometry object. occ.bb &lt;- occ.bb |&gt; select(lon, lat) |&gt; filter(!is.na(lon) &amp; !is.na(lat)) |&gt; st_as_sf(coords = c(&quot;lon&quot;, &quot;lat&quot;), crs = 4326) Further, we omit all observations outside of Germany with an intersection. occ.bb &lt;- st_intersection(x = occ.bb, y = ger0) mapview(occ.bb) As a last step to simplify this map, we will lay a grid of hexagons (hexgrid) over Germany and count the number of observations in each cell. First, we need to create the grid, with the st_make_grid() function. #- create hexagonal grid over Germany ger.grid &lt;- st_make_grid(x = occ.bb, square = FALSE, cellsize = .5) This grid is still a square of hexagons. mapview(ger.grid) To reduce it to the actual area of Germany we intersect it we the administrative boundaries from ger0. The result of the intersection a simple feature collection (sfc) and we need to transform it to sf to aggregate the observations. ger.grid %&lt;&gt;% st_intersection(ger0) %&gt;% st_as_sf() ## although coordinates are longitude/latitude, st_intersection assumes that they are planar mapview(ger.grid) + mapview(occ.bb) ## Warning in cbind(`Feature ID` = fid, mat): number of rows of result is not a multiple of vector length (arg 1) We use the sf::st_contains() function to count the number of observations (points in occ.bb) per polygon in ger.grid. ger.grid$density &lt;- st_contains(x = ger.grid, y = occ.bb) |&gt; lengths() ## although coordinates are longitude/latitude, st_contains assumes that they are planar tm_shape(ger.grid) + tm_polygons(col = &quot;density&quot;, palette = &quot;viridis&quot;) 8.2.2 osmdata Here, I will introduce you to the osmdata package. osm is short for open street map and you can use the package to access the data from osm directly from R. install.packages(&quot;osmdata&quot;) libary(osmdata) osm uses the read-only overpass API to download data from Open Street Map and read them in the formats of sp, sf or silicate. A complete call with osmdata looks like his: x &lt;- getbb(&quot;Landau&quot;) %&gt;% opq() %&gt;% add_osm_feature(key = &#39;name&#39;, value = &quot;Queich&quot;) %&gt;% osmdata_sf() As you can see, four different functions are part of the call. getbb() opq() add_osm_feature() osmdata_sf() Lets go through them one after another to see what they do. getbb() returns the bounding box of some place in the world. The only argument you usually have to supply is place_name. ld = getbb(&quot;Landau&quot;) This worked, and returned the bounding box of Landau in longitude and latitude. Using our new bounding box we can now call opq(). Of course you can also write a custom bbox directly into opq(). This function builds a query to Overpass (overpass query). ld = opq(bbox = ld) ld ## $bbox ## [1] &quot;49.1548639,7.8652674,49.3164331,8.180767&quot; ## ## $prefix ## [1] &quot;[out:xml][timeout:25];\\n(\\n&quot; ## ## $suffix ## [1] &quot;);\\n(._;&gt;;);\\nout body;&quot; ## ## $features ## NULL ## ## $osm_types ## [1] &quot;node&quot; &quot;way&quot; &quot;relation&quot; ## ## attr(,&quot;class&quot;) ## [1] &quot;list&quot; &quot;overpass_query&quot; ## attr(,&quot;nodes_only&quot;) ## [1] FALSE This is no spatial object yet but just the request (query) that will be send to the API. This request is further expanded in add_osm_feature(). All items in the Open Street Map data have a key and a value. A few example are key: amenity - value: bar, cafe or fast_food; key: building - value: school, public or church, key: landuse - value: basin, cementery, grass. See here for a complete overview of all keys and features. Let’s download one of each. ld_amenity = add_osm_feature(ld, key = &quot;amenity&quot;, value = &quot;bar&quot;) ld_building = add_osm_feature(ld, key = &quot;building&quot;, value = &quot;school&quot;) ld_landuse = add_osm_feature(ld, key = &quot;landuse&quot;, value = &quot;grass&quot;) Now our queries are complete and we can download the data using osmdata_sf(). ld_amenity %&lt;&gt;% osmdata_sf() ld_building %&lt;&gt;% osmdata_sf() ld_landuse %&lt;&gt;% osmdata_sf() The suffix of osmdata_sf() specifies that we want the downloaded data to be in the sf format. Alternatives are: osmdata_sp,osmdata_sc and osmdata_xml. The resulting objects are lists which have the different geometries as elements. Below we plot the three data sets together. tmap_mode(&quot;view&quot;) ## tmap mode set to interactive viewing tm_shape(ld_amenity$osm_points) + tm_dots(col = &quot;red&quot;) + tm_shape(ld_building$osm_polygons) + tm_polygons(col = &quot;blue&quot;) + tm_shape(ld_landuse$osm_polygons) + tm_polygons(col = &quot;green&quot;) "],["usefull-packages.html", "Chapter 9 Usefull packages 9.1 ‘CoordinateCleaner’", " Chapter 9 Usefull packages 9.1 ‘CoordinateCleaner’ This packages is designed for instanced where you downloaded large databases from the web. For example from GBIF or eBIRD. Such databases can include many small errors that are hard to sport, because there are so many observations. CoordinateCleaner performs many small checks, like is the observations on a border, is it at the capital of a country, is it at the centroid of a country, is the date an outlier, and is the observation of species \\(X\\) within the range of that species. The package is described in Zizka et al. (2019) and its website. Both sources also provide example code. References Zizka, Alexander, Daniele Silvestro, Tobias Andermann, Josué Azevedo, Camila Duarte Ritter, Daniel Edler, Harith Farooq, et al. 2019. “\\(\\less\\)Scp\\(\\greater\\)CoordinateCleaner\\(\\less\\)/Scp\\(\\greater\\): Standardized Cleaning of Occurrence Records from Biological Collection Databases.” Edited by Tiago Quental. Methods in Ecology and Evolution 10 (5): 744–51. https://doi.org/10.1111/2041-210x.13152. "],["references.html", "Chapter 10 References", " Chapter 10 References Appelhans, Tim, Florian Detsch, Christoph Reudenbach, and Stefan Woellauer. 2021. “Mapview: Interactive Viewing of Spatial Data in r.” Baddeley, Adrian, Ege Rubak, and Rolf Turner. 2015. Spatial Point Patterns: Methodology and Applications with r. CRC press. Baddeley, Adrian, and Rolf Turner. 2005. “Spatstat : An R Package for Analyzing Spatial Point Patterns.” Journal of Statistical Software 12 (6). https://doi.org/10.18637/jss.v012.i06. Bivand, Roger S., Edzer Pebesma, and Virgilio Gómez-Rubio. 2013. Applied Spatial Data Analysis with R. New York, NY: Springer New York. https://doi.org/10.1007/978-1-4614-7618-4. Braun, John Maindonald AND W. John. 2018. Data Analysis and Graphics Using r: An Example-Based Approach. Cambridge University Press. Butts, Carter T. 2008. “Network: A Package for Managing Relational Data in r.” Journal of Statistical Software 24 (2). https://www.jstatsoft.org/htaccess.php?volume=24&amp;type=i&amp;issue=02&amp;filename=paper. ———. 2022. Sna: Tools for Social Network Analysis. https://CRAN.R-project.org/package=sna. Carraro, Luca, Enrico Bertuzzo, Emanuel A. Fronhofer, Reinhard Furrer, Isabelle Gounand, Andrea Rinaldo, and Florian Altermatt. 2020. “Generation and Application of River Network Analogues for Use in Ecology and Evolution.” Preprint. Ecology. https://doi.org/10.1101/2020.02.17.948851. Cruz Rot, Marcelino de la. 2008. “Metodos Para Analizar Datos Puntuales.” In Introduccion Al Analisis Espacial de Datos En Ecologia y Ciencias Ambientales: Metodos y Aplicaciones., edited by Fernando T. Maestre, Adrian Escudero, and Andreu Bonet, 76–127. Asociacion Espanola de Ecologia Terrestre, Universidad Rey Juan Carlos; Caja de Ahorros del Mediterraneo. Csardi, Gabor, and Tamas Nepusz. 2006. “The Igraph Software Package for Complex Network Research.” InterJournal Complex Systems: 1695. https://igraph.org. Douglas Nychka, Reinhard Furrer, John Paige, and Stephan Sain. 2021. “Fields: Tools for Spatial Data.” Boulder, CO, USA: University Corporation for Atmospheric Research. https://github.com/dnychka/fieldsRPackage. Dowle, Matt, and Arun Srinivasan. 2021. Data.table: Extension of ‘Data.frame‘. https://CRAN.R-project.org/package=data.table. Dunn, Peter K, and Gordon K Smyth. 1996. “Randomized Quantile Residuals” 5 (3): 236–44. Fletcher, Robert, and Marie-Josée Fortin. 2018. Spatial Ecology and Conservation Modeling: Applications with R. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-030-01989-1. Fox, John. 2002. An r and s-Plus Companion to Applied Regression. Sage Publications. Gabriel, Edith, Peter J Diggle, Barry Rowlingson, and Francisco J Rodriguez-Cortes. 2022. Stpp: Space-Time Point Pattern Simulation, Visualisation and Analysis. https://CRAN.R-project.org/package=stpp. Gelb, Jérémy. 2021. spNetwork: Spatial Analysis on Network. https://jeremygelb.github.io/spNetwork/. Giraud, Timothée. 2022. Maptiles: Download and Display Map Tiles. https://CRAN.R-project.org/package=maptiles. Gräler, Benedikt, Edzer Pebesma, and Gerard Heuvelink. 2016. “Spatio-Temporal Interpolation Using Gstat.” The R Journal 8: 204–18. https://journal.r-project.org/archive/2016/RJ-2016-014/index.html. Hiemstra, P. H., E. J. Pebesma, C. J. W. Twenh\"ofel, and G. B. M. Heuvelink. 2008. “Real-Time Automatic Interpolation of Ambient Gamma Dose Rates from the Dutch Radioactivity Monitoring Network.” Computers &amp; Geosciences. Hijmans, Robert J. 2022a. Raster: Geographic Data Analysis and Modeling. https://CRAN.R-project.org/package=raster. ———. 2022b. Terra: Spatial Data Analysis. https://CRAN.R-project.org/package=terra. Hijmans, Robert J., Aniruddha Ghosh, and Alex Mandel. 2022. Geodata: Download Geographic Data. https://CRAN.R-project.org/package=geodata. Lang, Michel, Martin Binder, Jakob Richter, Patrick Schratz, Florian Pfisterer, Stefan Coors, Quay Au, Giuseppe Casalicchio, Lars Kotthoff, and Bernd Bischl. 2019. “mlr3: A Modern Object-Oriented Machine Learning Framework in R.” Journal of Open Source Software, December. https://doi.org/10.21105/joss.01903. Lang, Michel, Bernd Bischl, Jakob Richter, Xudong Sun, and Martin Binder. 2022. Paradox: Define and Work with Parameter Spaces for Complex Algorithms. https://CRAN.R-project.org/package=paradox. Lloyd, Christopher D. 2010. Spatial Data Analysis. Oxford University Press. Lovelace, Robin, Jakub Nowosad, and Jannes Muenchow. 2019. Geocomputation with r. CRC Press. Luke, Douglas. 2015. A User’s Guide to Network Analysis in r. Use r! Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-23883-8. Meyer, David, Evgenia Dimitriadou, Kurt Hornik, Andreas Weingessel, and Friedrich Leisch. 2022. E1071: Misc Functions of the Department of Statistics, Probability Theory Group (Formerly: E1071), TU Wien. https://CRAN.R-project.org/package=e1071. Pebesma, Edzer. 2018. “Simple Features for R: Standardized Support for Spatial Vector Data.” The R Journal 10 (1): 439. https://doi.org/10.32614/RJ-2018-009. Pebesma, Edzer, and Roger Bivand. 2022. Spatial Data Sciencewith Applications in r. https://r-spatial.org/book/. Pebesma, Edzer, Thomas Mailund, and James Hiebert. 2016. “Measurement Units in R.” R Journal 8 (2): 486–94. https://doi.org/10.32614/RJ-2016-061. Pedersen, Thomas Lin. 2022a. Ggraph: An Implementation of Grammar of Graphics for Graphs and Networks. https://CRAN.R-project.org/package=ggraph. ———. 2022b. Tidygraph: A Tidy API for Graph Manipulation. https://CRAN.R-project.org/package=tidygraph. Pélissier, Raphaël, and François Goreaud. 2015. “ads Package for R: A Fast Unbiased Implementation of the \\(K\\)-Function Family for Studying Spatial Point Patterns in Irregular-Shaped Sampling Windows.” Journal of Statistical Software 63 (6): 1–18. http://dx.doi.org/10.18637/jss.v063.i06. Rowlingson, Barry, and Peter Diggle. 2022. Splancs: Spatial and Space-Time Point Pattern Analysis. https://CRAN.R-project.org/package=splancs. Schratz, Patrick, and Marc Becker. 2022. Mlr3spatiotempcv: Spatiotemporal Resampling Methods for ’Mlr3’. https://CRAN.R-project.org/package=mlr3spatiotempcv. Tennekes, Martijn. 2018. “tmap: Thematic Maps in R.” Journal of Statistical Software 84 (6): 1–39. https://doi.org/10.18637/jss.v084.i06. van der Meer, Lucas, Lorena Abad, Andrea Gilardi, and Robin Lovelace. 2024. Sfnetworks: Tidy Geospatial Networks. https://CRAN.R-project.org/package=sfnetworks. Wickham, Hadley. 2016. Ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org. Wickham, Hadley, Romain François, Lionel Henry, and Kirill Müller. 2022. Dplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr. Wright, Marvin N., and Andreas Ziegler. 2017. “ranger: A Fast Implementation of Random Forests for High Dimensional Data in C++ and R.” Journal of Statistical Software 77 (1): 1–17. https://doi.org/10.18637/jss.v077.i01. Zizka, Alexander, Daniele Silvestro, Tobias Andermann, Josué Azevedo, Camila Duarte Ritter, Daniel Edler, Harith Farooq, et al. 2019. “\\(\\less\\)Scp\\(\\greater\\)CoordinateCleaner\\(\\less\\)/Scp\\(\\greater\\): Standardized Cleaning of Occurrence Records from Biological Collection Databases.” Edited by Tiago Quental. Methods in Ecology and Evolution 10 (5): 744–51. https://doi.org/10.1111/2041-210x.13152. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
