---
title: 'Species distribution model with HMSC '
author: 'Jonathan Jupke '
date: '2020-10-26'
slug: species-distribution-model-with-hmsc
categories: []
tags:
  - R
  - MOD3
  - HMSC
toc: no
images: ~
---

<link href="index_files/anchor-sections/anchor-sections.css" rel="stylesheet" />
<script src="index_files/anchor-sections/anchor-sections.js"></script>
<script src="index_files/kePrint/kePrint.js"></script>
<link href="index_files/lightable/lightable.css" rel="stylesheet" />

<div id="TOC">
no
</div>

<pre class="r"><code>#TODO where did I get the Data from? 
#TODO add link to multispecies example</code></pre>
<p>Intro text</p>
<p>To run this code you will need to install the pacman R package beforehand but this will take care of all the other packages required.</p>
<pre class="r"><code>if(!require(pacman)) install.packages(&quot;pacman&quot;)
p_load(abind, data.table, dplyr, ggplot2, here, kableExtra, Hmsc, magrittr)</code></pre>
<p>We use the Finnish bird data that are also often used by the creators of HMSC to demonstrate the package (e.g.Â here and here). In this example we will focus on a single species, the Western Jackdaw (<em>Corvus monedula</em>). You can find an example with a joint species distribution model, that makes use of all features of HMSC, here.
We also look at just one year (2014) while the data set contains the year 2006 to 2014.</p>
<pre class="r"><code>data = fread(file.path(data.directory, &quot;data.csv&quot;))
data = data[Year == 2014]
data %&lt;&gt;% droplevels()

XData = as.data.frame(data[, c(&quot;Habitat&quot;, &quot;AprMay&quot;)])
names(XData) = c(&quot;hab&quot;, &quot;clim&quot;)

Y = as.matrix(data$Corvus_monedula)
colnames(Y) = &quot;Corvus monedula&quot;

xy = as.matrix(data[, c(&quot;x&quot;, &quot;y&quot;)])
studyDesign = data.frame(route = factor(data$Route))
rownames(xy) = studyDesign[, 1]
rL = HmscRandomLevel(sData = xy)

XFormula =  ~ hab + poly(clim, degree = 2, raw = TRUE)</code></pre>
<p>In the next step we define the model using the Hmsc function.
EXPLAIN ARGUMENTS</p>
<pre class="r"><code>m_full = Hmsc(Y=Y,
              XData=XData,
              XFormula=XFormula,
              distr = &quot;lognormal poisson&quot;,
              studyDesign = studyDesign,
              ranLevels=list(route=rL))</code></pre>
<p>Ok now lets have a look at the object we created by defining the model. From the environment pane we can already see that it is has the class Hmsc. If we type in the object name a string is returned giving the number of sampling units, species, environmental covariates, traits and random levels. If we were to use str() to have a closer look at the structure of the object we would see that it is actually a list of, in this case, 71 objects.</p>
<pre class="r"><code>m_full</code></pre>
<pre><code>## Hmsc object with 137 sampling units, 1 species, 7 covariates, 1 traits and 1 random levels</code></pre>
<p>Next comes the truly time consuming step: fitting the model with Markov Chain Monte Carlo.</p>
<pre class="r"><code>nChains = 2
thin = c(5, 10, 100)
nParallel = max(round(parallel::detectCores() / 2), 
                nChains)
samples = 1000
transient = 500 * thin
verbose = 500 * thin

for (i in seq_along(thin)) {
        model[[i]] = sampleMcmc(
                m_full,
                thin = thin[i],
                samples = samples,
                transient = transient,
                nChains = nChains,
                verbose = verbose,
                initPar = &quot;fixed effects&quot;

        )
}</code></pre>
<pre class="r"><code>mpost=list()
for (i in seq_along(model)) {
        mpost[[i]] = convertToCodaObject(model[[i]],
                                         spNamesNumbers = c(T, F),
                                         covNamesNumbers = c(T, F))
}
plot(mpost[[1]]$Beta[,1:3])</code></pre>
<p><img src="index_files/figure-html/mcmc-plots1-1.png" width="672" /></p>
<pre class="r"><code>plot(mpost[[2]]$Beta[,1:3])</code></pre>
<p><img src="index_files/figure-html/mcmc-plot2-1.png" width="672" /></p>
<pre class="r"><code>plot(mpost[[3]]$Beta[,1:3])</code></pre>
<p><img src="index_files/figure-html/mcmc-plot3-1.png" width="672" /></p>
<p>Based on these plots we can clearly see that using a thinning of 100 is necessary for satisfactory MCMC convergence. In addition to these visual checks we can also look at the potential scale reduction factor (aka Gelman statistic) and the effective samples size.</p>
<pre class="r"><code>ess.beta = effectiveSize(mpost[[3]]$Beta)</code></pre>
<table class=" lightable-minimal" style='font-family: "Trebuchet MS", verdana, sans-serif; margin-left: auto; margin-right: auto;'>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
ess
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
(Intercept)
</td>
<td style="text-align:right;">
110.1091
</td>
</tr>
<tr>
<td style="text-align:left;">
habCo
</td>
<td style="text-align:right;">
433.1376
</td>
</tr>
<tr>
<td style="text-align:left;">
habOp
</td>
<td style="text-align:right;">
593.9599
</td>
</tr>
<tr>
<td style="text-align:left;">
habUrb
</td>
<td style="text-align:right;">
787.8184
</td>
</tr>
<tr>
<td style="text-align:left;">
habWe
</td>
<td style="text-align:right;">
236.7238
</td>
</tr>
<tr>
<td style="text-align:left;">
poly(clim, degree = 2, raw = TRUE)1
</td>
<td style="text-align:right;">
184.9870
</td>
</tr>
<tr>
<td style="text-align:left;">
poly(clim, degree = 2, raw = TRUE)2
</td>
<td style="text-align:right;">
404.2062
</td>
</tr>
</tbody>
</table>
<pre class="r"><code>psrf.beta = gelman.diag(mpost[[3]]$Beta,
                         multivariate = FALSE)$psrf</code></pre>
<table class=" lightable-minimal" style='font-family: "Trebuchet MS", verdana, sans-serif; margin-left: auto; margin-right: auto;'>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
psrf.Point.est.
</th>
<th style="text-align:right;">
psrf.Upper.C.I.
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
(Intercept)
</td>
<td style="text-align:right;">
1.0598018
</td>
<td style="text-align:right;">
1.133281
</td>
</tr>
<tr>
<td style="text-align:left;">
habCo
</td>
<td style="text-align:right;">
0.9995814
</td>
<td style="text-align:right;">
1.000565
</td>
</tr>
<tr>
<td style="text-align:left;">
habOp
</td>
<td style="text-align:right;">
1.0024632
</td>
<td style="text-align:right;">
1.007084
</td>
</tr>
<tr>
<td style="text-align:left;">
habUrb
</td>
<td style="text-align:right;">
0.9996922
</td>
<td style="text-align:right;">
1.001133
</td>
</tr>
<tr>
<td style="text-align:left;">
habWe
</td>
<td style="text-align:right;">
1.0000163
</td>
<td style="text-align:right;">
1.000624
</td>
</tr>
<tr>
<td style="text-align:left;">
poly(clim, degree = 2, raw = TRUE)1
</td>
<td style="text-align:right;">
1.0750217
</td>
<td style="text-align:right;">
1.261937
</td>
</tr>
<tr>
<td style="text-align:left;">
poly(clim, degree = 2, raw = TRUE)2
</td>
<td style="text-align:right;">
1.0630630
</td>
<td style="text-align:right;">
1.245347
</td>
</tr>
</tbody>
</table>
<p>Next we evaluate the model fit. HMSC has a custom function that computes several goodness-of-fit metrics</p>
<pre class="r"><code>MF = list()
for (i in 1:3){
        preds = computePredictedValues(model[[i]], expected = FALSE)
        MF[[i]] = evaluateModelFit(hM = model[[i]], predY = preds)
}
metrics = names(MF[[1]])
values = 
  append(unlist(MF[[1]]), 
         append(unlist(MF[[2]]), 
                unlist(MF[[3]])))



dt_MF = tibble(
  value = values,
  metric = rep(metrics, 
               times = 3),
  thinning = factor(rep(c(5,10,100), each = length(MF[[1]])))
)

dt_MF %&gt;% 
  ggplot(aes(x=metric, y=value,col=thinning)) + 
  geom_point()</code></pre>
<p><img src="index_files/figure-html/evaluate-model%20fit-1.png" width="672" />
We can use variance partitioning to see how important sets how variables are in determining the abundance of <em>Corvus monedula</em>. First we group all covarites into one of two groups: habitat or climate. In our example the number of covariates is quite low and each group basically represents one variable. The group habitat represents the factor habitat with all its dummy-variable levels and the group climate second order polynomial of April temperatures.
Again we are interested in the results for all three models, to see whether the non-convergent models perform differently.</p>
<pre class="r"><code>groupnames = c(&quot;habitat&quot;, &quot;climate&quot;)
group = c(1,1,1,1,1,2,2)
VP = list()
for (i in 1:3){
        VP[[i]] = computeVariancePartitioning(model[[i]],
                                              group = group, 
                                              groupnames = groupnames)
}
dt_VP = tibble(
  variable = rep(c(&quot;habitat&quot;, &quot;climate&quot;, &quot;Random:route&quot;), times=3),
  value= c(c(VP[[1]]$vals), c(VP[[2]]$vals), c(VP[[3]]$vals)),
  thinning = factor(rep(c(5,10,100), each = 3))
)

dt_VP %&gt;% 
  ggplot(aes(x=variable, y=value,col=thinning)) + 
  geom_point()</code></pre>
<p><img src="index_files/figure-html/variance-partitioning-1.png" width="672" /></p>
<p>Lastly we will predict the portability of occurrence and the abundance of <em>Corvus monedula</em>.
First along hypothetical gradients of covariates and afterwards across a grid of Finnland.</p>
<pre class="r"><code>m = model[[3]]
m$XData$hab %&lt;&gt;% factor()
Gradient = constructGradient(m, 
                             focalVariable = &quot;clim&quot;,
                             non.focalVariables = list(hab = 1))

predY = predict(m,
                Gradient = Gradient, 
                expected = TRUE)

plotGradient(m,
             Gradient, 
             pred = predY, 
             measure = &quot;Y&quot;,
             index = 1,
             showData = TRUE)</code></pre>
<p><img src="index_files/figure-html/predict-graduient-1.png" width="672" /></p>
<pre><code>## [1] 1</code></pre>
<pre class="r"><code>Gradient2 = constructGradient(m,
                              focalVariable = &quot;clim&quot;,
                              non.focalVariables = list(hab = 2))</code></pre>
<pre><code>## # weights:  15 (8 variable)
## initial  value 220.492994 
## iter  10 value 160.509875
## iter  20 value 158.882636
## final  value 158.882627 
## converged</code></pre>
<pre class="r"><code>predY2 = predict(m, 
                Gradient = Gradient2, 
                expected = TRUE)
plotGradient(m, 
             Gradient2, 
             pred = predY2, 
             measure = &quot;Y&quot;,
             index = 1,
             showData = TRUE)</code></pre>
<p><img src="index_files/figure-html/predict-graduient-2.png" width="672" /></p>
<pre><code>## [1] 1</code></pre>
<p>The prediction performance can be evaluated with cross-validation.</p>
<pre class="r"><code># Create partitions (folds)
partition = list()
MF = list()
for (i in 1:3){
  partition = createPartition(model[[i]],
                              nfolds = 2,
                              column = &quot;route&quot;)
  preds = computePredictedValues(model[[i]],
                                 partition = partition)
  MF[[i]] = evaluateModelFit(hM = model[[i]], predY = preds)
  rm(partition, preds)
}</code></pre>
<pre class="r"><code>dt_MF = data.table(
  variable = rep(c(&quot;RMSE&quot;, &quot;SR2&quot;, &quot;O.AUC&quot;, &quot;O.TjurR2&quot;, &quot;O.RMSE&quot;, &quot;C.SR2&quot;, &quot;C.RMSE&quot;), times=3),
  value= unlist(lapply(MF, unname)),
  thinning = factor(rep(c(5,10,100), each = 7))
)
dt_MF[!variable %in% c(&quot;RMSE&quot;, &quot;C.RMSE&quot;), value := value * 10]
dt_MF %&gt;% 
  ggplot(aes(x=variable, y=value,col=thinning)) + 
  geom_point()</code></pre>
<p><img src="index_files/figure-html/pred-perf-plot-1.png" width="672" /></p>
<p><img src="index_files/figure-html/hmsc-spatial-prediction-hidden-1.png" width="672" /><img src="index_files/figure-html/hmsc-spatial-prediction-hidden-2.png" width="672" /></p>
<pre class="r"><code>m = model[[3]]
grid = read.csv(file.path(data.directory,
                          &quot;grid_1000.csv&quot;))
grid = droplevels(subset(grid, !(Habitat==&quot;Ma&quot;)))
xy.grid = as.matrix(cbind(grid$x, grid$y))
XData.grid = data.frame(hab = grid$Habitat,
                        clim = grid$AprMay)
Gradient = prepareGradient(m, XDataNew = XData.grid,
                           sDataNew = list(route = xy.grid))
predY = predict(m, Gradient = Gradient)
EpredY = apply(abind(predY,along = 3), c(1,2), mean)
EpredO = apply(abind(predY,along = 3), c(1,2), FUN =
                       function(a) {mean(a &gt; 0)})
mapData=data.frame(xy.grid, EpredY,EpredO)
names(mapData)=c(&quot;xCoordinates&quot;, &quot;yCoordinates&quot;, &quot;PredictedAbundance&quot;, &quot;PredictedOccurence&quot;)
spO &lt;- ggplot(data = mapData, 
             aes(x= xCoordinates, 
                 y= yCoordinates, 
                 color=PredictedOccurence)
             ) +
  geom_point(size=2)
spC &lt;- ggplot(data = mapData, 
              aes(x= xCoordinates, 
                  y= yCoordinates, 
                  color=PredictedAbundance)
) +
  geom_point(size=2)

spO + 
  ggtitle(&quot;Predicted Corvus monedula occurrence&quot;) +
  xlab(&quot;East coordinate (km)&quot;) + 
  ylab(&quot;North coordinate (km)&quot;) + 
  scale_color_gradient(low = &quot;blue&quot;, 
                       high=&quot;red&quot;, 
                       name =&quot;Occurrence probability&quot;)
spC + 
  ggtitle(&quot;Predicted Corvus monedula abundance&quot;) +
  xlab(&quot;East coordinate (km)&quot;) + 
  ylab(&quot;North coordinate (km)&quot;) + 
  scale_color_gradient(low = &quot;blue&quot;, 
                       high=&quot;red&quot;, 
                       name =&quot;Abundance&quot;)</code></pre>
